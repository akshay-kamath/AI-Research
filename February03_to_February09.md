# üìå AI Research Papers (February03 to February09)

## üîπ LLM

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Fillerbuster: Multi-View Scene Completion for Casual Captures](http://arxiv.org/abs/2502.05175v1) | Ethan Weber, Norman M√ºller, Yash Kant, Vasu Agrawal, Michael Zollh√∂fer, Angjoo Kanazawa, Christian Richardt | 2025-02-07 | LLM, Diffusion Models | We present Fillerbuster, a method that completes unknown regions of a 3D scene by utilizing a novel large-scale multi-view latent diffusion transformer. Casual captures are often sparse and miss surrounding content behind objects or above the scene. Existing methods are not suitable for handling this challenge as they focus on making the known pixels look good with sparse-view priors, or on creating the missing sides of objects from just one or two photos. In reality, we often have hundreds of input frames and want to complete areas that are missing and unobserved from the input frames. Additionally, the images often do not have known camera parameters. Our solution is to train a generative model that can consume a large context of input frames while generating unknown target views and recovering image poses when desired. We show results where we complete partial captures on two existing datasets. We also present an uncalibrated scene completion task where our unified model predicts both poses and creates new content. Our model is the first to predict many images and poses together for scene completion. | [üîó Paper](http://arxiv.org/abs/2502.05175v1) |
| [Joint MoE Scaling Laws: Mixture of Experts Can Be Memory Efficient](http://arxiv.org/abs/2502.05172v1) | Jan Ludziejewski, Maciej Pi√≥ro, Jakub Krajewski, Maciej Stefaniak, Micha≈Ç Krutul, Jan Ma≈Ça≈õnicki, Marek Cygan, Piotr Sankowski, Kamil Adamczewski, Piotr Mi≈Ço≈õ, Sebastian Jaszczur | 2025-02-07 | LLM, Scaling Laws | Mixture of Experts (MoE) architectures have significantly increased computational efficiency in both research and real-world applications of large-scale machine learning models. However, their scalability and efficiency under memory constraints remain relatively underexplored. In this work, we present joint scaling laws for dense and MoE models, incorporating key factors such as the number of active parameters, dataset size, and the number of experts. Our findings provide a principled framework for selecting the optimal MoE configuration under fixed memory and compute budgets. Surprisingly, we show that MoE models can be more memory-efficient than dense models, contradicting conventional wisdom. To derive and validate the theoretical predictions of our scaling laws, we conduct over 280 experiments with up to 2.7B active parameters and up to 5B total parameters. These results offer actionable insights for designing and deploying MoE models in practical large-scale training scenarios. | [üîó Paper](http://arxiv.org/abs/2502.05172v1) |
| [NoLiMa: Long-Context Evaluation Beyond Literal Matching](http://arxiv.org/abs/2502.05167v1) | Ali Modarressi, Hanieh Deilamsalehy, Franck Dernoncourt, Trung Bui, Ryan A. Rossi, Seunghyun Yoon, Hinrich Sch√ºtze | 2025-02-07 | LLM, Training & Evaluation | Recent large language models (LLMs) support long contexts ranging from 128K to 1M tokens. A popular method for evaluating these capabilities is the needle-in-a-haystack (NIAH) test, which involves retrieving a "needle" (relevant information) from a "haystack" (long irrelevant context). Extensions of this approach include increasing distractors, fact chaining, and in-context reasoning. However, in these benchmarks, models can exploit existing literal matches between the needle and haystack to simplify the task. To address this, we introduce NoLiMa, a benchmark extending NIAH with a carefully designed needle set, where questions and needles have minimal lexical overlap, requiring models to infer latent associations to locate the needle within the haystack. We evaluate 12 popular LLMs that claim to support contexts of at least 128K tokens. While they perform well in short contexts (<1K), performance degrades significantly as context length increases. At 32K, for instance, 10 models drop below 50% of their strong short-length baselines. Even GPT-4o, one of the top-performing exceptions, experiences a reduction from an almost-perfect baseline of 99.3% to 69.7%. Our analysis suggests these declines stem from the increased difficulty the attention mechanism faces in longer contexts when literal matches are absent, making it harder to retrieve relevant information. | [üîó Paper](http://arxiv.org/abs/2502.05167v1) |
| [In-context denoising with one-layer transformers: connections between
  attention and associative memory retrieval](http://arxiv.org/abs/2502.05164v1) | Matthew Smart, Alberto Bietti, Anirvan M. Sengupta | 2025-02-07 | LLM, Diffusion Models, Training & Evaluation | We introduce in-context denoising, a task that refines the connection between attention-based architectures and dense associative memory (DAM) networks, also known as modern Hopfield networks. Using a Bayesian framework, we show theoretically and empirically that certain restricted denoising problems can be solved optimally even by a single-layer transformer. We demonstrate that a trained attention layer processes each denoising prompt by performing a single gradient descent update on a context-aware DAM energy landscape, where context tokens serve as associative memories and the query token acts as an initial state. This one-step update yields better solutions than exact retrieval of either a context token or a spurious local minimum, providing a concrete example of DAM networks extending beyond the standard retrieval paradigm. Overall, this work solidifies the link between associative memory and attention mechanisms first identified by Ramsauer et al., and demonstrates the relevance of associative memory models in the study of in-context learning. | [üîó Paper](http://arxiv.org/abs/2502.05164v1) |
## üîπ RLHF

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM
  Guardrails](http://arxiv.org/abs/2502.05163v1) | Yihe Deng, Yu Yang, Junkai Zhang, Wei Wang, Bo Li | 2025-02-07 | RLHF | The rapid advancement of large language models (LLMs) has increased the need for guardrail models to ensure responsible use, particularly in detecting unsafe and illegal content. While substantial safety data exist in English, multilingual guardrail modeling remains underexplored due to the scarcity of open-source safety data in other languages. To address this gap, we propose a novel two-player Reinforcement Learning (RL) framework, where a generator and a guardrail model co-evolve adversarially to produce high-quality synthetic data for multilingual guardrail training. We theoretically formalize this interaction as a two-player game, proving convergence to a Nash equilibrium. Empirical evaluations show that our model \ours outperforms state-of-the-art models, achieving nearly 10% improvement over LlamaGuard3 (8B) on English benchmarks while being 4.5x faster at inference with a significantly smaller model (0.5B). We achieve substantial advancements in multilingual safety tasks, particularly in addressing the imbalance for lower-resource languages in a collected real dataset. Ablation studies emphasize the critical role of synthetic data generation in bridging the imbalance in open-source data between English and other languages. These findings establish a scalable and efficient approach to synthetic data generation, paving the way for improved multilingual guardrail models to enhance LLM safety. Code, model, and data will be open-sourced at https://github.com/yihedeng9/DuoGuard. | [üîó Paper](http://arxiv.org/abs/2502.05163v1) |
## üîπ Multimodal AI

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [FlashVideo:Flowing Fidelity to Detail for Efficient High-Resolution
  Video Generation](http://arxiv.org/abs/2502.05179v1) | Shilong Zhang, Wenbo Li, Shoufa Chen, Chongjian Ge, Peize Sun, Yida Zhang, Yi Jiang, Zehuan Yuan, Binyue Peng, Ping Luo | 2025-02-07 | Multimodal AI, Diffusion Models | DiT diffusion models have achieved great success in text-to-video generation, leveraging their scalability in model capacity and data scale. High content and motion fidelity aligned with text prompts, however, often require large model parameters and a substantial number of function evaluations (NFEs). Realistic and visually appealing details are typically reflected in high resolution outputs, further amplifying computational demands especially for single stage DiT models. To address these challenges, we propose a novel two stage framework, FlashVideo, which strategically allocates model capacity and NFEs across stages to balance generation fidelity and quality. In the first stage, prompt fidelity is prioritized through a low resolution generation process utilizing large parameters and sufficient NFEs to enhance computational efficiency. The second stage establishes flow matching between low and high resolutions, effectively generating fine details with minimal NFEs. Quantitative and visual results demonstrate that FlashVideo achieves state-of-the-art high resolution video generation with superior computational efficiency. Additionally, the two-stage design enables users to preview the initial output before committing to full resolution generation, thereby significantly reducing computational costs and wait times as well as enhancing commercial viability . | [üîó Paper](http://arxiv.org/abs/2502.05179v1) |
| [QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive
  Multimodal Understanding and Generation](http://arxiv.org/abs/2502.05178v1) | Yue Zhao, Fuzhao Xue, Scott Reed, Linxi Fan, Yuke Zhu, Jan Kautz, Zhiding Yu, Philipp Kr√§henb√ºhl, De-An Huang | 2025-02-07 | Multimodal AI, RLHF, Optimization, Prompt Engineering | We introduce Quantized Language-Image Pretraining (QLIP), a visual tokenization method that combines state-of-the-art reconstruction quality with state-of-the-art zero-shot image understanding. QLIP trains a binary-spherical-quantization-based autoencoder with reconstruction and language-image alignment objectives. We are the first to show that the two objectives do not need to be at odds. We balance the two loss terms dynamically during training and show that a two-stage training pipeline effectively mixes the large-batch requirements of image-language pre-training with the memory bottleneck imposed by the reconstruction objective. We validate the effectiveness of QLIP for multimodal understanding and text-conditioned image generation with a single model. Specifically, QLIP serves as a drop-in replacement for the visual encoder for LLaVA and the image tokenizer for LlamaGen with comparable or even better performance. Finally, we demonstrate that QLIP enables a unified mixed-modality auto-regressive model for understanding and generation. | [üîó Paper](http://arxiv.org/abs/2502.05178v1) |
| [AuraFusion360: Augmented Unseen Region Alignment for Reference-based
  360¬∞ Unbounded Scene Inpainting](http://arxiv.org/abs/2502.05176v1) | Chung-Ho Wu, Yang-Jung Chen, Ying-Huan Chen, Jie-Ying Lee, Bo-Hsu Ke, Chun-Wei Tuan Mu, Yi-Chuan Huang, Chin-Yang Lin, Min-Hung Chen, Yen-Yu Lin, Yu-Lun Liu | 2025-02-07 | Multimodal AI, RLHF, Diffusion Models, Prompt Engineering | Three-dimensional scene inpainting is crucial for applications from virtual reality to architectural visualization, yet existing methods struggle with view consistency and geometric accuracy in 360{\deg} unbounded scenes. We present AuraFusion360, a novel reference-based method that enables high-quality object removal and hole filling in 3D scenes represented by Gaussian Splatting. Our approach introduces (1) depth-aware unseen mask generation for accurate occlusion identification, (2) Adaptive Guided Depth Diffusion, a zero-shot method for accurate initial point placement without requiring additional training, and (3) SDEdit-based detail enhancement for multi-view coherence. We also introduce 360-USID, the first comprehensive dataset for 360{\deg} unbounded scene inpainting with ground truth. Extensive experiments demonstrate that AuraFusion360 significantly outperforms existing methods, achieving superior perceptual quality while maintaining geometric accuracy across dramatic viewpoint changes. See our project page for video results and the dataset at https://kkennethwu.github.io/aurafusion360/. | [üîó Paper](http://arxiv.org/abs/2502.05176v1) |
| [VideoRoPE: What Makes for Good Video Rotary Position Embedding?](http://arxiv.org/abs/2502.05173v1) | Xilin Wei, Xiaoran Liu, Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Jian Tong, Haodong Duan, Qipeng Guo, Jiaqi Wang, Xipeng Qiu, Dahua Lin | 2025-02-07 | Multimodal AI, Model Evaluation | While Rotary Position Embedding (RoPE) and its variants are widely adopted for their long-context capabilities, the extension of the 1D RoPE to video, with its complex spatio-temporal structure, remains an open challenge. This work first introduces a comprehensive analysis that identifies four key characteristics essential for the effective adaptation of RoPE to video, which have not been fully considered in prior work. As part of our analysis, we introduce a challenging V-NIAH-D (Visual Needle-In-A-Haystack with Distractors) task, which adds periodic distractors into V-NIAH. The V-NIAH-D task demonstrates that previous RoPE variants, lacking appropriate temporal dimension allocation, are easily misled by distractors. Based on our analysis, we introduce \textbf{VideoRoPE}, with a \textit{3D structure} designed to preserve spatio-temporal relationships. VideoRoPE features \textit{low-frequency temporal allocation} to mitigate periodic oscillations, a \textit{diagonal layout} to maintain spatial symmetry, and \textit{adjustable temporal spacing} to decouple temporal and spatial indexing. VideoRoPE consistently surpasses previous RoPE variants, across diverse downstream tasks such as long video retrieval, video understanding, and video hallucination. Our code will be available at \href{https://github.com/Wiselnn570/VideoRoPE}{https://github.com/Wiselnn570/VideoRoPE}. | [üîó Paper](http://arxiv.org/abs/2502.05173v1) |
| [Multitwine: Multi-Object Compositing with Text and Layout Control](http://arxiv.org/abs/2502.05165v1) | Gemma Canet Tarr√©s, Zhe Lin, Zhifei Zhang, He Zhang, Andrew Gilbert, John Collomosse, Soo Ye Kim | 2025-02-07 | Multimodal AI | We introduce the first generative model capable of simultaneous multi-object compositing, guided by both text and layout. Our model allows for the addition of multiple objects within a scene, capturing a range of interactions from simple positional relations (e.g., next to, in front of) to complex actions requiring reposing (e.g., hugging, playing guitar). When an interaction implies additional props, like `taking a selfie', our model autonomously generates these supporting objects. By jointly training for compositing and subject-driven generation, also known as customization, we achieve a more balanced integration of textual and visual inputs for text-driven object compositing. As a result, we obtain a versatile model with state-of-the-art performance in both tasks. We further present a data generation pipeline leveraging visual and language models to effortlessly synthesize multimodal, aligned training data. | [üîó Paper](http://arxiv.org/abs/2502.05165v1) |
## üîπ Scaling Laws

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Long-VITA: Scaling Large Multi-modal Models to 1 Million Tokens with
  Leading Short-Context Accuray](http://arxiv.org/abs/2502.05177v1) | Yunhang Shen, Chaoyou Fu, Shaoqi Dong, Xiong Wang, Peixian Chen, Mengdan Zhang, Haoyu Cao, Ke Li, Xiawu Zheng, Yan Zhang, Yiyi Zhou, Rongrong Ji, Xing Sun | 2025-02-07 | Scaling Laws, Multimodal AI, RLHF, Optimization | Establishing the long-context capability of large vision-language models is crucial for video understanding, high-resolution image understanding, multi-modal agents and reasoning. We introduce Long-VITA, a simple yet effective large multi-modal model for long-context visual-language understanding tasks. It is adept at concurrently processing and analyzing modalities of image, video, and text over 4K frames or 1M tokens while delivering advanced performances on short-context multi-modal tasks. We propose an effective multi-modal training schema that starts with large language models and proceeds through vision-language alignment, general knowledge learning, and two sequential stages of long-sequence fine-tuning. We further implement context-parallelism distributed inference and logits-masked language modeling head to scale Long-VITA to infinitely long inputs of images and texts during model inference. Regarding training data, Long-VITA is built on a mix of $17$M samples from public datasets only and demonstrates the state-of-the-art performance on various multi-modal benchmarks, compared against recent cutting-edge models with internal data. Long-VITA is fully reproducible and supports both NPU and GPU platforms for training and testing. We hope Long-VITA can serve as a competitive baseline and offer valuable insights for the open-source community in advancing long-context multi-modal understanding. | [üîó Paper](http://arxiv.org/abs/2502.05177v1) |
| [Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth
  Approach](http://arxiv.org/abs/2502.05171v1) | Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, Tom Goldstein | 2025-02-07 | Scaling Laws, Prompt Engineering | We study a novel language model architecture that is capable of scaling test-time computation by implicitly reasoning in latent space. Our model works by iterating a recurrent block, thereby unrolling to arbitrary depth at test-time. This stands in contrast to mainstream reasoning models that scale up compute by producing more tokens. Unlike approaches based on chain-of-thought, our approach does not require any specialized training data, can work with small context windows, and can capture types of reasoning that are not easily represented in words. We scale a proof-of-concept model to 3.5 billion parameters and 800 billion tokens. We show that the resulting model can improve its performance on reasoning benchmarks, sometimes dramatically, up to a computation load equivalent to 50 billion parameters. | [üîó Paper](http://arxiv.org/abs/2502.05171v1) |
| [Impulse measurements enhanced with squeezed readout light](http://arxiv.org/abs/2502.05168v1) | Tsai-Chen Lee, Jacob L. Beckey, Giacomo Marocco, Daniel Carney | 2025-02-07 | Scaling Laws | We quantify how squeezed light can reduce quantum measurement noise to levels below the standard quantum limit in impulse measurements with mechanical detectors. The broadband nature of the signal implies that frequency-dependent squeezing performs better than frequency-independent squeezing. We calculate the optimal scaling of the impulse sensitivity with the squeezing strength, and quantify degradations due to photodetection losses. Even for lossless measurement, we find there exists a fundamental limit to the benefit of squeezing that depends only on the system's mechanical properties. | [üîó Paper](http://arxiv.org/abs/2502.05168v1) |
## üîπ Security & Adversarial ML

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [MELON: Indirect Prompt Injection Defense via Masked Re-execution and
  Tool Comparison](http://arxiv.org/abs/2502.05174v1) | Kaijie Zhu, Xianjun Yang, Jindong Wang, Wenbo Guo, William Yang Wang | 2025-02-07 | Security & Adversarial ML, Training & Evaluation | Recent research has explored that LLM agents are vulnerable to indirect prompt injection (IPI) attacks, where malicious tasks embedded in tool-retrieved information can redirect the agent to take unauthorized actions. Existing defenses against IPI have significant limitations: either require essential model training resources, lack effectiveness against sophisticated attacks, or harm the normal utilities. We present MELON (Masked re-Execution and TooL comparisON), a novel IPI defense. Our approach builds on the observation that under a successful attack, the agent's next action becomes less dependent on user tasks and more on malicious tasks. Following this, we design MELON to detect attacks by re-executing the agent's trajectory with a masked user prompt modified through a masking function. We identify an attack if the actions generated in the original and masked executions are similar. We also include three key designs to reduce the potential false positives and false negatives. Extensive evaluation on the IPI benchmark AgentDojo demonstrates that MELON outperforms SOTA defenses in both attack prevention and utility preservation. Moreover, we show that combining MELON with a SOTA prompt augmentation defense (denoted as MELON-Aug) further improves its performance. We also conduct a detailed ablation study to validate our key designs. | [üîó Paper](http://arxiv.org/abs/2502.05174v1) |
## üîπ General AI

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Observation of a dynamic magneto-chiral instability in photoexcited
  tellurium](http://arxiv.org/abs/2502.05170v1) | Yijing Huang, Nick Abboud, Yinchuan Lv, Penghao Zhu, Azel Murzabekova, Changjun Lee, Emma A. Pappas, Dominic Petruzzi, Jason Y. Yan, Dipanjan Chauduri, Peter Abbamonte, Daniel P. Shoemaker, Rafael M. Fernandes, Jorge Noronha, Fahad Mahmood | 2025-02-07 | General AI | In a system of charged chiral fermions driven out of equilibrium, an electric current parallel to the magnetic field can generate a dynamic instability by which electromagnetic waves become amplified. Whether a similar instability can occur in chiral solid-state systems remains an open question. Using time-domain terahertz (THz) emission spectroscopy, we detect signatures of what we dub a ``dynamic magneto-chiral instability" in elemental tellurium, a structurally chiral crystal. Upon transient photoexcitation in a moderate external magnetic field, tellurium emits THz radiation consisting of coherent modes that amplify over time. An explanation for this amplification is proposed using a theoretical model based on a dynamic instability of electromagnetic waves interacting with infrared-active oscillators of impurity acceptor states in tellurium to form an amplifying polariton. Our work not only uncovers the presence of a magneto-chiral instability but also highlights its promise for THz-wave amplification in chiral materials. | [üîó Paper](http://arxiv.org/abs/2502.05170v1) |
| [Flopping for FLOPs: Leveraging equivariance for computational efficiency](http://arxiv.org/abs/2502.05169v1) | Georg B√∂kman, David Nordstr√∂m, Fredrik Kahl | 2025-02-07 | General AI | Incorporating geometric invariance into neural networks enhances parameter efficiency but typically increases computational costs. This paper introduces new equivariant neural networks that preserve symmetry while maintaining a comparable number of floating-point operations (FLOPs) per parameter to standard non-equivariant networks. We focus on horizontal mirroring (flopping) invariance, common in many computer vision tasks. The main idea is to parametrize the feature spaces in terms of mirror-symmetric and mirror-antisymmetric features, i.e., irreps of the flopping group. This decomposes the linear layers to be block-diagonal, requiring half the number of FLOPs. Our approach reduces both FLOPs and wall-clock time, providing a practical solution for efficient, scalable symmetry-aware architectures. | [üîó Paper](http://arxiv.org/abs/2502.05169v1) |
| [Stirring supercooled colloidal liquids at the particle scale](http://arxiv.org/abs/2502.05166v1) | Piotr Habdas, Eric R. Weeks | 2025-02-07 | General AI | We study the decay of tangential velocity profiles with distance from a local disturbance in hard-sphere colloidal suspensions as the colloidal glass transition is approached. The disturbance, generated by a dimer of superparamagnetic particles rotated by an external magnetic field, enables a precise characterization of the system's response through confocal microscopy and tracking of individual particle dynamics. The tangential velocity profiles exhibit nearly exponential decay with distance. As particle density increases toward the colloidal glass transition, the characteristic length scale derived from exponential fits grows. We also observe that the colloidal particles slip against the rotating dimer, with less slip in samples which are closer to the glass transition. | [üîó Paper](http://arxiv.org/abs/2502.05166v1) |
| [Ramsey Theory on the Integer Grid: The "L" Problem](http://arxiv.org/abs/2502.05162v1) | Isaac Mammel, William Smith, Carl Yerger | 2025-02-07 | General AI | In an $[n] \times [n]$ integer grid, a monochromatic $L$ is any set of points $\{(i, j), (i, j+t), (i+t, j+t)\}$ for some positive integer $t$, where $1 \leq i, j, i+t, j+t \leq n$. In this paper, we investigate the upper bound for the smallest integer $n$ such that a $3$-colored $n \times n$ grid is guaranteed to contain a monochromatic $L$. We use various methods, such as counting intervals on the main diagonal and using Golomb rulers, to improve the upper bound. This bound originally sat at 2593, and we improve it first to 1803, then to 1573, then to 772, and finally to 493. In the latter part of this paper, we discuss the lower bound and our attempts to improve it using SAT solvers. | [üîó Paper](http://arxiv.org/abs/2502.05162v1) |
| [Estimated Roadway Segment Traffic Data by Vehicle Class for the United
  States: A Machine Learning Approach](http://arxiv.org/abs/2502.05161v1) | Brittany Antonczak, Meg Fay, Aviral Chawla, Gregory Rowangould | 2025-02-07 | General AI | The Highway Performance Monitoring System, managed by the Federal Highway Administration, provides essential data on average annual daily traffic across U.S. roadways, but it has limited representation of medium- and heavy-duty vehicles on non-interstate roads. This gap limits research and policy analysis on the impacts of truck traffic, especially concerning air quality and public health. To address this, we use random forest regression to estimate medium- and heavy-duty vehicle traffic volumes in areas with sparse data. This results in a more comprehensive dataset, which enables the estimation of traffic density at the census block level as a proxy for traffic-related air pollution exposure. Our high-resolution spatial data products, rigorously validated, provide a more accurate representation of truck traffic and its environmental and health impacts. These datasets are valuable for transportation planning, public health research, and policy decisions aimed at mitigating the effects of truck traffic on vulnerable communities exposed to air pollution. | [üîó Paper](http://arxiv.org/abs/2502.05161v1) |
| [A parameter study for LLL and BKZ with application to shortest vector
  problems](http://arxiv.org/abs/2502.05160v1) | Tobias K√∂ppl, Ren√© Zander, Louis Henkel, Nikolay Tcholtchev | 2025-02-07 | General AI | In this work, we study the solution of shortest vector problems (SVPs) arising in terms of learning with error problems (LWEs). LWEs are linear systems of equations over a modular ring, where a perturbation vector is added to the right-hand side. This type of problem is of great interest, since LWEs have to be solved in order to be able to break lattice-based cryptosystems as the Module-Lattice-Based Key-Encapsulation Mechanism published by NIST in 2024. Due to this fact, several classical and quantum-based algorithms have been studied to solve SVPs. Two well-known algorithms that can be used to simplify a given SVP are the Lenstra-Lenstra-Lov\'asz (LLL) algorithm and the Block Korkine-Zolotarev (BKZ) algorithm. LLL and BKZ construct bases that can be used to compute or approximate solutions of the SVP. We study the performance of both algorithms for SVPs with different sizes and modular rings. Thereby, application of LLL or BKZ to a given SVP is considered to be successful if they produce bases containing a solution vector of the SVP. | [üîó Paper](http://arxiv.org/abs/2502.05160v1) |
