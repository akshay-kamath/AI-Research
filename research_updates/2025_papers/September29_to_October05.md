# üìå AI Research Papers (September29 to October05)

## üîπ Diffusion Models

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [NoiseShift: Resolution-Aware Noise Recalibration for Better
  Low-Resolution Image Generation](http://arxiv.org/abs/2510.02307v1) | Ruozhen He, Moayed Haji-Ali, Ziyan Yang, Vicente Ordonez | 2025-10-02 | Diffusion Models | Text-to-image diffusion models trained on a fixed set of resolutions often fail to generalize, even when asked to generate images at lower resolutions than those seen during training. High-resolution text-to-image generators are currently unable to easily offer an out-of-the-box budget-efficient alternative to their users who might not need high-resolution images. We identify a key technical insight in diffusion models that when addressed can help tackle this limitation: Noise schedulers have unequal perceptual effects across resolutions. The same level of noise removes disproportionately more signal from lower-resolution images than from high-resolution images, leading to a train-test mismatch. We propose NoiseShift, a training-free method that recalibrates the noise level of the denoiser conditioned on resolution size. NoiseShift requires no changes to model architecture or sampling schedule and is compatible with existing models. When applied to Stable Diffusion 3, Stable Diffusion 3.5, and Flux-Dev, quality at low resolutions is significantly improved. On LAION-COCO, NoiseShift improves SD3.5 by 15.89%, SD3 by 8.56%, and Flux-Dev by 2.44% in FID on average. On CelebA, NoiseShift improves SD3.5 by 10.36%, SD3 by 5.19%, and Flux-Dev by 3.02% in FID on average. These results demonstrate the effectiveness of NoiseShift in mitigating resolution-dependent artifacts and enhancing the quality of low-resolution image generation. | [üîó Paper](http://arxiv.org/abs/2510.02307v1) |
| [Diffusion Models and the Manifold Hypothesis: Log-Domain Smoothing is
  Geometry Adaptive](http://arxiv.org/abs/2510.02305v1) | Tyler Farghly, Peter Potaptchik, Samuel Howard, George Deligiannidis, Jakiw Pidstrigach | 2025-10-02 | Diffusion Models | Diffusion models have achieved state-of-the-art performance, demonstrating remarkable generalisation capabilities across diverse domains. However, the mechanisms underpinning these strong capabilities remain only partially understood. A leading conjecture, based on the manifold hypothesis, attributes this success to their ability to adapt to low-dimensional geometric structure within the data. This work provides evidence for this conjecture, focusing on how such phenomena could result from the formulation of the learning problem through score matching. We inspect the role of implicit regularisation by investigating the effect of smoothing minimisers of the empirical score matching objective. Our theoretical and empirical results confirm that smoothing the score function -- or equivalently, smoothing in the log-density domain -- produces smoothing tangential to the data manifold. In addition, we show that the manifold along which the diffusion model generalises can be controlled by choosing an appropriate smoothing. | [üîó Paper](http://arxiv.org/abs/2510.02305v1) |
| [Test-Time Anchoring for Discrete Diffusion Posterior Sampling](http://arxiv.org/abs/2510.02291v1) | Litu Rout, Andreas Lugmayr, Yasamin Jafarian, Srivatsan Varadharajan, Constantine Caramanis, Sanjay Shakkottai, Ira Kemelmacher-Shlizerman | 2025-10-02 | Diffusion Models | We study the problem of posterior sampling using pretrained discrete diffusion foundation models, aiming to recover images from noisy measurements without retraining task-specific models. While diffusion models have achieved remarkable success in generative modeling, most advances rely on continuous Gaussian diffusion. In contrast, discrete diffusion offers a unified framework for jointly modeling categorical data such as text and images. Beyond unification, discrete diffusion provides faster inference, finer control, and principled training-free Bayesian inference, making it particularly well-suited for posterior sampling. However, existing approaches to discrete diffusion posterior sampling face severe challenges: derivative-free guidance yields sparse signals, continuous relaxations limit applicability, and split Gibbs samplers suffer from the curse of dimensionality. To overcome these limitations, we introduce Anchored Posterior Sampling (APS) for masked diffusion foundation models, built on two key innovations -- quantized expectation for gradient-like guidance in discrete embedding space, and anchored remasking for adaptive decoding. Our approach achieves state-of-the-art performance among discrete diffusion samplers across linear and nonlinear inverse problems on the standard benchmarks. We further demonstrate the benefits of our approach in training-free stylization and text-guided editing. | [üîó Paper](http://arxiv.org/abs/2510.02291v1) |
## üîπ Multimodal AI

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Clink! Chop! Thud! -- Learning Object Sounds from Real-World
  Interactions](http://arxiv.org/abs/2510.02313v1) | Mengyu Yang, Yiming Chen, Haozheng Pei, Siddhant Agarwal, Arun Balajee Vasudevan, James Hays | 2025-10-02 | Multimodal AI | Can a model distinguish between the sound of a spoon hitting a hardwood floor versus a carpeted one? Everyday object interactions produce sounds unique to the objects involved. We introduce the sounding object detection task to evaluate a model's ability to link these sounds to the objects directly involved. Inspired by human perception, our multimodal object-aware framework learns from in-the-wild egocentric videos. To encourage an object-centric approach, we first develop an automatic pipeline to compute segmentation masks of the objects involved to guide the model's focus during training towards the most informative regions of the interaction. A slot attention visual encoder is used to further enforce an object prior. We demonstrate state of the art performance on our new task along with existing multimodal action understanding tasks. | [üîó Paper](http://arxiv.org/abs/2510.02313v1) |
| [Inferring Dynamic Physical Properties from Video Foundation Models](http://arxiv.org/abs/2510.02311v1) | Guanqi Zhan, Xianzheng Ma, Weidi Xie, Andrew Zisserman | 2025-10-02 | Multimodal AI, Training & Evaluation | We study the task of predicting dynamic physical properties from videos. More specifically, we consider physical properties that require temporal information to be inferred: elasticity of a bouncing object, viscosity of a flowing liquid, and dynamic friction of an object sliding on a surface. To this end, we make the following contributions: (i) We collect a new video dataset for each physical property, consisting of synthetic training and testing splits, as well as a real split for real world evaluation. (ii) We explore three ways to infer the physical property from videos: (a) an oracle method where we supply the visual cues that intrinsically reflect the property using classical computer vision techniques; (b) a simple read out mechanism using a visual prompt and trainable prompt vector for cross-attention on pre-trained video generative and self-supervised models; and (c) prompt strategies for Multi-modal Large Language Models (MLLMs). (iii) We show that video foundation models trained in a generative or self-supervised manner achieve a similar performance, though behind that of the oracle, and MLLMs are currently inferior to the other models, though their performance can be improved through suitable prompting. | [üîó Paper](http://arxiv.org/abs/2510.02311v1) |
| [VideoNSA: Native Sparse Attention Scales Video Understanding](http://arxiv.org/abs/2510.02295v1) | Enxin Song, Wenhao Chai, Shusheng Yang, Ethan Armand, Xiaojun Shan, Haiyang Xu, Jianwen Xie, Zhuowen Tu | 2025-10-02 | Multimodal AI, Scaling Laws | Video understanding in multimodal language models remains limited by context length: models often miss key transition frames and struggle to maintain coherence across long time scales. To address this, we adapt Native Sparse Attention (NSA) to video-language models. Our method, VideoNSA, adapts Qwen2.5-VL through end-to-end training on a 216K video instruction dataset. We employ a hardware-aware hybrid approach to attention, preserving dense attention for text, while employing NSA for video. Compared to token-compression and training-free sparse baselines, VideoNSA achieves improved performance on long-video understanding, temporal reasoning, and spatial benchmarks. Further ablation analysis reveals four key findings: (1) reliable scaling to 128K tokens; (2) an optimal global-local attention allocation at a fixed budget; (3) task-dependent branch usage patterns; and (4) the learnable combined sparse attention help induce dynamic attention sinks. | [üîó Paper](http://arxiv.org/abs/2510.02295v1) |
| [From Behavioral Performance to Internal Competence: Interpreting
  Vision-Language Models with VLM-Lens](http://arxiv.org/abs/2510.02292v1) | Hala Sheta, Eric Huang, Shuyu Wu, Ilia Alenabi, Jiajun Hong, Ryker Lin, Ruoxi Ning, Daniel Wei, Jialin Yang, Jiawei Zhou, Ziqiao Ma, Freda Shi | 2025-10-02 | Multimodal AI, Training & Evaluation, Model Evaluation, AI Safety | We introduce VLM-Lens, a toolkit designed to enable systematic benchmarking, analysis, and interpretation of vision-language models (VLMs) by supporting the extraction of intermediate outputs from any layer during the forward pass of open-source VLMs. VLM-Lens provides a unified, YAML-configurable interface that abstracts away model-specific complexities and supports user-friendly operation across diverse VLMs. It currently supports 16 state-of-the-art base VLMs and their over 30 variants, and is extensible to accommodate new models without changing the core logic.   The toolkit integrates easily with various interpretability and analysis methods. We demonstrate its usage with two simple analytical experiments, revealing systematic differences in the hidden representations of VLMs across layers and target concepts. VLM-Lens is released as an open-sourced project to accelerate community efforts in understanding and improving VLMs. | [üîó Paper](http://arxiv.org/abs/2510.02292v1) |
| [MultiModal Action Conditioned Video Generation](http://arxiv.org/abs/2510.02287v1) | Yichen Li, Antonio Torralba | 2025-10-02 | Multimodal AI | Current video models fail as world model as they lack fine-graiend control. General-purpose household robots require real-time fine motor control to handle delicate tasks and urgent situations. In this work, we introduce fine-grained multimodal actions to capture such precise control. We consider senses of proprioception, kinesthesia, force haptics, and muscle activation. Such multimodal senses naturally enables fine-grained interactions that are difficult to simulate with text-conditioned generative models. To effectively simulate fine-grained multisensory actions, we develop a feature learning paradigm that aligns these modalities while preserving the unique information each modality provides. We further propose a regularization scheme to enhance causality of the action trajectory features in representing intricate interaction dynamics. Experiments show that incorporating multimodal senses improves simulation accuracy and reduces temporal drift. Extensive ablation studies and downstream applications demonstrate the effectiveness and practicality of our work. | [üîó Paper](http://arxiv.org/abs/2510.02287v1) |
| [Learning to Generate Object Interactions with Physics-Guided Video
  Diffusion](http://arxiv.org/abs/2510.02284v1) | David Romero, Ariana Bermudez, Hao Li, Fabio Pizzati, Ivan Laptev | 2025-10-02 | Multimodal AI, Diffusion Models | Recent models for video generation have achieved remarkable progress and are now deployed in film, social media production, and advertising. Beyond their creative potential, such models also hold promise as world simulators for robotics and embodied decision making. Despite strong advances, however, current approaches still struggle to generate physically plausible object interactions and lack physics-grounded control mechanisms. To address this limitation, we introduce KineMask, an approach for physics-guided video generation that enables realistic rigid body control, interactions, and effects. Given a single image and a specified object velocity, our method generates videos with inferred motions and future object interactions. We propose a two-stage training strategy that gradually removes future motion supervision via object masks. Using this strategy we train video diffusion models (VDMs) on synthetic scenes of simple interactions and demonstrate significant improvements of object interactions in real scenes. Furthermore, KineMask integrates low-level motion control with high-level textual conditioning via predictive scene descriptions, leading to effective support for synthesis of complex dynamical phenomena. Extensive experiments show that KineMask achieves strong improvements over recent models of comparable size. Ablation studies further highlight the complementary roles of low- and high-level conditioning in VDMs. Our code, model, and data will be made publicly available. | [üîó Paper](http://arxiv.org/abs/2510.02284v1) |
| [Self-Forcing++: Towards Minute-Scale High-Quality Video Generation](http://arxiv.org/abs/2510.02283v1) | Justin Cui, Jie Wu, Ming Li, Tao Yang, Xiaojie Li, Rui Wang, Andrew Bai, Yuanhao Ban, Cho-Jui Hsieh | 2025-10-02 | Multimodal AI, Scaling Laws, Diffusion Models, LLM | Diffusion models have revolutionized image and video generation, achieving unprecedented visual quality. However, their reliance on transformer architectures incurs prohibitively high computational costs, particularly when extending generation to long videos. Recent work has explored autoregressive formulations for long video generation, typically by distilling from short-horizon bidirectional teachers. Nevertheless, given that teacher models cannot synthesize long videos, the extrapolation of student models beyond their training horizon often leads to pronounced quality degradation, arising from the compounding of errors within the continuous latent space. In this paper, we propose a simple yet effective approach to mitigate quality degradation in long-horizon video generation without requiring supervision from long-video teachers or retraining on long video datasets. Our approach centers on exploiting the rich knowledge of teacher models to provide guidance for the student model through sampled segments drawn from self-generated long videos. Our method maintains temporal consistency while scaling video length by up to 20x beyond teacher's capability, avoiding common issues such as over-exposure and error-accumulation without recomputing overlapping frames like previous methods. When scaling up the computation, our method shows the capability of generating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the maximum span supported by our base model's position embedding and more than 50x longer than that of our baseline model. Experiments on standard benchmarks and our proposed improved benchmark demonstrate that our approach substantially outperforms baseline methods in both fidelity and consistency. Our long-horizon videos demo can be found at https://self-forcing-plus-plus.github.io/ | [üîó Paper](http://arxiv.org/abs/2510.02283v1) |
| [InfoMosaic-Bench: Evaluating Multi-Source Information Seeking in
  Tool-Augmented Agents](http://arxiv.org/abs/2510.02271v1) | Yaxin Du, Yuanshuo Zhang, Xiyuan Yang, Yifan Zhou, Cheng Wang, Gongyi Zou, Xianghe Pang, Wenhao Wang, Menglan Chen, Shuo Tang, Zhiyu Li, Siheng Chen | 2025-10-02 | Multimodal AI, LLM | Information seeking is a fundamental requirement for humans. However, existing LLM agents rely heavily on open-web search, which exposes two fundamental weaknesses: online content is noisy and unreliable, and many real-world tasks require precise, domain-specific knowledge unavailable from the web. The emergence of the Model Context Protocol (MCP) now allows agents to interface with thousands of specialized tools, seemingly resolving this limitation. Yet it remains unclear whether agents can effectively leverage such tools -- and more importantly, whether they can integrate them with general-purpose search to solve complex tasks. Therefore, we introduce InfoMosaic-Bench, the first benchmark dedicated to multi-source information seeking in tool-augmented agents. Covering six representative domains (medicine, finance, maps, video, web, and multi-domain integration), InfoMosaic-Bench requires agents to combine general-purpose search with domain-specific tools. Tasks are synthesized with InfoMosaic-Flow, a scalable pipeline that grounds task conditions in verified tool outputs, enforces cross-source dependencies, and filters out shortcut cases solvable by trivial lookup. This design guarantees both reliability and non-triviality. Experiments with 14 state-of-the-art LLM agents reveal three findings: (i) web information alone is insufficient, with GPT-5 achieving only 38.2% accuracy and 67.5% pass rate; (ii) domain tools provide selective but inconsistent benefits, improving some domains while degrading others; and (iii) 22.4% of failures arise from incorrect tool usage or selection, highlighting that current LLMs still struggle with even basic tool handling. | [üîó Paper](http://arxiv.org/abs/2510.02271v1) |
| [microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for
  Fine-Grained Image Classification](http://arxiv.org/abs/2510.02270v1) | Sathira Silva, Eman Ali, Chetan Arora, Muhammad Haris Khan | 2025-10-02 | Multimodal AI, Prompt Engineering, LLM, RLHF | Unsupervised adaptation of CLIP-based vision-language models (VLMs) for fine-grained image classification requires sensitivity to microscopic local cues. While CLIP exhibits strong zero-shot transfer, its reliance on coarse global features restricts its performance on fine-grained classification tasks. Prior efforts inject fine-grained knowledge by aligning large language model (LLM) descriptions with the CLIP $\texttt{[CLS]}$ token; however, this approach overlooks spatial precision. We propose $\textbf{microCLIP}$, a self-training framework that jointly refines CLIP's visual and textual representations using fine-grained cues. At its core is Saliency-Oriented Attention Pooling (SOAP) within a lightweight TokenFusion module, which builds a saliency-guided $\texttt{[FG]}$ token from patch embeddings and fuses it with the global $\texttt{[CLS]}$ token for coarse-fine alignment. To stabilize adaptation, we introduce a two-headed LLM-derived classifier: a frozen classifier that, via multi-view alignment, provides a stable text-based prior for pseudo-labeling, and a learnable classifier initialized from LLM descriptions and fine-tuned with TokenFusion. We further develop Dynamic Knowledge Aggregation, which convexly combines fixed LLM/CLIP priors with TokenFusion's evolving logits to iteratively refine pseudo-labels. Together, these components uncover latent fine-grained signals in CLIP, yielding a consistent $2.90\%$ average accuracy gain across 13 fine-grained benchmarks while requiring only light adaptation. Our code is available at https://github.com/sathiiii/microCLIP. | [üîó Paper](http://arxiv.org/abs/2510.02270v1) |
| [NeuroSwift: A Lightweight Cross-Subject Framework for fMRI Visual
  Reconstruction of Complex Scenes](http://arxiv.org/abs/2510.02266v1) | Shiyi Zhang, Dong Liang, Yihang Zhou | 2025-10-02 | Multimodal AI, Optimization, Diffusion Models | Reconstructing visual information from brain activity via computer vision technology provides an intuitive understanding of visual neural mechanisms. Despite progress in decoding fMRI data with generative models, achieving accurate cross-subject reconstruction of visual stimuli remains challenging and computationally demanding. This difficulty arises from inter-subject variability in neural representations and the brain's abstract encoding of core semantic features in complex visual inputs. To address these challenges, we propose NeuroSwift, which integrates complementary adapters via diffusion: AutoKL for low-level features and CLIP for semantics. NeuroSwift's CLIP Adapter is trained on Stable Diffusion generated images paired with COCO captions to emulate higher visual cortex encoding. For cross-subject generalization, we pretrain on one subject and then fine-tune only 17 percent of parameters (fully connected layers) for new subjects, while freezing other components. This enables state-of-the-art performance with only one hour of training per subject on lightweight GPUs (three RTX 4090), and it outperforms existing methods. | [üîó Paper](http://arxiv.org/abs/2510.02266v1) |
## üîπ Optimization

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject
  Fidelity](http://arxiv.org/abs/2510.02315v1) | Eric Tillmann Bill, Enis Simsar, Thomas Hofmann | 2025-10-02 | Optimization, Diffusion Models, RLHF | Text-to-image (T2I) models excel on single-entity prompts but struggle with multi-subject descriptions, often showing attribute leakage, identity entanglement, and subject omissions. We introduce the first theoretical framework with a principled, optimizable objective for steering sampling dynamics toward multi-subject fidelity. Viewing flow matching (FM) through stochastic optimal control (SOC), we formulate subject disentanglement as control over a trained FM sampler. This yields two architecture-agnostic algorithms: (i) a training-free test-time controller that perturbs the base velocity with a single-pass update, and (ii) Adjoint Matching, a lightweight fine-tuning rule that regresses a control network to a backward adjoint signal while preserving base-model capabilities. The same formulation unifies prior attention heuristics, extends to diffusion models via a flow-diffusion correspondence, and provides the first fine-tuning route explicitly designed for multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and Stable Diffusion XL, both algorithms consistently improve multi-subject alignment while maintaining base-model style. Test-time control runs efficiently on commodity GPUs, and fine-tuned controllers trained on limited prompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal Control for Unentangled Subjects), which achieves state-of-the-art multi-subject fidelity across models. | [üîó Paper](http://arxiv.org/abs/2510.02315v1) |
| [KaVa: Latent Reasoning via Compressed KV-Cache Distillation](http://arxiv.org/abs/2510.02312v1) | Anna Kuzina, Maciej Pioro, Paul N. Whatmough, Babak Ehteshami Bejnordi | 2025-10-02 | Optimization, Prompt Engineering | Large Language Models (LLMs) excel at multi-step reasoning problems with explicit chain-of-thought (CoT), but verbose traces incur significant computational costs and memory overhead, and often carry redundant, stylistic artifacts. Latent reasoning has emerged as an efficient alternative that internalizes the thought process, but it suffers from a critical lack of supervision, limiting its effectiveness on complex, natural-language reasoning traces. In this work, we propose KaVa, the first framework that bridges this gap by distilling knowledge directly from a compressed KV-cache of the teacher into a latent-reasoning student via self-distillation, leveraging the representational flexibility of continuous latent tokens to align stepwise KV trajectories. We show that the abstract, unstructured knowledge within compressed KV-cache, which lacks direct token correspondence, can serve as a rich supervisory signal for a latent reasoning student. Empirically, the approach consistently outperforms strong latent baselines, exhibits markedly smaller degradation from equation-only to natural-language traces, and scales to larger backbones while preserving efficiency. These results establish compressed KV-cache distillation as a scalable supervision signal for latent reasoning, combining the accuracy of CoT-trained teachers with the efficiency and deployability of latent inference. | [üîó Paper](http://arxiv.org/abs/2510.02312v1) |
| [Knowledge Distillation Detection for Open-weights Models](http://arxiv.org/abs/2510.02302v1) | Qin Shi, Amber Yijia Zheng, Qifan Song, Raymond A. Yeh | 2025-10-02 | Optimization, Diffusion Models | We propose the task of knowledge distillation detection, which aims to determine whether a student model has been distilled from a given teacher, under a practical setting where only the student's weights and the teacher's API are available. This problem is motivated by growing concerns about model provenance and unauthorized replication through distillation. To address this task, we introduce a model-agnostic framework that combines data-free input synthesis and statistical score computation for detecting distillation. Our approach is applicable to both classification and generative models. Experiments on diverse architectures for image classification and text-to-image generation show that our method improves detection accuracy over the strongest baselines by 59.6% on CIFAR-10, 71.2% on ImageNet, and 20.0% for text-to-image generation. The code is available at https://github.com/shqii1j/distillation_detection. | [üîó Paper](http://arxiv.org/abs/2510.02302v1) |
| [Equilibrium Matching: Generative Modeling with Implicit Energy-Based
  Models](http://arxiv.org/abs/2510.02300v1) | Runqian Wang, Yilun Du | 2025-10-02 | Optimization, Diffusion Models | We introduce Equilibrium Matching (EqM), a generative modeling framework built from an equilibrium dynamics perspective. EqM discards the non-equilibrium, time-conditional dynamics in traditional diffusion and flow-based generative models and instead learns the equilibrium gradient of an implicit energy landscape. Through this approach, we can adopt an optimization-based sampling process at inference time, where samples are obtained by gradient descent on the learned landscape with adjustable step sizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation performance of diffusion/flow models empirically, achieving an FID of 1.90 on ImageNet 256$\times$256. EqM is also theoretically justified to learn and sample from the data manifold. Beyond generation, EqM is a flexible framework that naturally handles tasks including partially noised image denoising, OOD detection, and image composition. By replacing time-conditional velocities with a unified equilibrium landscape, EqM offers a tighter bridge between flow and energy-based models and a simple route to optimization-driven inference. | [üîó Paper](http://arxiv.org/abs/2510.02300v1) |
| [Interactive Training: Feedback-Driven Neural Network Optimization](http://arxiv.org/abs/2510.02297v1) | Wentao Zhang, Yang Young Lu, Yuntian Deng | 2025-10-02 | Optimization, Autonomous Agents | Traditional neural network training typically follows fixed, predefined optimization recipes, lacking the flexibility to dynamically respond to instabilities or emerging training issues. In this paper, we introduce Interactive Training, an open-source framework that enables real-time, feedback-driven intervention during neural network training by human experts or automated AI agents. At its core, Interactive Training uses a control server to mediate communication between users or agents and the ongoing training process, allowing users to dynamically adjust optimizer hyperparameters, training data, and model checkpoints. Through three case studies, we demonstrate that Interactive Training achieves superior training stability, reduced sensitivity to initial hyperparameters, and improved adaptability to evolving user needs, paving the way toward a future training paradigm where AI agents autonomously monitor training logs, proactively resolve instabilities, and optimize training dynamics. | [üîó Paper](http://arxiv.org/abs/2510.02297v1) |
| [BioX-Bridge: Model Bridging for Unsupervised Cross-Modal Knowledge
  Transfer across Biosignals](http://arxiv.org/abs/2510.02276v1) | Chenqi Li, Yu Liu, Timothy Denison, Tingting Zhu | 2025-10-02 | Optimization, RLHF | Biosignals offer valuable insights into the physiological states of the human body. Although biosignal modalities differ in functionality, signal fidelity, sensor comfort, and cost, they are often intercorrelated, reflecting the holistic and interconnected nature of human physiology. This opens up the possibility of performing the same tasks using alternative biosignal modalities, thereby improving the accessibility, usability, and adaptability of health monitoring systems. However, the limited availability of large labeled datasets presents challenges for training models tailored to specific tasks and modalities of interest. Unsupervised cross-modal knowledge transfer offers a promising solution by leveraging knowledge from an existing modality to support model training for a new modality. Existing methods are typically based on knowledge distillation, which requires running a teacher model alongside student model training, resulting in high computational and memory overhead. This challenge is further exacerbated by the recent development of foundation models that demonstrate superior performance and generalization across tasks at the cost of large model sizes. To this end, we explore a new framework for unsupervised cross-modal knowledge transfer of biosignals by training a lightweight bridge network to align the intermediate representations and enable information flow between foundation models and across modalities. Specifically, we introduce an efficient strategy for selecting alignment positions where the bridge should be constructed, along with a flexible prototype network as the bridge architecture. Extensive experiments across multiple biosignal modalities, tasks, and datasets show that BioX-Bridge reduces the number of trainable parameters by 88--99\% while maintaining or even improving transfer performance compared to state-of-the-art methods. | [üîó Paper](http://arxiv.org/abs/2510.02276v1) |
## üîπ Scaling Laws

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Amplified magnetic catalysis in non-Hermitian Euclidean and hyperbolic
  Dirac liquids](http://arxiv.org/abs/2510.02304v1) | Christopher A. Leong, Bitan Roy | 2025-10-02 | Scaling Laws | Due to their iconic linearly vanishing density of states near the zero-energy, half-filled two-dimensional Dirac materials in flat Euclidean and negatively-curved hyperbolic spaces exhibit dynamic mass generation only once a critical interaction strength is surpassed. Application of external magnetic fields onto these systems can, however, trigger the formation of such ordered phases yielding isotropic insulation near the band-center at arbitrarily weak coupling, a phenomenon known as magnetic catalysis. Recently, it has been proposed that a specific type of non-Hermiticity, allowing the system to feature an all-real eigenvalue spectrum otherwise squeezed toward the zero-energy, can bring down the requisite critical coupling of a specific family of ordered phases, commuting class masses, to a desired lower finite value in Dirac systems, a phenomenon known as non-Hermitian catalysis (arXiv:2501.18591). Here, we predict that a confluence of external magnetic fields and such a non-Hermiticity can amplify the magnitude of commuting class masses for subcritical strengths of interactions in Dirac liquids, an emergent phenomenon named non-Hermitian amplification of magnetic catalysis. We anchor this prediction from numerical self-consistent mean-field solutions of the commuting class mass charge-density-wave (antiferromagnetic) order displaying a staggered pattern of average electronic density (magnetization) between the nearest neighboring sites of the half-filled Euclidean honeycomb and hyperbolic {10, 3} and {14, 3} lattices, all featuring emergent non-Hermitian Dirac quasiparticles, after decomposing the nearest-neighbor Coulomb (on-site Hubbard) repulsion in the Hartree channel. We discuss the scaling behavior of these two orders with magnetic field and non-Hermiticity over a wide range of subcritical interactions.. Possible experimental setups to test our predictions are discussed. | [üîó Paper](http://arxiv.org/abs/2510.02304v1) |
| [Monodromy Pinning Defects in the Critical $\mathrm{O}(2N)$ Model](http://arxiv.org/abs/2510.02281v1) | Petr Kravchuk, Alex Radcliffe | 2025-10-02 | Scaling Laws | We investigate a novel class of defects in the critical $\mathrm{O}(2N)$ model that preserve conformal symmetry along the defect, but not the symmetry under rotations transverse to the defect. Instead, they only preserve a combination of transverse rotations and a global symmetry. These defects are constructed as IR fixed points of RG flows originating at monodromy defects, triggered by a relevant operator with non-zero transverse spin. Using large-$N$ and $4-\varepsilon$ expansions, we compute leading-order scaling dimensions of defect operators and the one-point functions of the bulk fields. In various limits this theory coincides with the monodromy defect or the pinning field defect, and we compare our results to existing results for these defects. | [üîó Paper](http://arxiv.org/abs/2510.02281v1) |
| [Parallel Scaling Law: Unveiling Reasoning Generalization through A
  Cross-Linguistic Perspective](http://arxiv.org/abs/2510.02272v1) | Wen Yang, Junhong Wu, Chong Li, Chengqing Zong, Jiajun Zhang | 2025-10-02 | Scaling Laws | Recent advancements in Reinforcement Post-Training (RPT) have significantly enhanced the capabilities of Large Reasoning Models (LRMs), sparking increased interest in the generalization of RL-based reasoning. While existing work has primarily focused on investigating its generalization across tasks or modalities, this study proposes a novel cross-linguistic perspective to investigate reasoning generalization. This raises a crucial question: $\textit{Does the reasoning capability achieved from English RPT effectively transfer to other languages?}$ We address this by systematically evaluating English-centric LRMs on multilingual reasoning benchmarks and introducing a metric to quantify cross-lingual transferability. Our findings reveal that cross-lingual transferability varies significantly across initial model, target language, and training paradigm. Through interventional studies, we find that models with stronger initial English capabilities tend to over-rely on English-specific patterns, leading to diminished cross-lingual generalization. To address this, we conduct a thorough parallel training study. Experimental results yield three key findings: $\textbf{First-Parallel Leap}$, a substantial leap in performance when transitioning from monolingual to just a single parallel language, and a predictable $\textbf{Parallel Scaling Law}$, revealing that cross-lingual reasoning transfer follows a power-law with the number of training parallel languages. Moreover, we identify the discrepancy between actual monolingual performance and the power-law prediction as $\textbf{Monolingual Generalization Gap}$, indicating that English-centric LRMs fail to fully generalize across languages. Our study challenges the assumption that LRM reasoning mirrors human cognition, providing critical insights for the development of more language-agnostic LRMs. | [üîó Paper](http://arxiv.org/abs/2510.02272v1) |
## üîπ Training & Evaluation

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided
  Illusions](http://arxiv.org/abs/2510.02314v1) | Bo-Hsu Ke, You-Zhe Xie, Yu-Lun Liu, Wei-Chen Chiu | 2025-10-02 | Training & Evaluation, Model Evaluation, Responsible AI | 3D scene representation methods like Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have significantly advanced novel view synthesis. As these methods become prevalent, addressing their vulnerabilities becomes critical. We analyze 3DGS robustness against image-level poisoning attacks and propose a novel density-guided poisoning method. Our method strategically injects Gaussian points into low-density regions identified via Kernel Density Estimation (KDE), embedding viewpoint-dependent illusory objects clearly visible from poisoned views while minimally affecting innocent views. Additionally, we introduce an adaptive noise strategy to disrupt multi-view consistency, further enhancing attack effectiveness. We propose a KDE-based evaluation protocol to assess attack difficulty systematically, enabling objective benchmarking for future research. Extensive experiments demonstrate our method's superior performance compared to state-of-the-art techniques. Project page: https://hentci.github.io/stealthattack/ | [üîó Paper](http://arxiv.org/abs/2510.02314v1) |
| [Drawing Conclusions from Draws: Rethinking Preference Semantics in
  Arena-Style LLM Evaluation](http://arxiv.org/abs/2510.02306v1) | Raphael Tang, Crystina Zhang, Wenyan Li, Carmen Lai, Pontus Stenetorp, Yao Lu | 2025-10-02 | Training & Evaluation | In arena-style evaluation of large language models (LLMs), two LLMs respond to a user query, and the user chooses the winning response or deems the "battle" a draw, resulting in an adjustment to the ratings of both models. The prevailing approach for modeling these rating dynamics is to view battles as two-player game matches, as in chess, and apply the Elo rating system and its derivatives. In this paper, we critically examine this paradigm. Specifically, we question whether a draw genuinely means that the two models are equal and hence whether their ratings should be equalized. Instead, we conjecture that draws are more indicative of query difficulty: if the query is too easy, then both models are more likely to succeed equally. On three real-world arena datasets, we show that ignoring rating updates for draws yields a 1-3% relative increase in battle outcome prediction accuracy (which includes draws) for all four rating systems studied. Further analyses suggest that draws occur more for queries rated as very easy and those as highly objective, with risk ratios of 1.37 and 1.35, respectively. We recommend future rating systems to reconsider existing draw semantics and to account for query properties in rating updates. | [üîó Paper](http://arxiv.org/abs/2510.02306v1) |
| [Continual Personalization for Diffusion Models](http://arxiv.org/abs/2510.02296v1) | Yu-Chien Liao, Jr-Jen Chen, Chi-Pin Huang, Ci-Siang Lin, Meng-Lin Wu, Yu-Chiang Frank Wang | 2025-10-02 | Training & Evaluation, Diffusion Models, Ongoing Learning, Prompt Engineering | Updating diffusion models in an incremental setting would be practical in real-world applications yet computationally challenging. We present a novel learning strategy of Concept Neuron Selection (CNS), a simple yet effective approach to perform personalization in a continual learning scheme. CNS uniquely identifies neurons in diffusion models that are closely related to the target concepts. In order to mitigate catastrophic forgetting problems while preserving zero-shot text-to-image generation ability, CNS finetunes concept neurons in an incremental manner and jointly preserves knowledge learned of previous concepts. Evaluation of real-world datasets demonstrates that CNS achieves state-of-the-art performance with minimal parameter adjustments, outperforming previous methods in both single and multi-concept personalization works. CNS also achieves fusion-free operation, reducing memory storage and processing time for continual personalization. | [üîó Paper](http://arxiv.org/abs/2510.02296v1) |
| [Addressing Pitfalls in the Evaluation of Uncertainty Estimation Methods
  for Natural Language Generation](http://arxiv.org/abs/2510.02279v1) | Mykyta Ielanskyi, Kajetan Schweighofer, Lukas Aichberger, Sepp Hochreiter | 2025-10-02 | Training & Evaluation, Model Evaluation, Responsible AI | Hallucinations are a common issue that undermine the reliability of large language models (LLMs). Recent studies have identified a specific subset of hallucinations, known as confabulations, which arise due to predictive uncertainty of LLMs. To detect confabulations, various methods for estimating predictive uncertainty in natural language generation (NLG) have been developed. These methods are typically evaluated by correlating uncertainty estimates with the correctness of generated text, with question-answering (QA) datasets serving as the standard benchmark. However, commonly used approximate correctness functions have substantial disagreement between each other and, consequently, in the ranking of the uncertainty estimation methods. This allows one to inflate the apparent performance of uncertainty estimation methods. We propose using several alternative risk indicators for risk correlation experiments that improve robustness of empirical assessment of UE algorithms for NLG. For QA tasks, we show that marginalizing over multiple LLM-as-a-judge variants leads to reducing the evaluation biases. Furthermore, we explore structured tasks as well as out of distribution and perturbation detection tasks which provide robust and controllable risk indicators. Finally, we propose to use an Elo rating of uncertainty estimation methods to give an objective summarization over extensive evaluation settings. | [üîó Paper](http://arxiv.org/abs/2510.02279v1) |
| [Fine-Grained Urban Traffic Forecasting on Metropolis-Scale Road Networks](http://arxiv.org/abs/2510.02278v1) | Fedor Velikonivtsev, Oleg Platonov, Gleb Bazhenov, Liudmila Prokhorenkova | 2025-10-02 | Training & Evaluation, Scaling Laws, Graph AI | Traffic forecasting on road networks is a complex task of significant practical importance that has recently attracted considerable attention from the machine learning community, with spatiotemporal graph neural networks (GNNs) becoming the most popular approach. The proper evaluation of traffic forecasting methods requires realistic datasets, but current publicly available benchmarks have significant drawbacks, including the absence of information about road connectivity for road graph construction, limited information about road properties, and a relatively small number of road segments that falls short of real-world applications. Further, current datasets mostly contain information about intercity highways with sparsely located sensors, while city road networks arguably present a more challenging forecasting task due to much denser roads and more complex urban traffic patterns. In this work, we provide a more complete, realistic, and challenging benchmark for traffic forecasting by releasing datasets representing the road networks of two major cities, with the largest containing almost 100,000 road segments (more than a 10-fold increase relative to existing datasets). Our datasets contain rich road features and provide fine-grained data about both traffic volume and traffic speed, allowing for building more holistic traffic forecasting systems. We show that most current implementations of neural spatiotemporal models for traffic forecasting have problems scaling to datasets of our size. To overcome this issue, we propose an alternative approach to neural traffic forecasting that uses a GNN without a dedicated module for temporal sequence processing, thus achieving much better scalability, while also demonstrating stronger forecasting performance. We hope our datasets and modeling insights will serve as a valuable resource for research in traffic forecasting. | [üîó Paper](http://arxiv.org/abs/2510.02278v1) |
| [Diffusion^2: Turning 3D Environments into Radio Frequency Heatmaps](http://arxiv.org/abs/2510.02274v1) | Kyoungjun Park, Yifan Yang, Changhan Ge, Lili Qiu, Shiqi Jiang | 2025-10-02 | Training & Evaluation, Optimization, Diffusion Models | Modeling radio frequency (RF) signal propagation is essential for understanding the environment, as RF signals offer valuable insights beyond the capabilities of RGB cameras, which are limited by the visible-light spectrum, lens coverage, and occlusions. It is also useful for supporting wireless diagnosis, deployment, and optimization. However, accurately predicting RF signals in complex environments remains a challenge due to interactions with obstacles such as absorption and reflection. We introduce Diffusion^2, a diffusion-based approach that uses 3D point clouds to model the propagation of RF signals across a wide range of frequencies, from Wi-Fi to millimeter waves. To effectively capture RF-related features from 3D data, we present the RF-3D Encoder, which encapsulates the complexities of 3D geometry along with signal-specific details. These features undergo multi-scale embedding to simulate the actual RF signal dissemination process. Our evaluation, based on synthetic and real-world measurements, demonstrates that Diffusion^2 accurately estimates the behavior of RF signals in various frequency bands and environmental conditions, with an error margin of just 1.9 dB and 27x faster than existing methods, marking a significant advancement in the field. Refer to https://rfvision-project.github.io/ for more information. | [üîó Paper](http://arxiv.org/abs/2510.02274v1) |
## üîπ Model Evaluation

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Do You Know Where Your Camera Is? View-Invariant Policy Learning with
  Camera Conditioning](http://arxiv.org/abs/2510.02268v1) | Tianchong Jiang, Jingtian Ji, Xiangshan Tan, Jiading Fang, Anand Bhattad, Vitor Guizilini, Matthew R. Walter | 2025-10-02 | Model Evaluation, Responsible AI, Diffusion Models | We study view-invariant imitation learning by explicitly conditioning policies on camera extrinsics. Using Plucker embeddings of per-pixel rays, we show that conditioning on extrinsics significantly improves generalization across viewpoints for standard behavior cloning policies, including ACT, Diffusion Policy, and SmolVLA. To evaluate policy robustness under realistic viewpoint shifts, we introduce six manipulation tasks in RoboSuite and ManiSkill that pair "fixed" and "randomized" scene variants, decoupling background cues from camera pose. Our analysis reveals that policies without extrinsics often infer camera pose using visual cues from static backgrounds in fixed scenes; this shortcut collapses when workspace geometry or camera placement shifts. Conditioning on extrinsics restores performance and yields robust RGB-only control without depth. We release the tasks, demonstrations, and code at https://ripl.github.io/know_your_camera/ . | [üîó Paper](http://arxiv.org/abs/2510.02268v1) |
| [Quantum gates in coupled quantum dots controlled by coupling modulation](http://arxiv.org/abs/2510.02267v1) | Alejandro D. Bendersky, Sergio S. Gomez, Rodolfo H. Romero | 2025-10-02 | Model Evaluation, Responsible AI | We studied the dynamics of a pair of single-electron double quantum dots (DQD) under longitudinal and transverse static magnetic fields and time-dependent harmonic modulation of their interaction couplings. We propose to modulate the tunnel coupling between the QDs to produce one-qubit gates and the exchange coupling between DQDs to generate entangling gates, the set of operations required for quantum computing. We developed analytical approximations to set the conditions to control the qubits and applied them to numerical calculations to test the accuracy and robustness of the analytical model. The results shows that the unitary evolution of the two-electron state performs the designed operations even under conditions shifted from the ideal ones. | [üîó Paper](http://arxiv.org/abs/2510.02267v1) |
## üîπ AI Safety

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming
  Attacks](http://arxiv.org/abs/2510.02286v1) | Ruohao Guo, Afshin Oroojlooy, Roshan Sridhar, Miguel Ballesteros, Alan Ritter, Dan Roth | 2025-10-02 | AI Safety, Optimization, Security & Adversarial ML, RLHF | Despite recent rapid progress in AI safety, current large language models remain vulnerable to adversarial attacks in multi-turn interaction settings, where attackers strategically adapt their prompts across conversation turns and pose a more critical yet realistic challenge. Existing approaches that discover safety vulnerabilities either rely on manual red-teaming with human experts or employ automated methods using pre-defined templates and human-curated attack data, with most focusing on single-turn attacks. However, these methods did not explore the vast space of possible multi-turn attacks, failing to consider novel attack trajectories that emerge from complex dialogue dynamics and strategic conversation planning. This gap is particularly critical given recent findings that LLMs exhibit significantly higher vulnerability to multi-turn attacks compared to single-turn attacks. We propose DialTree-RPO, an on-policy reinforcement learning framework integrated with tree search that autonomously discovers diverse multi-turn attack strategies by treating the dialogue as a sequential decision-making problem, enabling systematic exploration without manually curated data. Through extensive experiments, our approach not only achieves more than 25.9% higher ASR across 10 target models compared to previous state-of-the-art approaches, but also effectively uncovers new attack strategies by learning optimal dialogue policies that maximize attack success across multiple turns. | [üîó Paper](http://arxiv.org/abs/2510.02286v1) |
## üîπ Fine-Tuning

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning
  MLLMs and RL](http://arxiv.org/abs/2510.02282v1) | Kyoungjun Park, Yifan Yang, Juheon Yi, Shicheng Zheng, Yifei Shen, Dongqi Han, Caihua Shan, Muhammad Muaz, Lili Qiu | 2025-10-02 | Fine-Tuning, LLM, Multimodal AI, Prompt Engineering, Optimization | With the rapid advancement of AI-generated videos, there is an urgent need for effective detection tools to mitigate societal risks such as misinformation and reputational harm. In addition to accurate classification, it is essential that detection models provide interpretable explanations to ensure transparency for regulators and end users. To address these challenges, we introduce VidGuard-R1, the first video authenticity detector that fine-tunes a multi-modal large language model (MLLM) using group relative policy optimization (GRPO). Our model delivers both highly accurate judgments and insightful reasoning. We curate a challenging dataset of 140k real and AI-generated videos produced by state-of-the-art generation models, carefully designing the generation process to maximize discrimination difficulty. We then fine-tune Qwen-VL using GRPO with two specialized reward models that target temporal artifacts and generation complexity. Extensive experiments demonstrate that VidGuard-R1 achieves state-of-the-art zero-shot performance on existing benchmarks, with additional training pushing accuracy above 95%. Case studies further show that VidGuard-R1 produces precise and interpretable rationales behind its predictions. The code is publicly available at https://VidGuard-R1.github.io. | [üîó Paper](http://arxiv.org/abs/2510.02282v1) |
## üîπ General AI

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Astrophysical Consequences of an Electroweak $\etaw$ Pseudo-Scalar](http://arxiv.org/abs/2510.02310v1) | Hooman Davoudiasl | 2025-10-02 | General AI | Recently, it has been suggested that the spectrum of physical states in the Standard Model may include an ultralight pseudo-scalar, denoted by $\eta_w$, in analogy with the $\eta'$ state arising from the strong interactions. We find that typical expectations for the properties of $\eta_w$ get challenged by astrophysical constraints on the couplings of ultralight bosons. Our strongest limit sets a lower bound of O(1000 TeV) on the decay constant of the hypothesized pseudo-scalar. We also briefly discuss whether $\eta_w$ could be a dark matter candidate, or the origin of dark energy, but conclude that those identifications appear unlikely. Given the important implications of a potentially overlooked $\eta_w$ state for a more complete understanding of the electroweak interactions and a fundamental description of Nature, further theoretical and phenomenological investigations of this possibility and its associated physics are warranted. | [üîó Paper](http://arxiv.org/abs/2510.02310v1) |
| [Robust Tangent Space Estimation via Laplacian Eigenvector Gradient
  Orthogonalization](http://arxiv.org/abs/2510.02308v1) | Dhruv Kohli, Sawyer J. Robertson, Gal Mishne, Alexander Cloninger | 2025-10-02 | General AI | Estimating the tangent spaces of a data manifold is a fundamental problem in data analysis. The standard approach, Local Principal Component Analysis (LPCA), struggles in high-noise settings due to a critical trade-off in choosing the neighborhood size. Selecting an optimal size requires prior knowledge of the geometric and noise characteristics of the data that are often unavailable. In this paper, we propose a spectral method, Laplacian Eigenvector Gradient Orthogonalization (LEGO), that utilizes the global structure of the data to guide local tangent space estimation. Instead of relying solely on local neighborhoods, LEGO estimates the tangent space at each data point by orthogonalizing the gradients of low-frequency eigenvectors of the graph Laplacian. We provide two theoretical justifications of our method. First, a differential geometric analysis on a tubular neighborhood of a manifold shows that gradients of the low-frequency Laplacian eigenfunctions of the tube align closely with the manifold's tangent bundle, while an eigenfunction with high gradient in directions orthogonal to the manifold lie deeper in the spectrum. Second, a random matrix theoretic analysis also demonstrates that low-frequency eigenvectors are robust to sub-Gaussian noise. Through comprehensive experiments, we demonstrate that LEGO yields tangent space estimates that are significantly more robust to noise than those from LPCA, resulting in marked improvements in downstream tasks such as manifold learning, boundary detection, and local intrinsic dimension estimation. | [üîó Paper](http://arxiv.org/abs/2510.02308v1) |
| [Effective Brauer-Siegel theorems for Artin $L$-functions](http://arxiv.org/abs/2510.02309v1) | Peter J. Cho, Robert J. Lemke Oliver, Asif Zaman | 2025-10-02 | General AI | Given a number field $K \neq \mathbb{Q}$, in a now classic work, Stark pinpointed the possible source of a so-called Landau-Siegel zero of the Dedekind zeta function $\zeta_K(s)$ and used this to give effective upper and lower bounds on the residue of $\zeta_K(s)$ at $s=1$. We extend Stark's work to give effective upper and lower bounds for the leading term of the Laurent expansion of general Artin $L$-functions at $s=1$ that are, up to the value of implied constants, as strong as could reasonably be expected given current progress toward the generalized Riemann hypothesis. Our bounds are completely unconditional, and rely on no unproven hypotheses about Artin $L$-functions. | [üîó Paper](http://arxiv.org/abs/2510.02309v1) |
| [ALMA Deep Field in SSA22: Reconstructed [CII] Luminosity Function at z =
  6](http://arxiv.org/abs/2510.02303v1) | Natsuki H. Hayatsu, Rob J. Ivison, Paola Andreani, Fabrizia Guglielmetti, Zhi-Yu Zhang, Andy Biggs, Hideki Umehata, Yuichi Matsuda, Naoki Yoshida, Mark A. Swinbank, Kotaro Kohno, Yoichi Tamura, Bunyo Hatsukade, Kouichiro Nakanishi, Yiping Ao, Tohru Nagao, Mariko Kubo, Tsutomu T. Takeuchi, Minju Lee, Takuma Izumi, Soh Ikarashi, Tohru Yamada | 2025-10-02 | General AI | The ADF22 line survey reported detections of two high-$z$ line-emitting source candidates above 6-$\sigma$, both of which were shown to be spurious after follow-up observations. We investigate the detectability of far-infrared emitters in ALMA deep fields using mock observations by injecting artificial line-emitting sources into the visibility planes. We also discuss our investigation, conducted together with the ALMA operations team, of a possible technical problem in the original observations. Finally, we devise a method to estimate the [CII] luminosity function (LF) at $z \sim 6$, including a full analysis of signal contamination and sample completeness.   The comparison of pixel distributions between the real and mock datacubes does not show significant differences, confirming that the effect of non-Gaussian noise is negligible for the ADF22 datacube. Using 100 blank mock-mosaic datasets, we show 0.43 $\pm$ 0.67 false detections per datacube with the previous source-finding method. We argue that the underestimation of the contamination rate in the previous work is caused by the smaller number of datacubes, using only 4 real ADF22 datacubes. We compare the results of clump-finding between the time division mode and frequency division mode correlator datacubes and confirm that the velocity widths of the clumps in the TDM case are up to 3 times wider than in the FDM case.   The LF estimation using our model shows that a correction for the number count is required, up to one order of magnitude, in the luminosity range of $\geq 5 \times 10^8 L_\odot$. Our reconstruction method for the line LF can be applied to future blind line surveys. | [üîó Paper](http://arxiv.org/abs/2510.02303v1) |
| [Relativistic Jets and Winds in Radio-Identified Supermassive Black Hole
  Binary Candidates](http://arxiv.org/abs/2510.02301v1) | Andrew G. Sullivan, Roger D. Blandford, Anna Synani, Philipe V. de la Parra, No√©mie Globus, Mitchell C. Begelman, Anthony C. S. Readhead | 2025-10-02 | General AI | Supermassive black hole binary systems (SMBHBs) are thought to emit the recently discovered nHz gravitational wave background; however, not a single individual nHz source has been confirmed to date. Long-term radio-monitoring at the Owens Valley Radio Observatory has revealed two potential SMBHB candidates: blazars PKS 2131-021 and PKS J0805-0111. These sources show periodic flux density variations across the electromagnetic spectrum, signaling the presence of a good clock. To explain the emission, we propose a generalizable jet model, where a mildly relativistic wind creates an outward-moving helical channel, along which the ultra-relativistic jet propagates. The observed flux variation from the jet is mostly due to aberration. The emission at lower frequency arises at larger radius and its variation is consequently delayed, as observed. Our model reproduces the main observable features of both sources and can be applied to other sources as they are discovered. We make predictions for radio polarization, direct imaging, and emission line variation, which can be tested with forthcoming observations. Our results motivate future numerical simulations of jetted SMBHB systems and have implications for the fueling, structure, and evolution of blazar jets. | [üîó Paper](http://arxiv.org/abs/2510.02301v1) |
| [Uniqueness in the Plateau problem for calibrated currents](http://arxiv.org/abs/2510.02299v1) | Bryan Dimler, Chen-Kuan Lee | 2025-10-02 | General AI | We show that every compactly supported calibrated integral current with connected $C^{3,\alpha}$ boundary is the unique solution to the oriented Plateau problem for its boundary data. This is proved as a consequence of the boundary regularity theory for area-minimizing currents and classical unique continuation principles adapted to the minimal surface system. | [üîó Paper](http://arxiv.org/abs/2510.02299v1) |
| [ARMADA: Autonomous Online Failure Detection and Human Shared Control
  Empower Scalable Real-world Deployment and Adaptation](http://arxiv.org/abs/2510.02298v1) | Wenye Yu, Jun Lv, Zixi Ying, Yang Jin, Chuan Wen, Cewu Lu | 2025-10-02 | General AI | Imitation learning has shown promise in learning from large-scale real-world datasets. However, pretrained policies usually perform poorly without sufficient in-domain data. Besides, human-collected demonstrations entail substantial labour and tend to encompass mixed-quality data and redundant information. As a workaround, human-in-the-loop systems gather domain-specific data for policy post-training, and exploit closed-loop policy feedback to offer informative guidance, but usually require full-time human surveillance during policy rollout. In this work, we devise ARMADA, a multi-robot deployment and adaptation system with human-in-the-loop shared control, featuring an autonomous online failure detection method named FLOAT. Thanks to FLOAT, ARMADA enables paralleled policy rollout and requests human intervention only when necessary, significantly reducing reliance on human supervision. Hence, ARMADA enables efficient acquisition of in-domain data, and leads to more scalable deployment and faster adaptation to new scenarios. We evaluate the performance of ARMADA on four real-world tasks. FLOAT achieves nearly 95% accuracy on average, surpassing prior state-of-the-art failure detection approaches by over 20%. Besides, ARMADA manifests more than 4$\times$ increase in success rate and greater than 2$\times$ reduction in human intervention rate over multiple rounds of policy rollout and post-training, compared to previous human-in-the-loop learning methods. | [üîó Paper](http://arxiv.org/abs/2510.02298v1) |
| [F2LLM Technical Report: Matching SOTA Embedding Performance with 6
  Million Open-Source Data](http://arxiv.org/abs/2510.02294v1) | Ziyin Zhang, Zihan Liao, Hang Yu, Peng Di, Rui Wang | 2025-10-02 | General AI | We introduce F2LLM - Foundation to Feature Large Language Models, a suite of state-of-the-art embedding models in three sizes: 0.6B, 1.7B, and 4B. Unlike previous top-ranking embedding models that require massive contrastive pretraining, sophisticated training pipelines, and costly synthetic training data, F2LLM is directly finetuned from foundation models on 6 million query-document-negative tuples curated from open-source, non-synthetic datasets, striking a strong balance between training cost, model size, and embedding performance. On the MTEB English leaderboard, F2LLM-4B ranks 2nd among models with approximately 4B parameters and 7th overall, while F2LLM-1.7B ranks 1st among models in the 1B-2B size range. To facilitate future research in the field, we release the models, training dataset, and code, positioning F2LLM as a strong, reproducible, and budget-friendly baseline for future works. | [üîó Paper](http://arxiv.org/abs/2510.02294v1) |
| [Theoretical Exploration of the Diene-Transmissive Hetero-Diels-Alder
  Strategy Toward Boron-Functionalized Octahydroquinolines](http://arxiv.org/abs/2510.02293v1) | Amine Rafik, Abdeljabbar Jaddi, Khalid Abbiche, Mohammed Salah, Miguel Carvajal, Khadija Marakchi | 2025-10-02 | General AI | A diene-transmissive hetero-Diels-Alder strategy, grounded in previous experimental works and employing boronated dienophiles, is proposed for the synthesis of boron-bearing octahydroquinolines. To assess its feasibility, three representative reactions were investigated, and their thermodynamics were evaluated in toluene and acetonitrile at various temperatures using the WB97X-D level of theory. The peri-, regio-, stereo-, and $\pi$-facial selectivities were predicted. The reactions mechanisms were elucidated through exploration of the reaction pathways. The predictions are consistent with available experimental work, and show the reactions are feasible with low to moderate polarity. The results also demonstrate that the reactions selectivity can in some cases be tuned by judicious choice of reaction conditions to deliver specific products with high selectivity. | [üîó Paper](http://arxiv.org/abs/2510.02293v1) |
| [Beyond Belief Propagation: Cluster-Corrected Tensor Network Contraction
  with Exponential Convergence](http://arxiv.org/abs/2510.02290v1) | Siddhant Midha, Yifan F. Zhang | 2025-10-02 | General AI | Tensor network contraction on arbitrary graphs is a fundamental computational challenge with applications ranging from quantum simulation to error correction. While belief propagation (BP) provides a powerful approximation algorithm for this task, its accuracy limitations are poorly understood and systematic improvements remain elusive. Here, we develop a rigorous theoretical framework for BP in tensor networks, leveraging insights from statistical mechanics to devise a \emph{cluster expansion} that systematically improves the BP approximation. We prove that the cluster expansion converges exponentially fast if an object called the \emph{loop contribution} decays sufficiently fast with the loop size, giving a rigorous error bound on BP. We also provide a simple and efficient algorithm to compute the cluster expansion to arbitrary order. We demonstrate the efficacy of our method on the two-dimensional Ising model, where we find that our method significantly improves upon BP and existing corrective algorithms such as loop series expansion. Our work opens the door to a systematic theory of BP for tensor networks and its applications in decoding classical and quantum error-correcting codes and simulating quantum systems. | [üîó Paper](http://arxiv.org/abs/2510.02290v1) |
| [Charge order through crystallization of Frenkel excitons: realization in
  kagome metals](http://arxiv.org/abs/2510.02289v1) | Ruoshi Jiang, Bartomeu Monserrat, Wei Ku | 2025-10-02 | General AI | Charge order is a widely observed and representative example of spontaneous broken symmetries in quantum states of matter. Owing to the large intra-atomic Coulomb energy, the charge redistribution in such an order typically implies significant alteration of the electronic and lattice properties of materials. While the standard description of charge order, namely a "charge density wave" instability of the Fermi surface, has been broadly and successfully applied to good metals, its applicability to correlated ionic materials has been rather limited. Here, we propose an alternative general scenario of charge order - crystallization of long-lived Frenkel excitons - suitable for these ionic materials. We demonstrate this scenario on the recently discovered kagome superconductors and successfully reproduce all the characteristics of experimental observations on both local charge correlations and long-range ordering. The proposed generic scenario offers a long-sought understanding of charge order applicable to modern correlated functional materials. | [üîó Paper](http://arxiv.org/abs/2510.02289v1) |
| [Optimal Lieb-Thirring type inequalities for Schr√∂dinger and Jacobi
  operators with complex potentials](http://arxiv.org/abs/2510.02288v1) | Sabine B√∂gli, Sukrid Petpradittha | 2025-10-02 | General AI | We prove optimal Lieb-Thirring type inequalities for Schr\"odinger and Jacobi operators with complex potentials. Our results bound eigenvalue power sums (Riesz means) by the $L^p$ norm of the potential, where in contrast to the self-adjoint case, each term needs to be weighted by a function of the ratio of the distance of the eigenvalue to the essential spectrum and the distance to the endpoint(s) thereof. Our Lieb-Thirring type bounds only hold for integrable weight functions. To prove optimality, we establish divergence estimates for non-integrable weight functions. The divergence rates exhibit a logarithmic or even polynomial gain compared to semiclassical methods (Weyl asymptotics) for real potentials. | [üîó Paper](http://arxiv.org/abs/2510.02288v1) |
| [Markov chains on Weyl groups from the geometry of the flag variety](http://arxiv.org/abs/2510.02285v1) | Persi Diaconis, Calder Morton-Ferguson | 2025-10-02 | General AI | This paper studies a basic Markov chain, the Burnside process, on the space of flags $G/B$ with $G = GL_n(\mathbb{F}_q)$ and $B$ its upper triangular matrices. This gives rise to a shuffling: a Markov chain on the symmetric group realized via the Bruhat decomposition. Actually running and describing this Markov chain requires understanding Springer fibers and the Steinberg variety. The main results give a practical algorithm for all n and q and determine the limiting behavior of the chain when q is large. In describing this behavior, we find interesting connections to the combinatorics of the Robinson-Schensted correspondence and to the geometry of orbital varieties. The construction and description is then carried over to finite Chevalley groups of arbitrary type, describing a new class of Markov chains on Weyl groups. | [üîó Paper](http://arxiv.org/abs/2510.02285v1) |
| [An efficient quantum algorithm for computing $S$-units and its
  applications](http://arxiv.org/abs/2510.02280v1) | Jean-Francois Biasse, Fang Song | 2025-10-02 | General AI | In this paper, we provide details on the proofs of the quantum polynomial time algorithm of Biasse and Song (SODA 16) for computing the $S$-unit group of a number field. This algorithm directly implies polynomial time methods to calculate class groups, S-class groups, relative class group and the unit group, ray class groups, solve the principal ideal problem, solve certain norm equations, and decompose ideal classes in the ideal class group. Additionally, combined with a result of Cramer, Ducas, Peikert and Regev (Eurocrypt 2016), the resolution of the principal ideal problem allows one to find short generators of a principal ideal. Likewise, methods due to Cramer, Ducas and Wesolowski (Eurocrypt 2017) use the resolution of the principal ideal problem and the decomposition of ideal classes to find so-called ``mildly short vectors'' in ideal lattices of cyclotomic fields. | [üîó Paper](http://arxiv.org/abs/2510.02280v1) |
| [How to invert well-pointed endofunctors](http://arxiv.org/abs/2510.02277v1) | Matt Booth | 2025-10-02 | General AI | In this short note we observe that Kelly's transfinite construction of free algebras yields a way to invert well-pointed endofunctors. In enriched settings, this recovers constructions of Keller, Seidel, and Chen-Wang. We also relate this procedure to localisation by spectra and to Heller's stabilisation. | [üîó Paper](http://arxiv.org/abs/2510.02277v1) |
| [Lower bounds on the complexity of preparing mixed states](http://arxiv.org/abs/2510.02275v1) | Max McGinley, Samuel J. Garratt | 2025-10-02 | General AI | We establish a relationship between the correlations in a many-qubit mixed state and the minimum circuit depth needed for its preparation. If the mutual information between two subsystems exceeds the mutual information between one of those subsystems and the environment, which purifies the mixed state of the system, then the past lightcones of the subsystems must intersect one another. This results in a lower bound on the circuit depth of any ensemble of geometrically local unitaries that prepares the state to some specified degree of approximation. As an application, we derive lower bounds on the circuit depth needed to prepare thermal states of one-dimensional quantum critical systems described by conformal field theory, showing that the depth diverges as temperature is decreased up to a cutoff set by the preparation error. | [üîó Paper](http://arxiv.org/abs/2510.02275v1) |
| [A note on Poisson summation for GL(2)](http://arxiv.org/abs/2510.02273v1) | Tian An Wong | 2025-10-02 | General AI | Using analytic number theory techniques, Altu\u{g} showed that the contribution of the trivial representation to the Arthur-Selberg trace formula for GL(2) over $\Q$ could be cancelled by applying a modified Poisson summation formula to the regular elliptic contribution. Drawing on recent works, we re-examine these methods from an adelic perspective. | [üîó Paper](http://arxiv.org/abs/2510.02273v1) |
| [Game-theoretic Social Distancing in Competitive Bi-Virus SIS Epidemics](http://arxiv.org/abs/2510.02269v1) | Benjamin Catalano, Keith Paarporn, Sebin Gracy | 2025-10-02 | General AI | Numerous elements drive the spread of infectious diseases in complex real-world networks. Of particular interest is social behaviors that evolve in tandem with the spread of disease. Moreover, recent studies highlight the importance of understanding how multiple strains spread simultaneously through a population (e.g. Delta and Omicron variants of SARS-CoV-2). In this paper, we propose a bi-virus SIS epidemic model coupled with a game-theoretic social distancing behavior model. The behaviors are governed by replicator equations from evolutionary game theory. The prevalence of each strain impacts the choice of an individual to social distance, and, in turn, their behavior affects the spread of each virus in the SIS model. Our analysis identifies equilibria of the system and their local stability properties, which reveal several isolated fixed points with varying levels of social distancing. We find that endemic co-existence is possible only when the reproduction numbers of both strains are equal. Assuming the reproduction number for each virus is the same, we identify suitable parameter regimes that give rise to lines of coexistence equilibria. Moreover, we also identify conditions for local exponential stability of said lines of equilibria. We illustrate our findings with several numerical simulations. | [üîó Paper](http://arxiv.org/abs/2510.02269v1) |
