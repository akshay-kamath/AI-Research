# üìå AI Research Papers (April07 to April13)

## üîπ LLM

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [C3PO: Critical-Layer, Core-Expert, Collaborative Pathway Optimization
  for Test-Time Expert Re-Mixing](http://arxiv.org/abs/2504.07964v1) | Zhongyang Li, Ziyue Li, Tianyi Zhou | 2025-04-10 | LLM, Training & Evaluation, Optimization, Scaling Laws | Mixture-of-Experts (MoE) Large Language Models (LLMs) suffer from severely sub-optimal expert pathways-our study reveals that naive expert selection learned from pretraining leaves a surprising 10-20% accuracy gap for improvement. Motivated by this observation, we develop a novel class of test-time optimization methods to re-weight or "re-mixing" the experts in different layers jointly for each test sample. Since the test sample's ground truth is unknown, we propose to optimize a surrogate objective defined by the sample's "successful neighbors" from a reference set of samples. We introduce three surrogates and algorithms based on mode-finding, kernel regression, and the average loss of similar reference samples/tasks. To reduce the cost of optimizing whole pathways, we apply our algorithms merely to the core experts' mixing weights in critical layers, which enjoy similar performance but save significant computation. This leads to "Critical-Layer, Core-Expert, Collaborative Pathway Optimization (C3PO)". We apply C3PO to two recent MoE LLMs and examine it on six widely-used benchmarks. It consistently improves the base model by 7-15% in accuracy and outperforms widely used test-time learning baselines, e.g., in-context learning and prompt/prefix tuning, by a large margin. Moreover, C3PO enables MoE LLMs with 1-3B active parameters to outperform LLMs of 7-9B parameters, hence improving MoE's advantages on efficiency. Our thorough ablation study further sheds novel insights on achieving test-time improvement on MoE. | [üîó Paper](http://arxiv.org/abs/2504.07964v1) |
| [GLUS: Global-Local Reasoning Unified into A Single Large Language Model
  for Video Segmentation](http://arxiv.org/abs/2504.07962v1) | Lang Lin, Xueyang Yu, Ziqi Pang, Yu-Xiong Wang | 2025-04-10 | LLM, Fine-Tuning, Multimodal AI, Memory & Context Length | This paper proposes a novel framework utilizing multi-modal large language models (MLLMs) for referring video object segmentation (RefVOS). Previous MLLM-based methods commonly struggle with the dilemma between "Ref" and "VOS": they either specialize in understanding a few key frames (global reasoning) or tracking objects on continuous frames (local reasoning), and rely on external VOS or frame selectors to mitigate the other end of the challenge. However, our framework GLUS shows that global and local consistency can be unified into a single video segmentation MLLM: a set of sparse "context frames" provides global information, while a stream of continuous "query frames" conducts local object tracking. This is further supported by jointly training the MLLM with a pre-trained VOS memory bank to simultaneously digest short-range and long-range temporal information. To improve the information efficiency within the limited context window of MLLMs, we introduce object contrastive learning to distinguish hard false-positive objects and a self-refined framework to identify crucial frames and perform propagation. By collectively integrating these insights, our GLUS delivers a simple yet effective baseline, achieving new state-of-the-art for MLLMs on the MeViS and Ref-Youtube-VOS benchmark. Our project page is at https://glus-video.github.io/. | [üîó Paper](http://arxiv.org/abs/2504.07962v1) |
| [MM-IFEngine: Towards Multimodal Instruction Following](http://arxiv.org/abs/2504.07957v1) | Shengyuan Ding, Shenxi Wu, Xiangyu Zhao, Yuhang Zang, Haodong Duan, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Dahua Lin, Jiaqi Wang | 2025-04-10 | LLM, Multimodal AI, Optimization, Fine-Tuning, Training & Evaluation | The Instruction Following (IF) ability measures how well Multi-modal Large Language Models (MLLMs) understand exactly what users are telling them and whether they are doing it right. Existing multimodal instruction following training data is scarce, the benchmarks are simple with atomic instructions, and the evaluation strategies are imprecise for tasks demanding exact output constraints. To address this, we present MM-IFEngine, an effective pipeline to generate high-quality image-instruction pairs. Our MM-IFEngine pipeline yields large-scale, diverse, and high-quality training data MM-IFInstruct-23k, which is suitable for Supervised Fine-Tuning (SFT) and extended as MM-IFDPO-23k for Direct Preference Optimization (DPO). We further introduce MM-IFEval, a challenging and diverse multi-modal instruction-following benchmark that includes (1) both compose-level constraints for output responses and perception-level constraints tied to the input images, and (2) a comprehensive evaluation pipeline incorporating both rule-based assessment and judge model. We conduct SFT and DPO experiments and demonstrate that fine-tuning MLLMs on MM-IFInstruct-23k and MM-IFDPO-23k achieves notable gains on various IF benchmarks, such as MM-IFEval (+10.2$\%$), MIA (+7.6$\%$), and IFEval (+12.3$\%$). The full data and evaluation code will be released on https://github.com/SYuan03/MM-IFEngine. | [üîó Paper](http://arxiv.org/abs/2504.07957v1) |
| [Dynamic Cheatsheet: Test-Time Learning with Adaptive Memory](http://arxiv.org/abs/2504.07952v1) | Mirac Suzgun, Mert Yuksekgonul, Federico Bianchi, Dan Jurafsky, James Zou | 2025-04-10 | LLM, RLHF | Despite their impressive performance on complex tasks, current language models (LMs) typically operate in a vacuum: Each input query is processed separately, without retaining insights from previous attempts. Here, we present Dynamic Cheatsheet (DC), a lightweight framework that endows a black-box LM with a persistent, evolving memory. Rather than repeatedly re-discovering or re-committing the same solutions and mistakes, DC enables models to store and reuse accumulated strategies, code snippets, and general problem-solving insights at inference time. This test-time learning enhances performance substantially across a range of tasks without needing explicit ground-truth labels or human feedback. Leveraging DC, Claude 3.5 Sonnet's accuracy more than doubled on AIME math exams once it began retaining algebraic insights across questions. Similarly, GPT-4o's success rate on Game of 24 increased from 10% to 99% after the model discovered and reused a Python-based solution. In tasks prone to arithmetic mistakes, such as balancing equations, DC enabled GPT-4o and Claude to reach near-perfect accuracy by recalling previously validated code, whereas their baselines stagnated around 50%. Beyond arithmetic challenges, DC yields notable accuracy gains on knowledge-demanding tasks. Claude achieved a 9% improvement in GPQA-Diamond and an 8% boost on MMLU-Pro problems. Crucially, DC's memory is self-curated, focusing on concise, transferable snippets rather than entire transcript. Unlike finetuning or static retrieval methods, DC adapts LMs' problem-solving skills on the fly, without modifying their underlying parameters. Overall, our findings present DC as a promising approach for augmenting LMs with persistent memory, bridging the divide between isolated inference events and the cumulative, experience-driven learning characteristic of human cognition. | [üîó Paper](http://arxiv.org/abs/2504.07952v1) |
## üîπ Diffusion Models

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [PixelFlow: Pixel-Space Generative Models with Flow](http://arxiv.org/abs/2504.07963v1) | Shoufa Chen, Chongjian Ge, Shilong Zhang, Peize Sun, Ping Luo | 2025-04-10 | Diffusion Models | We present PixelFlow, a family of image generation models that operate directly in the raw pixel space, in contrast to the predominant latent-space models. This approach simplifies the image generation process by eliminating the need for a pre-trained Variational Autoencoder (VAE) and enabling the whole model end-to-end trainable. Through efficient cascade flow modeling, PixelFlow achieves affordable computation cost in pixel space. It achieves an FID of 1.98 on 256$\times$256 ImageNet class-conditional image generation benchmark. The qualitative text-to-image results demonstrate that PixelFlow excels in image quality, artistry, and semantic control. We hope this new paradigm will inspire and open up new opportunities for next-generation visual generation models. Code and models are available at https://github.com/ShoufaChen/PixelFlow. | [üîó Paper](http://arxiv.org/abs/2504.07963v1) |
| [Pushing the Accuracy Limit of Foundation Neural Network Models with
  Quantum Monte Carlo Forces and Path Integrals](http://arxiv.org/abs/2504.07948v2) | Anouar Benali, Thomas Pl√©, Olivier Adjoua, Valay Agarawal, Thomas Applencourt, Marharyta Blazhynska, Raymond Clay III, Kevin Gasperich, Khalid Hossain, Jeongnim Kim, Christopher Knight, Jaron T. Krogel, Yvon Maday, Maxime Maria, Matthieu Montes, Ye Luo, Evgeny Posenitskiy, Corentin Villot, Venkat Vishwanath, Louis Lagard√®re, Jean-Philip Piquemal | 2025-04-10 | Diffusion Models | We propose an end-to-end integrated strategy to produce highly accurate quantum chemistry (QC) synthetic datasets (energies and forces) aimed at deriving Foundation Machine Learning models for molecular simulation. Starting from Density Functional Theory (DFT), a "Jacob's Ladder" approach leverages computationally-optimized layers of massively parallel GPU-accelerated software with increasing accuracy. Thanks to Exascale, this is the first time that the computationally intensive calculation of Diffusion Quantum Monte Carlo (QMC) forces, and the combination of multi-determinant QMC energies and forces with selected-CI wavefunctions, are computed at such scale at the complete basis-set-limit. To bridge the gap between accurate QC and condensed-phase molecular dynamics, we leverage transfer learning to improve the FeNNix-Bio1 DFT-based foundation model. The resulting approach is coupled to path integrals adaptive sampling quantum dynamics to perform nanosecond reactive simulations at unprecedented accuracy. These results demonstrate the promise of Exascale to deepen our understanding of the inner machinery of complex biosystems. | [üîó Paper](http://arxiv.org/abs/2504.07948v2) |
## üîπ RLHF

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Cat, Rat, Meow: On the Alignment of Language Model and Human
  Term-Similarity Judgments](http://arxiv.org/abs/2504.07965v1) | Lorenz Linhardt, Tom Neuh√§user, Lenka Tƒõtkov√°, Oliver Eberle | 2025-04-10 | RLHF, Training & Evaluation | Small and mid-sized generative language models have gained increasing attention. Their size and availability make them amenable to being analyzed at a behavioral as well as a representational level, allowing investigations of how these levels interact. We evaluate 32 publicly available language models for their representational and behavioral alignment with human similarity judgments on a word triplet task. This provides a novel evaluation setting to probe semantic associations in language beyond common pairwise comparisons. We find that (1) even the representations of small language models can achieve human-level alignment, (2) instruction-tuned model variants can exhibit substantially increased agreement, (3) the pattern of alignment across layers is highly model dependent, and (4) alignment based on models' behavioral responses is highly dependent on model size, matching their representational alignment only for the largest evaluated models. | [üîó Paper](http://arxiv.org/abs/2504.07965v1) |
| [Geo4D: Leveraging Video Generators for Geometric 4D Scene Reconstruction](http://arxiv.org/abs/2504.07961v1) | Zeren Jiang, Chuanxia Zheng, Iro Laina, Diane Larlus, Andrea Vedaldi | 2025-04-10 | RLHF, Prompt Engineering, Multimodal AI, Diffusion Models | We introduce Geo4D, a method to repurpose video diffusion models for monocular 3D reconstruction of dynamic scenes. By leveraging the strong dynamic prior captured by such video models, Geo4D can be trained using only synthetic data while generalizing well to real data in a zero-shot manner. Geo4D predicts several complementary geometric modalities, namely point, depth, and ray maps. It uses a new multi-modal alignment algorithm to align and fuse these modalities, as well as multiple sliding windows, at inference time, thus obtaining robust and accurate 4D reconstruction of long videos. Extensive experiments across multiple benchmarks show that Geo4D significantly surpasses state-of-the-art video depth estimation methods, including recent methods such as MonST3R, which are also designed to handle dynamic scenes. | [üîó Paper](http://arxiv.org/abs/2504.07961v1) |
| [Perception-R1: Pioneering Perception Policy with Reinforcement Learning](http://arxiv.org/abs/2504.07954v1) | En Yu, Kangheng Lin, Liang Zhao, Jisheng Yin, Yana Wei, Yuang Peng, Haoran Wei, Jianjian Sun, Chunrui Han, Zheng Ge, Xiangyu Zhang, Daxin Jiang, Jingyu Wang, Wenbing Tao | 2025-04-10 | RLHF | Inspired by the success of DeepSeek-R1, we explore the potential of rule-based reinforcement learning (RL) in MLLM post-training for perception policy learning. While promising, our initial experiments reveal that incorporating a thinking process through RL does not consistently lead to performance gains across all visual perception tasks. This leads us to delve into the essential role of RL in the context of visual perception. In this work, we return to the fundamentals and explore the effects of RL on different perception tasks. We observe that the perceptual complexity is a major factor in determining the effectiveness of RL. We also observe that reward design plays a crucial role in further approching the upper limit of model perception. To leverage these findings, we propose Perception-R1, a scalable RL framework using GRPO during MLLM post-training. With a standard Qwen2.5-VL-3B-Instruct, Perception-R1 achieves +4.2% on RefCOCO+, +17.9% on PixMo-Count, +4.2% on PageOCR, and notably, 31.9% AP on COCO2017 val for the first time, establishing a strong baseline for perception policy learning. | [üîó Paper](http://arxiv.org/abs/2504.07954v1) |
## üîπ Multimodal AI

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [BoxDreamer: Dreaming Box Corners for Generalizable Object Pose
  Estimation](http://arxiv.org/abs/2504.07955v1) | Yuanhong Yu, Xingyi He, Chen Zhao, Junhao Yu, Jiaqi Yang, Ruizhen Hu, Yujun Shen, Xing Zhu, Xiaowei Zhou, Sida Peng | 2025-04-10 | Multimodal AI | This paper presents a generalizable RGB-based approach for object pose estimation, specifically designed to address challenges in sparse-view settings. While existing methods can estimate the poses of unseen objects, their generalization ability remains limited in scenarios involving occlusions and sparse reference views, restricting their real-world applicability. To overcome these limitations, we introduce corner points of the object bounding box as an intermediate representation of the object pose. The 3D object corners can be reliably recovered from sparse input views, while the 2D corner points in the target view are estimated through a novel reference-based point synthesizer, which works well even in scenarios involving occlusions. As object semantic points, object corners naturally establish 2D-3D correspondences for object pose estimation with a PnP algorithm. Extensive experiments on the YCB-Video and Occluded-LINEMOD datasets show that our approach outperforms state-of-the-art methods, highlighting the effectiveness of the proposed representation and significantly enhancing the generalization capabilities of object pose estimation, which is crucial for real-world applications. | [üîó Paper](http://arxiv.org/abs/2504.07955v1) |
| [Scaling Laws for Native Multimodal Models](http://arxiv.org/abs/2504.07951v2) | Mustafa Shukor, Enrico Fini, Victor Guilherme Turrisi da Costa, Matthieu Cord, Joshua Susskind, Alaaeldin El-Nouby | 2025-04-10 | Multimodal AI, Scaling Laws | Building general-purpose models that can effectively perceive the world through multimodal signals has been a long-standing goal. Current approaches involve integrating separately pre-trained components, such as connecting vision encoders to LLMs and continuing multimodal training. While such approaches exhibit remarkable sample efficiency, it remains an open question whether such late-fusion architectures are inherently superior. In this work, we revisit the architectural design of native multimodal models (NMMs)--those trained from the ground up on all modalities--and conduct an extensive scaling laws study, spanning 457 trained models with different architectures and training mixtures. Our investigation reveals no inherent advantage to late-fusion architectures over early-fusion ones, which do not rely on image encoders. On the contrary, early-fusion exhibits stronger performance at lower parameter counts, is more efficient to train, and is easier to deploy. Motivated by the strong performance of the early-fusion architectures, we show that incorporating Mixture of Experts (MoEs) allows for models that learn modality-specific weights, significantly enhancing performance. | [üîó Paper](http://arxiv.org/abs/2504.07951v2) |
## üîπ Training & Evaluation

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [VisualCloze: A Universal Image Generation Framework via Visual
  In-Context Learning](http://arxiv.org/abs/2504.07960v1) | Zhong-Yu Li, Ruoyi Du, Juncheng Yan, Le Zhuo, Zhen Li, Peng Gao, Zhanyu Ma, Ming-Ming Cheng | 2025-04-10 | Training & Evaluation, Diffusion Models | Recent progress in diffusion models significantly advances various image generation tasks. However, the current mainstream approach remains focused on building task-specific models, which have limited efficiency when supporting a wide range of different needs. While universal models attempt to address this limitation, they face critical challenges, including generalizable task instruction, appropriate task distributions, and unified architectural design. To tackle these challenges, we propose VisualCloze, a universal image generation framework, which supports a wide range of in-domain tasks, generalization to unseen ones, unseen unification of multiple tasks, and reverse generation. Unlike existing methods that rely on language-based task instruction, leading to task ambiguity and weak generalization, we integrate visual in-context learning, allowing models to identify tasks from visual demonstrations. Meanwhile, the inherent sparsity of visual task distributions hampers the learning of transferable knowledge across tasks. To this end, we introduce Graph200K, a graph-structured dataset that establishes various interrelated tasks, enhancing task density and transferable knowledge. Furthermore, we uncover that our unified image generation formulation shared a consistent objective with image infilling, enabling us to leverage the strong generative priors of pre-trained infilling models without modifying the architectures. | [üîó Paper](http://arxiv.org/abs/2504.07960v1) |
## üîπ Prompt Engineering

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Detect Anything 3D in the Wild](http://arxiv.org/abs/2504.07958v1) | Hanxue Zhang, Haoran Jiang, Qingsong Yao, Yanan Sun, Renrui Zhang, Hao Zhao, Hongyang Li, Hongzi Zhu, Zetong Yang | 2025-04-10 | Prompt Engineering | Despite the success of deep learning in close-set 3D object detection, existing approaches struggle with zero-shot generalization to novel objects and camera configurations. We introduce DetAny3D, a promptable 3D detection foundation model capable of detecting any novel object under arbitrary camera configurations using only monocular inputs. Training a foundation model for 3D detection is fundamentally constrained by the limited availability of annotated 3D data, which motivates DetAny3D to leverage the rich prior knowledge embedded in extensively pre-trained 2D foundation models to compensate for this scarcity. To effectively transfer 2D knowledge to 3D, DetAny3D incorporates two core modules: the 2D Aggregator, which aligns features from different 2D foundation models, and the 3D Interpreter with Zero-Embedding Mapping, which mitigates catastrophic forgetting in 2D-to-3D knowledge transfer. Experimental results validate the strong generalization of our DetAny3D, which not only achieves state-of-the-art performance on unseen categories and novel camera configurations, but also surpasses most competitors on in-domain data.DetAny3D sheds light on the potential of the 3D foundation model for diverse applications in real-world scenarios, e.g., rare object detection in autonomous driving, and demonstrates promise for further exploration of 3D-centric tasks in open-world settings. More visualization results can be found at DetAny3D project page. | [üîó Paper](http://arxiv.org/abs/2504.07958v1) |
| [VCR-Bench: A Comprehensive Evaluation Framework for Video
  Chain-of-Thought Reasoning](http://arxiv.org/abs/2504.07956v1) | Yukun Qi, Yiming Zhao, Yu Zeng, Xikun Bao, Wenxuan Huang, Lin Chen, Zehui Chen, Jie Zhao, Zhongang Qi, Feng Zhao | 2025-04-10 | Prompt Engineering, Multimodal AI, Training & Evaluation | The advancement of Chain-of-Thought (CoT) reasoning has significantly enhanced the capabilities of large language models (LLMs) and large vision-language models (LVLMs). However, a rigorous evaluation framework for video CoT reasoning remains absent. Current video benchmarks fail to adequately assess the reasoning process and expose whether failures stem from deficiencies in perception or reasoning capabilities. Therefore, we introduce VCR-Bench, a novel benchmark designed to comprehensively evaluate LVLMs' Video Chain-of-Thought Reasoning capabilities. VCR-Bench comprises 859 videos spanning a variety of video content and durations, along with 1,034 high-quality question-answer pairs. Each pair is manually annotated with a stepwise CoT rationale, where every step is tagged to indicate its association with the perception or reasoning capabilities. Furthermore, we design seven distinct task dimensions and propose the CoT score to assess the entire CoT process based on the stepwise tagged CoT rationals. Extensive experiments on VCR-Bench highlight substantial limitations in current LVLMs. Even the top-performing model, o1, only achieves a 62.8% CoT score and an 56.7% accuracy, while most models score below 40%. Experiments show most models score lower on perception than reasoning steps, revealing LVLMs' key bottleneck in temporal-spatial information processing for complex video reasoning. A robust positive correlation between the CoT score and accuracy confirms the validity of our evaluation framework and underscores the critical role of CoT reasoning in solving complex video reasoning tasks. We hope VCR-Bench to serve as a standardized evaluation framework and expose the actual drawbacks in complex video reasoning task. | [üîó Paper](http://arxiv.org/abs/2504.07956v1) |
## üîπ Responsible AI

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [What it takes to solve the Hubble tension through scale-dependent
  modifications of the primordial power spectrum](http://arxiv.org/abs/2504.07966v1) | Nanoom Lee, Matteo Braglia, Yacine Ali-Ha√Ømoud | 2025-04-10 | Responsible AI, Model Evaluation | We investigate scale-dependent modifications to the primordial scalar power spectrum as potential solutions to the Hubble tension. We use the Fisher-bias formalism, recently adapted to examine perturbed recombination solutions to the Hubble tension, and extend its range of validity with an iterative method. We first analyze the Planck cosmic microwave background (CMB) anisotropy data, demonstrating the existence of modifications to the primordial power spectrum capable of fully resolving the tension between Planck and SH0ES. As a proof of concept, we interpret these solutions in terms of small, time-dependent variations in the first slow roll parameter or in the sound speed of curvature perturbations during a stage of primordial inflation. However, these solutions are associated with a low total matter density $\Omega_m$, which makes them inconsistent with baryon acoustic oscillations (BAO) and uncalibrated supernovae (SNIa) data. When incorporating additional BOSS and PantheonPlus data, the solutions that reduce the Hubble tension tend to overfit Planck CMB data to compensate for the worsened fit to BAO and SNIa data, making them less compelling. These findings suggest that modifying the primordial power spectrum alone is unlikely to provide a robust resolution to the tension and highlight how the viability of such data-driven solutions depends on the specific datasets considered, emphasizing the role of future high-precision observations in further constraining possible resolutions to the tension. | [üîó Paper](http://arxiv.org/abs/2504.07966v1) |
## üîπ General AI

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [CCMNet: Leveraging Calibrated Color Correction Matrices for Cross-Camera
  Color Constancy](http://arxiv.org/abs/2504.07959v1) | Dongyoung Kim, Mahmoud Afifi, Dongyun Kim, Michael S. Brown, Seon Joo Kim | 2025-04-10 | General AI | Computational color constancy, or white balancing, is a key module in a camera's image signal processor (ISP) that corrects color casts from scene lighting. Because this operation occurs in the camera-specific raw color space, white balance algorithms must adapt to different cameras. This paper introduces a learning-based method for cross-camera color constancy that generalizes to new cameras without retraining. Our method leverages pre-calibrated color correction matrices (CCMs) available on ISPs that map the camera's raw color space to a standard space (e.g., CIE XYZ). Our method uses these CCMs to transform predefined illumination colors (i.e., along the Planckian locus) into the test camera's raw space. The mapped illuminants are encoded into a compact camera fingerprint embedding (CFE) that enables the network to adapt to unseen cameras. To prevent overfitting due to limited cameras and CCMs during training, we introduce a data augmentation technique that interpolates between cameras and their CCMs. Experimental results across multiple datasets and backbones show that our method achieves state-of-the-art cross-camera color constancy while remaining lightweight and relying only on data readily available in camera ISPs. | [üîó Paper](http://arxiv.org/abs/2504.07959v1) |
| [Free monad sequences and extension operations](http://arxiv.org/abs/2504.07953v1) | Christian Sattler | 2025-04-10 | General AI | In the first part of this article, we give an analysis of the free monad sequence in non-cocomplete categories, with the needed colimits explicitly parametrized. This enables us to state a more finely grained functoriality principle for free monad and monoid sequences.   In the second part, we deal with the problem of functorially extending via pullback squares a category of maps along the category of coalgebras of an algebraic weak factorization system. This generalizes the classical problem of extending a class of maps along the left class of a weak factorization system in the sense of pullback squares where the vertical maps are in the chosen class and the bottom map is in the left class. Such situations arise in the context of model structures where one might wish to extend fibrations along trivial cofibrations. We derive suitable conditions for the algebraic analogue of weak saturation of the extension problem, using the results of the first part to reduce the technical burden. | [üîó Paper](http://arxiv.org/abs/2504.07953v1) |
| [Localized quasiparticles in a fluxonium with quasi-two-dimensional
  amorphous kinetic inductors](http://arxiv.org/abs/2504.07950v1) | Trevyn F. Q. Larson, Sarah Garcia Jones, Tam√°s Kalm√°r, Pablo Aramburu Sanchez, Sai Pavan Chitta, Varun Verma, Kristen Genter, Katarina Cicak, Sae Woo Nam, Gerg≈ë F√ºl√∂p, Jens Koch, Ray W. Simmonds, Andr√°s Gyenis | 2025-04-10 | General AI | Disordered superconducting materials with high kinetic inductance are an important resource to generate nonlinearity in quantum circuits and create high-impedance environments. In thin films fabricated from these materials, the combination of disorder and the low effective dimensionality leads to increased order parameter fluctuations and enhanced kinetic inductance values. Among the challenges of harnessing these compounds in coherent devices are their proximity to the superconductor-insulator phase transition, the presence of broken Cooper pairs, and the two-level systems located in the disordered structure. In this work, we fabricate tungsten silicide wires from quasi-two-dimensional films with one spatial dimension smaller than the superconducting coherence length and embed them into microwave resonators and fluxonium qubits, where the kinetic inductance provides the inductive part of the circuits. We study the dependence of loss on the frequency, disorder, and geometry of the device, and find that the loss increases with the level of disorder and is dominated by the localized quasiparticles trapped in the spatial variations of the superconducting gap. | [üîó Paper](http://arxiv.org/abs/2504.07950v1) |
| [InteractAvatar: Modeling Hand-Face Interaction in Photorealistic Avatars
  with Deformable Gaussians](http://arxiv.org/abs/2504.07949v1) | Kefan Chen, Sergiu Oprea, Justin Theiss, Sreyas Mohan, Srinath Sridhar, Aayush Prakash | 2025-04-10 | General AI | With the rising interest from the community in digital avatars coupled with the importance of expressions and gestures in communication, modeling natural avatar behavior remains an important challenge across many industries such as teleconferencing, gaming, and AR/VR. Human hands are the primary tool for interacting with the environment and essential for realistic human behavior modeling, yet existing 3D hand and head avatar models often overlook the crucial aspect of hand-body interactions, such as between hand and face. We present InteracttAvatar, the first model to faithfully capture the photorealistic appearance of dynamic hand and non-rigid hand-face interactions. Our novel Dynamic Gaussian Hand model, combining template model and 3D Gaussian Splatting as well as a dynamic refinement module, captures pose-dependent change, e.g. the fine wrinkles and complex shadows that occur during articulation. Importantly, our hand-face interaction module models the subtle geometry and appearance dynamics that underlie common gestures. Through experiments of novel view synthesis, self reenactment and cross-identity reenactment, we demonstrate that InteracttAvatar can reconstruct hand and hand-face interactions from monocular or multiview videos with high-fidelity details and be animated with novel poses. | [üîó Paper](http://arxiv.org/abs/2504.07949v1) |
| [Activating high-power parametric oscillation in photonic-crystal
  resonators](http://arxiv.org/abs/2504.07947v1) | Grant M. Brodnik, Lindell M. Williams, Haixin Liu, David R. Carlson, Atasi Dan, Jennifer A. Black, Scott B. Papp | 2025-04-10 | General AI | By engineering the mode spectrum of a Kerr microresonator, we selectively activate nonlinear phase matching amongst broadband parametric gain. At threshold, optical parametric oscillators (OPOs) emerge from vacuum fluctuations in the presence of a pump laser, and above threshold, OPOs seed the formation of intraresonator patterns and states, such as chaos and solitons. These competing nonlinear processes hinder an important application of OPOs as wavelength-variable, low-noise sources. Recently, nanopatterned microresonator OPOs have leveraged photonic crystal bandgaps to enable universal phase matching and control of nonlinear interactions. Here, we explore a design paradigm optimized for high-output power that uses geometric dispersion to suppress nonlinear interactions and a photonic crystal bandgap to activate only a single OPO interaction. Our devices convert an input pump laser to output signal and idler waves with powers exceeding 40 mW while maintaining spectral purity and side-mode suppression ratios greater than 40 dB. We show that this approach suits custom wavelengths by measuring four independent oscillators that vary only photonic crystal parameters to select output waves. Our experiments demonstrate that microresonators functionalized by photonic crystals offer a versatile and lossless palette of controls for nonlinear laser conversion. | [üîó Paper](http://arxiv.org/abs/2504.07947v1) |
