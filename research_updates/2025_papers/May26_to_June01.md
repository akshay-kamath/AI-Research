# üìå AI Research Papers (May26 to June01)

## üîπ LLM

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [LoRAShop: Training-Free Multi-Concept Image Generation and Editing with
  Rectified Flow Transformers](http://arxiv.org/abs/2505.23758v1) | Yusuf Dalva, Hidir Yesiltepe, Pinar Yanardag | 2025-05-29 | LLM, Optimization, Diffusion Models | We introduce LoRAShop, the first framework for multi-concept image editing with LoRA models. LoRAShop builds on a key observation about the feature interaction patterns inside Flux-style diffusion transformers: concept-specific transformer features activate spatially coherent regions early in the denoising process. We harness this observation to derive a disentangled latent mask for each concept in a prior forward pass and blend the corresponding LoRA weights only within regions bounding the concepts to be personalized. The resulting edits seamlessly integrate multiple subjects or styles into the original scene while preserving global context, lighting, and fine details. Our experiments demonstrate that LoRAShop delivers better identity preservation compared to baselines. By eliminating retraining and external constraints, LoRAShop turns personalized diffusion models into a practical `photoshop-with-LoRAs' tool and opens new avenues for compositional visual storytelling and rapid creative iteration. | [üîó Paper](http://arxiv.org/abs/2505.23758v1) |
| [ThinkGeo: Evaluating Tool-Augmented Agents for Remote Sensing Tasks](http://arxiv.org/abs/2505.23752v1) | Akashah Shabbir, Muhammad Akhtar Munir, Akshay Dudhane, Muhammad Umer Sheikh, Muhammad Haris Khan, Paolo Fraccaro, Juan Bernabe Moreno, Fahad Shahbaz Khan, Salman Khan | 2025-05-29 | LLM, Multimodal AI | Recent progress in large language models (LLMs) has enabled tool-augmented agents capable of solving complex real-world tasks through step-by-step reasoning. However, existing evaluations often focus on general-purpose or multimodal scenarios, leaving a gap in domain-specific benchmarks that assess tool-use capabilities in complex remote sensing use cases. We present ThinkGeo, an agentic benchmark designed to evaluate LLM-driven agents on remote sensing tasks via structured tool use and multi-step planning. Inspired by tool-interaction paradigms, ThinkGeo includes human-curated queries spanning a wide range of real-world applications such as urban planning, disaster assessment and change analysis, environmental monitoring, transportation analysis, aviation monitoring, recreational infrastructure, and industrial site analysis. Each query is grounded in satellite or aerial imagery and requires agents to reason through a diverse toolset. We implement a ReAct-style interaction loop and evaluate both open and closed-source LLMs (e.g., GPT-4o, Qwen2.5) on 436 structured agentic tasks. The benchmark reports both step-wise execution metrics and final answer correctness. Our analysis reveals notable disparities in tool accuracy and planning consistency across models. ThinkGeo provides the first extensive testbed for evaluating how tool-enabled LLMs handle spatial reasoning in remote sensing. Our code and dataset are publicly available | [üîó Paper](http://arxiv.org/abs/2505.23752v1) |
| [How Animals Dance (When You're Not Looking)](http://arxiv.org/abs/2505.23738v1) | Xiaojuan Wang, Aleksander Holynski, Brian Curless, Ira Kemelmacher, Steve Seitz | 2025-05-29 | LLM, Optimization, Diffusion Models, Multimodal AI | We present a keyframe-based framework for generating music-synchronized, choreography aware animal dance videos. Starting from a few keyframes representing distinct animal poses -- generated via text-to-image prompting or GPT-4o -- we formulate dance synthesis as a graph optimization problem: find the optimal keyframe structure that satisfies a specified choreography pattern of beats, which can be automatically estimated from a reference dance video. We also introduce an approach for mirrored pose image generation, essential for capturing symmetry in dance. In-between frames are synthesized using an video diffusion model. With as few as six input keyframes, our method can produce up to 30 second dance videos across a wide range of animals and music tracks. | [üîó Paper](http://arxiv.org/abs/2505.23738v1) |
| [ATLAS: Learning to Optimally Memorize the Context at Test Time](http://arxiv.org/abs/2505.23735v1) | Ali Behrouz, Zeman Li, Praneeth Kacham, Majid Daliri, Yuan Deng, Peilin Zhong, Meisam Razaviyayn, Vahab Mirrokni | 2025-05-29 | LLM, Memory & Context Length | Transformers have been established as the most popular backbones in sequence modeling, mainly due to their effectiveness in in-context retrieval tasks and the ability to learn at scale. Their quadratic memory and time complexity, however, bound their applicability in longer sequences and so has motivated researchers to explore effective alternative architectures such as modern recurrent neural networks (a.k.a long-term recurrent memory module). Despite their recent success in diverse downstream tasks, they struggle in tasks that requires long context understanding and extrapolation to longer sequences. We observe that these shortcomings come from three disjoint aspects in their design: (1) limited memory capacity that is bounded by the architecture of memory and feature mapping of the input; (2) online nature of update, i.e., optimizing the memory only with respect to the last input; and (3) less expressive management of their fixed-size memory. To enhance all these three aspects, we present ATLAS, a long-term memory module with high capacity that learns to memorize the context by optimizing the memory based on the current and past tokens, overcoming the online nature of long-term memory models. Building on this insight, we present a new family of Transformer-like architectures, called DeepTransformers, that are strict generalizations of the original Transformer architecture. Our experimental results on language modeling, common-sense reasoning, recall-intensive, and long-context understanding tasks show that ATLAS surpasses the performance of Transformers and recent linear recurrent models. ATLAS further improves the long context performance of Titans, achieving +80\% accuracy in 10M context length of BABILong benchmark. | [üîó Paper](http://arxiv.org/abs/2505.23735v1) |
| [Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time](http://arxiv.org/abs/2505.23729v1) | Mohamad Chehade, Soumya Suvra Ghosal, Souradip Chakraborty, Avinash Reddy, Dinesh Manocha, Hao Zhu, Amrit Singh Bedi | 2025-05-29 | LLM, Optimization, RLHF | Aligning large language models with humans is challenging due to the inherently multifaceted nature of preference feedback. While existing approaches typically frame this as a multi-objective optimization problem, they often overlook how humans actually make decisions. Research on bounded rationality suggests that human decision making follows satisficing strategies-optimizing primary objectives while ensuring others meet acceptable thresholds. To bridge this gap and operationalize the notion of satisficing alignment, we propose SITAlign: an inference time framework that addresses the multifaceted nature of alignment by maximizing a primary objective while satisfying threshold-based constraints on secondary criteria. We provide theoretical insights by deriving sub-optimality bounds of our satisficing based inference alignment approach. We empirically validate SITAlign's performance through extensive experimentation on multiple benchmarks. For instance, on the PKU-SafeRLHF dataset with the primary objective of maximizing helpfulness while ensuring a threshold on harmlessness, SITAlign outperforms the state-of-the-art multi objective decoding strategy by a margin of 22.3% in terms of GPT-4 win-tie rate for helpfulness reward while adhering to the threshold on harmlessness. | [üîó Paper](http://arxiv.org/abs/2505.23729v1) |
| [MuLoCo: Muon is a practical inner optimizer for DiLoCo](http://arxiv.org/abs/2505.23725v1) | Benjamin Th√©rien, Xiaolong Huang, Irina Rish, Eugene Belilovsky | 2025-05-29 | LLM, Optimization | DiLoCo is a powerful framework for training large language models (LLMs) under networking constraints with advantages for increasing parallelism and accelerator utilization in data center settings. Despite significantly reducing communication frequency, however, DiLoCo's communication steps still involve all-reducing a complete copy of the model's parameters. While existing works have explored ways to reduce communication in DiLoCo, the role of error feedback accumulators and the effect of the inner-optimizer on compressibility remain under-explored. In this work, we investigate the effectiveness of standard compression methods including Top-k sparsification and quantization for reducing the communication overhead of DiLoCo when paired with two local optimizers (AdamW and Muon). Our experiments pre-training decoder-only transformer language models (LMs) reveal that leveraging Muon as the inner optimizer for DiLoCo along with an error-feedback accumulator allows to aggressively compress the communicated delta to 2-bits with next to no performance degradation. Crucially, MuLoCo (Muon inner optimizer DiLoCo) significantly outperforms DiLoCo while communicating 8X less and having identical memory complexity. | [üîó Paper](http://arxiv.org/abs/2505.23725v1) |
| [ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning
  Engineering](http://arxiv.org/abs/2505.23723v1) | Zexi Liu, Jingyi Chai, Xinyu Zhu, Shuo Tang, Rui Ye, Bo Zhang, Lei Bai, Siheng Chen | 2025-05-29 | LLM, Optimization, RLHF | The emergence of large language model (LLM)-based agents has significantly advanced the development of autonomous machine learning (ML) engineering. However, most existing approaches rely heavily on manual prompt engineering, failing to adapt and optimize based on diverse experimental experiences. Focusing on this, for the first time, we explore the paradigm of learning-based agentic ML, where an LLM agent learns through interactive experimentation on ML tasks using online reinforcement learning (RL). To realize this, we propose a novel agentic ML training framework with three key components: (1) exploration-enriched fine-tuning, which enables LLM agents to generate diverse actions for enhanced RL exploration; (2) step-wise RL, which enables training on a single action step, accelerating experience collection and improving training efficiency; (3) an agentic ML-specific reward module, which unifies varied ML feedback signals into consistent rewards for RL optimization. Leveraging this framework, we train ML-Agent, driven by a 7B-sized Qwen-2.5 LLM for autonomous ML. Remarkably, despite being trained on merely 9 ML tasks, our 7B-sized ML-Agent outperforms the 671B-sized DeepSeek-R1 agent. Furthermore, it achieves continuous performance improvements and demonstrates exceptional cross-task generalization capabilities. | [üîó Paper](http://arxiv.org/abs/2505.23723v1) |
| [DiffER: Categorical Diffusion for Chemical Retrosynthesis](http://arxiv.org/abs/2505.23721v1) | Sean Current, Ziqi Chen, Daniel Adu-Ampratwum, Xia Ning, Srinivasan Parthasarathy | 2025-05-29 | LLM, Diffusion Models | Methods for automatic chemical retrosynthesis have found recent success through the application of models traditionally built for natural language processing, primarily through transformer neural networks. These models have demonstrated significant ability to translate between the SMILES encodings of chemical products and reactants, but are constrained as a result of their autoregressive nature. We propose DiffER, an alternative template-free method for retrosynthesis prediction in the form of categorical diffusion, which allows the entire output SMILES sequence to be predicted in unison. We construct an ensemble of diffusion models which achieves state-of-the-art performance for top-1 accuracy and competitive performance for top-3, top-5, and top-10 accuracy among template-free methods. We prove that DiffER is a strong baseline for a new class of template-free model, capable of learning a variety of synthetic techniques used in laboratory settings and outperforming a variety of other template-free methods on top-k accuracy metrics. By constructing an ensemble of categorical diffusion models with a novel length prediction component with variance, our method is able to approximately sample from the posterior distribution of reactants, producing results with strong metrics of confidence and likelihood. Furthermore, our analyses demonstrate that accurate prediction of the SMILES sequence length is key to further boosting the performance of categorical diffusion models. | [üîó Paper](http://arxiv.org/abs/2505.23721v1) |
## üîπ Diffusion Models

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Model Immunization from a Condition Number Perspective](http://arxiv.org/abs/2505.23760v1) | Amber Yijia Zheng, Cedar Site Bai, Brian Bullins, Raymond A. Yeh | 2025-05-29 | Diffusion Models | Model immunization aims to pre-train models that are difficult to fine-tune on harmful tasks while retaining their utility on other non-harmful tasks. Though prior work has shown empirical evidence for immunizing text-to-image models, the key understanding of when immunization is possible and a precise definition of an immunized model remain unclear. In this work, we propose a framework, based on the condition number of a Hessian matrix, to analyze model immunization for linear models. Building on this framework, we design an algorithm with regularization terms to control the resulting condition numbers after pre-training. Empirical results on linear models and non-linear deep-nets demonstrate the effectiveness of the proposed algorithm on model immunization. The code is available at https://github.com/amberyzheng/model-immunization-cond-num. | [üîó Paper](http://arxiv.org/abs/2505.23760v1) |
| [DarkDiff: Advancing Low-Light Raw Enhancement by Retasking Diffusion
  Models for Camera ISP](http://arxiv.org/abs/2505.23743v1) | Amber Yijia Zheng, Yu Zhang, Jun Hu, Raymond A. Yeh, Chen Chen | 2025-05-29 | Diffusion Models | High-quality photography in extreme low-light conditions is challenging but impactful for digital cameras. With advanced computing hardware, traditional camera image signal processor (ISP) algorithms are gradually being replaced by efficient deep networks that enhance noisy raw images more intelligently. However, existing regression-based models often minimize pixel errors and result in oversmoothing of low-light photos or deep shadows. Recent work has attempted to address this limitation by training a diffusion model from scratch, yet those models still struggle to recover sharp image details and accurate colors. We introduce a novel framework to enhance low-light raw images by retasking pre-trained generative diffusion models with the camera ISP. Extensive experiments demonstrate that our method outperforms the state-of-the-art in perceptual quality across three challenging low-light raw image benchmarks. | [üîó Paper](http://arxiv.org/abs/2505.23743v1) |
| [MAGREF: Masked Guidance for Any-Reference Video Generation](http://arxiv.org/abs/2505.23742v1) | Yufan Deng, Xun Guo, Yuanyang Yin, Jacob Zhiyuan Fang, Yiding Yang, Yizhi Wang, Shenghai Yuan, Angtian Wang, Bo Liu, Haibin Huang, Chongyang Ma | 2025-05-29 | Diffusion Models, Training & Evaluation, Multimodal AI | Video generation has made substantial strides with the emergence of deep generative models, especially diffusion-based approaches. However, video generation based on multiple reference subjects still faces significant challenges in maintaining multi-subject consistency and ensuring high generation quality. In this paper, we propose MAGREF, a unified framework for any-reference video generation that introduces masked guidance to enable coherent multi-subject video synthesis conditioned on diverse reference images and a textual prompt. Specifically, we propose (1) a region-aware dynamic masking mechanism that enables a single model to flexibly handle various subject inference, including humans, objects, and backgrounds, without architectural changes, and (2) a pixel-wise channel concatenation mechanism that operates on the channel dimension to better preserve appearance features. Our model delivers state-of-the-art video generation quality, generalizing from single-subject training to complex multi-subject scenarios with coherent synthesis and precise control over individual subjects, outperforming existing open-source and commercial baselines. To facilitate evaluation, we also introduce a comprehensive multi-subject video benchmark. Extensive experiments demonstrate the effectiveness of our approach, paving the way for scalable, controllable, and high-fidelity multi-subject video synthesis. Code and model can be found at: https://github.com/MAGREF-Video/MAGREF | [üîó Paper](http://arxiv.org/abs/2505.23742v1) |
| [LayerPeeler: Autoregressive Peeling for Layer-wise Image Vectorization](http://arxiv.org/abs/2505.23740v1) | Ronghuan Wu, Wanchao Su, Jing Liao | 2025-05-29 | Diffusion Models, Multimodal AI | Image vectorization is a powerful technique that converts raster images into vector graphics, enabling enhanced flexibility and interactivity. However, popular image vectorization tools struggle with occluded regions, producing incomplete or fragmented shapes that hinder editability. While recent advancements have explored rule-based and data-driven layer-wise image vectorization, these methods face limitations in vectorization quality and flexibility. In this paper, we introduce LayerPeeler, a novel layer-wise image vectorization approach that addresses these challenges through a progressive simplification paradigm. The key to LayerPeeler's success lies in its autoregressive peeling strategy: by identifying and removing the topmost non-occluded layers while recovering underlying content, we generate vector graphics with complete paths and coherent layer structures. Our method leverages vision-language models to construct a layer graph that captures occlusion relationships among elements, enabling precise detection and description for non-occluded layers. These descriptive captions are used as editing instructions for a finetuned image diffusion model to remove the identified layers. To ensure accurate removal, we employ localized attention control that precisely guides the model to target regions while faithfully preserving the surrounding content. To support this, we contribute a large-scale dataset specifically designed for layer peeling tasks. Extensive quantitative and qualitative experiments demonstrate that LayerPeeler significantly outperforms existing techniques, producing vectorization results with superior path semantics, geometric regularity, and visual fidelity. | [üîó Paper](http://arxiv.org/abs/2505.23740v1) |
## üîπ RLHF

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [TextRegion: Text-Aligned Region Tokens from Frozen Image-Text Models](http://arxiv.org/abs/2505.23769v1) | Yao Xiao, Qiqian Fu, Heyi Tao, Yuqun Wu, Zhen Zhu, Derek Hoiem | 2025-05-29 | RLHF | Image-text models excel at image-level tasks but struggle with detailed visual understanding. While these models provide strong visual-language alignment, segmentation models like SAM2 offer precise spatial boundaries for objects. To this end, we propose TextRegion, a simple, effective, and training-free framework that combines the strengths of image-text models and SAM2 to generate powerful text-aligned region tokens. These tokens enable detailed visual understanding while preserving open-vocabulary capabilities. They can be directly applied to various downstream tasks, including open-world semantic segmentation, referring expression comprehension, and grounding. We conduct extensive evaluations and consistently achieve superior or competitive performance compared to state-of-the-art training-free methods. Additionally, our framework is compatible with many image-text models, making it highly practical and easily extensible as stronger models emerge. Code is available at: https://github.com/avaxiao/TextRegion. | [üîó Paper](http://arxiv.org/abs/2505.23769v1) |
| [ZeroGUI: Automating Online GUI Learning at Zero Human Cost](http://arxiv.org/abs/2505.23762v1) | Chenyu Yang, Shiqian Su, Shi Liu, Xuan Dong, Yue Yu, Weijie Su, Xuehui Wang, Zhaoyang Liu, Jinguo Zhu, Hao Li, Wenhai Wang, Yu Qiao, Xizhou Zhu, Jifeng Dai | 2025-05-29 | RLHF, Training & Evaluation, Multimodal AI | The rapid advancement of large Vision-Language Models (VLMs) has propelled the development of pure-vision-based GUI Agents, capable of perceiving and operating Graphical User Interfaces (GUI) to autonomously fulfill user instructions. However, existing approaches usually adopt an offline learning framework, which faces two core limitations: (1) heavy reliance on high-quality manual annotations for element grounding and action supervision, and (2) limited adaptability to dynamic and interactive environments. To address these limitations, we propose ZeroGUI, a scalable, online learning framework for automating GUI Agent training at Zero human cost. Specifically, ZeroGUI integrates (i) VLM-based automatic task generation to produce diverse training goals from the current environment state, (ii) VLM-based automatic reward estimation to assess task success without hand-crafted evaluation functions, and (iii) two-stage online reinforcement learning to continuously interact with and learn from GUI environments. Experiments on two advanced GUI Agents (UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance across OSWorld and AndroidLab environments. The code is available at https://github.com/OpenGVLab/ZeroGUI. | [üîó Paper](http://arxiv.org/abs/2505.23762v1) |
| [Hydrodynamic simulations of black hole evolution in AGN discs I: orbital
  alignment of highly inclined satellites](http://arxiv.org/abs/2505.23739v1) | Connar Rowan, Henry Whitehead, Gaia Fabj, Philip Kirkeberg, Martin E. Pessah, Bence Kocsis | 2025-05-29 | RLHF | The frequency of compact object interactions in AGN discs is naturally tied to the number of objects embedded within it. We investigate the evolution of black holes in the nuclear stellar cluster on inclined orbits to the AGN disc by performing adiabatic hydrodynamical simulations of isolated black hole disc crossings over a range of disc densities and inclinations $i\in[2^\circ,15^\circ]$. We find radiation dominates the pressure in the wake that forms around the BH across the full inclination and disc density range. We identify no well defined steady state wake morphology due to the thin geometry of the disc and the vertical exponential density drop off, where the wake morphology depends on the vertical depth of the transit within the disc. The inclination damping $\Delta i$ relative the pre-transit inclination behaves as a power law in $\sin(i)$ and the ambient Hill mass $m_\text{H,0}$ as $\Delta i/i \propto m_{\rm H,0}^{0.4} \sin(i)^{-2.7}$. The drag on the BH is dominated by the gravity of the wake for the majority of our inclination range until accretion effects become comparable at $\sin(i)\gtrsim30H_0/R_0$, where $H_0/R_0$ is the disc aspect ratio. At low inclinations ($\sin(i)\lesssim3H_0/R_0$) the wake morphology becomes more spherical, leading to a regime change in the inclination damping behaviour. Our results suggest that the inclination damping timescale is shorter than expected from only episodic Bondi-Hoyle-Lyttelton accretion events during each transit, implying inclined objects may captured by the AGN disc earlier in its lifetime than previously thought. | [üîó Paper](http://arxiv.org/abs/2505.23739v1) |
| [EmotionRankCLAP: Bridging Natural Language Speaking Styles and Ordinal
  Speech Emotion via Rank-N-Contrast](http://arxiv.org/abs/2505.23732v1) | Shreeram Suresh Chandra, Lucas Goncalves, Junchen Lu, Carlos Busso, Berrak Sisman | 2025-05-29 | RLHF, Fine-Tuning, Multimodal AI | Current emotion-based contrastive language-audio pretraining (CLAP) methods typically learn by na\"ively aligning audio samples with corresponding text prompts. Consequently, this approach fails to capture the ordinal nature of emotions, hindering inter-emotion understanding and often resulting in a wide modality gap between the audio and text embeddings due to insufficient alignment. To handle these drawbacks, we introduce EmotionRankCLAP, a supervised contrastive learning approach that uses dimensional attributes of emotional speech and natural language prompts to jointly capture fine-grained emotion variations and improve cross-modal alignment. Our approach utilizes a Rank-N-Contrast objective to learn ordered relationships by contrasting samples based on their rankings in the valence-arousal space. EmotionRankCLAP outperforms existing emotion-CLAP methods in modeling emotion ordinality across modalities, measured via a cross-modal retrieval task. | [üîó Paper](http://arxiv.org/abs/2505.23732v1) |
## üîπ Multimodal AI

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence](http://arxiv.org/abs/2505.23764v1) | Sihan Yang, Runsen Xu, Yiman Xie, Sizhe Yang, Mo Li, Jingli Lin, Chenming Zhu, Xiaochen Chen, Haodong Duan, Xiangyu Yue, Dahua Lin, Tai Wang, Jiangmiao Pang | 2025-05-29 | Multimodal AI | Spatial intelligence is essential for multimodal large language models (MLLMs) operating in the complex physical world. Existing benchmarks, however, probe only single-image relations and thus fail to assess the multi-image spatial reasoning that real-world deployments demand. We introduce MMSI-Bench, a VQA benchmark dedicated to multi-image spatial intelligence. Six 3D-vision researchers spent more than 300 hours meticulously crafting 1,000 challenging, unambiguous multiple-choice questions from over 120,000 images, each paired with carefully designed distractors and a step-by-step reasoning process. We conduct extensive experiments and thoroughly evaluate 34 open-source and proprietary MLLMs, observing a wide gap: the strongest open-source model attains roughly 30% accuracy and OpenAI's o3 reasoning model reaches 40%, while humans score 97%. These results underscore the challenging nature of MMSI-Bench and the substantial headroom for future research. Leveraging the annotated reasoning processes, we also provide an automated error analysis pipeline that diagnoses four dominant failure modes, including (1) grounding errors, (2) overlap-matching and scene-reconstruction errors, (3) situation-transformation reasoning errors, and (4) spatial-logic errors, offering valuable insights for advancing multi-image spatial intelligence. Project page: https://runsenxu.com/projects/MMSI_Bench . | [üîó Paper](http://arxiv.org/abs/2505.23764v1) |
| [Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint](http://arxiv.org/abs/2505.23759v1) | Heekyung Lee, Jiaxin Ge, Tsung-Han Wu, Minwoo Kang, Trevor Darrell, David M. Chan | 2025-05-29 | Multimodal AI | Rebus puzzles, visual riddles that encode language through imagery, spatial arrangement, and symbolic substitution, pose a unique challenge to current vision-language models (VLMs). Unlike traditional image captioning or question answering tasks, rebus solving requires multi-modal abstraction, symbolic reasoning, and a grasp of cultural, phonetic and linguistic puns. In this paper, we investigate the capacity of contemporary VLMs to interpret and solve rebus puzzles by constructing a hand-generated and annotated benchmark of diverse English-language rebus puzzles, ranging from simple pictographic substitutions to spatially-dependent cues ("head" over "heels"). We analyze how different VLMs perform, and our findings reveal that while VLMs exhibit some surprising capabilities in decoding simple visual clues, they struggle significantly with tasks requiring abstract reasoning, lateral thinking, and understanding visual metaphors. | [üîó Paper](http://arxiv.org/abs/2505.23759v1) |
| [Impromptu VLA: Open Weights and Open Data for Driving
  Vision-Language-Action Models](http://arxiv.org/abs/2505.23757v1) | Haohan Chi, Huan-ang Gao, Ziming Liu, Jianing Liu, Chenyu Liu, Jinwei Li, Kaisen Yang, Yangcheng Yu, Zeda Wang, Wenyi Li, Leichen Wang, Xingtao Hu, Hao Sun, Hang Zhao, Hao Zhao | 2025-05-29 | Multimodal AI | Vision-Language-Action (VLA) models for autonomous driving show promise but falter in unstructured corner case scenarios, largely due to a scarcity of targeted benchmarks. To address this, we introduce Impromptu VLA. Our core contribution is the Impromptu VLA Dataset: over 80,000 meticulously curated video clips, distilled from over 2M source clips sourced from 8 open-source large-scale datasets. This dataset is built upon our novel taxonomy of four challenging unstructured categories and features rich, planning-oriented question-answering annotations and action trajectories. Crucially, experiments demonstrate that VLAs trained with our dataset achieve substantial performance gains on established benchmarks--improving closed-loop NeuroNCAP scores and collision rates, and reaching near state-of-the-art L2 accuracy in open-loop nuScenes trajectory prediction. Furthermore, our Q&A suite serves as an effective diagnostic, revealing clear VLM improvements in perception, prediction, and planning. Our code, data and models are available at https://github.com/ahydchh/Impromptu-VLA. | [üîó Paper](http://arxiv.org/abs/2505.23757v1) |
## üîπ Optimization

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Sketch Down the FLOPs: Towards Efficient Networks for Human Sketch](http://arxiv.org/abs/2505.23763v1) | Aneeshan Sain, Subhajit Maity, Pinaki Nath Chowdhury, Subhadeep Koley, Ayan Kumar Bhunia, Yi-Zhe Song | 2025-05-29 | Optimization | As sketch research has collectively matured over time, its adaptation for at-mass commercialisation emerges on the immediate horizon. Despite an already mature research endeavour for photos, there is no research on the efficient inference specifically designed for sketch data. In this paper, we first demonstrate existing state-of-the-art efficient light-weight models designed for photos do not work on sketches. We then propose two sketch-specific components which work in a plug-n-play manner on any photo efficient network to adapt them to work on sketch data. We specifically chose fine-grained sketch-based image retrieval (FG-SBIR) as a demonstrator as the most recognised sketch problem with immediate commercial value. Technically speaking, we first propose a cross-modal knowledge distillation network to transfer existing photo efficient networks to be compatible with sketch, which brings down number of FLOPs and model parameters by 97.96% percent and 84.89% respectively. We then exploit the abstract trait of sketch to introduce a RL-based canvas selector that dynamically adjusts to the abstraction level which further cuts down number of FLOPs by two thirds. The end result is an overall reduction of 99.37% of FLOPs (from 40.18G to 0.254G) when compared with a full network, while retaining the accuracy (33.03% vs 32.77%) -- finally making an efficient network for the sparse sketch data that exhibit even fewer FLOPs than the best photo counterpart. | [üîó Paper](http://arxiv.org/abs/2505.23763v1) |
| [Rooms from Motion: Un-posed Indoor 3D Object Detection as Localization
  and Mapping](http://arxiv.org/abs/2505.23756v1) | Justin Lazarow, Kai Kang, Afshin Dehghan | 2025-05-29 | Optimization | We revisit scene-level 3D object detection as the output of an object-centric framework capable of both localization and mapping using 3D oriented boxes as the underlying geometric primitive. While existing 3D object detection approaches operate globally and implicitly rely on the a priori existence of metric camera poses, our method, Rooms from Motion (RfM) operates on a collection of un-posed images. By replacing the standard 2D keypoint-based matcher of structure-from-motion with an object-centric matcher based on image-derived 3D boxes, we estimate metric camera poses, object tracks, and finally produce a global, semantic 3D object map. When a priori pose is available, we can significantly improve map quality through optimization of global 3D boxes against individual observations. RfM shows strong localization performance and subsequently produces maps of higher quality than leading point-based and multi-view 3D object detection methods on CA-1M and ScanNet++, despite these global methods relying on overparameterization through point clouds or dense volumes. Rooms from Motion achieves a general, object-centric representation which not only extends the work of Cubify Anything to full scenes but also allows for inherently sparse localization and parametric mapping proportional to the number of objects in a scene. | [üîó Paper](http://arxiv.org/abs/2505.23756v1) |
| [DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural
  Language and Reinforcement Learning](http://arxiv.org/abs/2505.23754v1) | Ziyin Zhang, Jiahao Xu, Zhiwei He, Tian Liang, Qiuzhi Liu, Yansi Li, Linfeng Song, Zhengwen Liang, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, Dong Yu | 2025-05-29 | Optimization, RLHF, Training & Evaluation, Fine-Tuning | Theorem proving serves as a major testbed for evaluating complex reasoning abilities in large language models (LLMs). However, traditional automated theorem proving (ATP) approaches rely heavily on formal proof systems that poorly align with LLMs' strength derived from informal, natural language knowledge acquired during pre-training. In this work, we propose DeepTheorem, a comprehensive informal theorem-proving framework exploiting natural language to enhance LLM mathematical reasoning. DeepTheorem includes a large-scale benchmark dataset consisting of 121K high-quality IMO-level informal theorems and proofs spanning diverse mathematical domains, rigorously annotated for correctness, difficulty, and topic categories, accompanied by systematically constructed verifiable theorem variants. We devise a novel reinforcement learning strategy (RL-Zero) explicitly tailored to informal theorem proving, leveraging the verified theorem variants to incentivize robust mathematical inference. Additionally, we propose comprehensive outcome and process evaluation metrics examining proof correctness and the quality of reasoning steps. Extensive experimental analyses demonstrate DeepTheorem significantly improves LLM theorem-proving performance compared to existing datasets and supervised fine-tuning protocols, achieving state-of-the-art accuracy and reasoning quality. Our findings highlight DeepTheorem's potential to fundamentally advance automated informal theorem proving and mathematical exploration. | [üîó Paper](http://arxiv.org/abs/2505.23754v1) |
| [Distortion of AI Alignment: Does Preference Optimization Optimize for
  Preferences?](http://arxiv.org/abs/2505.23749v1) | Paul G√∂lz, Nika Haghtalab, Kunhe Yang | 2025-05-29 | Optimization, RLHF | After pre-training, large language models are aligned with human preferences based on pairwise comparisons. State-of-the-art alignment methods (such as PPO-based RLHF and DPO) are built on the assumption of aligning with a single preference model, despite being deployed in settings where users have diverse preferences. As a result, it is not even clear that these alignment methods produce models that satisfy users on average -- a minimal requirement for pluralistic alignment. Drawing on social choice theory and modeling users' comparisons through individual Bradley-Terry (BT) models, we introduce an alignment method's distortion: the worst-case ratio between the optimal achievable average utility, and the average utility of the learned policy.   The notion of distortion helps draw sharp distinctions between alignment methods: Nash Learning from Human Feedback achieves the minimax optimal distortion of $(\frac{1}{2} + o(1)) \cdot \beta$ (for the BT temperature $\beta$), robustly across utility distributions, distributions of comparison pairs, and permissible KL divergences from the reference policy. RLHF and DPO, by contrast, suffer $\geq (1 - o(1)) \cdot \beta$ distortion already without a KL constraint, and $e^{\Omega(\beta)}$ or even unbounded distortion in the full setting, depending on how comparison pairs are sampled. | [üîó Paper](http://arxiv.org/abs/2505.23749v1) |
| [Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial
  Intelligence](http://arxiv.org/abs/2505.23747v1) | Diankun Wu, Fangfu Liu, Yi-Hsin Hung, Yueqi Duan | 2025-05-29 | Optimization, Fine-Tuning, Multimodal AI | Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced performance on 2D visual tasks. However, improving their spatial intelligence remains a challenge. Existing 3D MLLMs always rely on additional 3D or 2.5D data to incorporate spatial awareness, restricting their utility in scenarios with only 2D inputs, such as images or videos. In this paper, we present Spatial-MLLM, a novel framework for visual-based spatial reasoning from purely 2D observations. Unlike conventional video MLLMs which rely on CLIP-based visual encoders optimized for semantic understanding, our key insight is to unleash the strong structure prior from the feed-forward visual geometry foundation model. Specifically, we propose a dual-encoder architecture: a pretrained 2D visual encoder to extract semantic features, and a spatial encoder-initialized from the backbone of the visual geometry model-to extract 3D structure features. A connector then integrates both features into unified visual tokens for enhanced spatial understanding. Furthermore, we propose a space-aware frame sampling strategy at inference time, which selects the spatially informative frames of a video sequence, ensuring that even under limited token length, the model focuses on frames critical for spatial reasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k dataset and train the model on it using supervised fine-tuning and GRPO. Extensive experiments on various real-world datasets demonstrate that our spatial-MLLM achieves state-of-the-art performance in a wide range of visual-based spatial understanding and reasoning tasks. Project page: https://diankun-wu.github.io/Spatial-MLLM/. | [üîó Paper](http://arxiv.org/abs/2505.23747v1) |
| [On the Convergence Analysis of Muon](http://arxiv.org/abs/2505.23737v1) | Wei Shen, Ruichuan Huang, Minhui Huang, Cong Shen, Jiawei Zhang | 2025-05-29 | Optimization | The majority of parameters in neural networks are naturally represented as matrices. However, most commonly used optimizers treat these matrix parameters as flattened vectors during optimization, potentially overlooking their inherent structural properties. Recently, an optimizer called Muon has been proposed, specifically designed to optimize matrix-structured parameters. Extensive empirical evidence shows that Muon can significantly outperform traditional optimizers when training neural networks. Nonetheless, the theoretical understanding of Muon's convergence behavior and the reasons behind its superior performance remain limited. In this work, we present a comprehensive convergence rate analysis of Muon and its comparison with Gradient Descent (GD). We further characterize the conditions under which Muon can outperform GD. Our theoretical results reveal that Muon can benefit from the low-rank and approximate blockwise diagonal structure of Hessian matrices -- phenomena widely observed in practical neural network training. Our experimental results support and corroborate the theoretical findings. | [üîó Paper](http://arxiv.org/abs/2505.23737v1) |
| [PixelThink: Towards Efficient Chain-of-Pixel Reasoning](http://arxiv.org/abs/2505.23727v1) | Song Wang, Gongfan Fang, Lingdong Kong, Xiangtai Li, Jianyun Xu, Sheng Yang, Qiang Li, Jianke Zhu, Xinchao Wang | 2025-05-29 | Optimization, Ongoing Learning, Multimodal AI, RLHF, Training & Evaluation | Existing reasoning segmentation approaches typically fine-tune multimodal large language models (MLLMs) using image-text pairs and corresponding mask labels. However, they exhibit limited generalization to out-of-distribution scenarios without an explicit reasoning process. Although recent efforts leverage reinforcement learning through group-relative policy optimization (GRPO) to enhance reasoning ability, they often suffer from overthinking - producing uniformly verbose reasoning chains irrespective of task complexity. This results in elevated computational costs and limited control over reasoning quality. To address this problem, we propose PixelThink, a simple yet effective scheme that integrates externally estimated task difficulty and internally measured model uncertainty to regulate reasoning generation within a reinforcement learning paradigm. The model learns to compress reasoning length in accordance with scene complexity and predictive confidence. To support comprehensive evaluation, we introduce ReasonSeg-Diff, an extended benchmark with annotated reasoning references and difficulty scores, along with a suite of metrics designed to assess segmentation accuracy, reasoning quality, and efficiency jointly. Experimental results demonstrate that the proposed approach improves both reasoning efficiency and overall segmentation performance. Our work contributes novel perspectives towards efficient and interpretable multimodal understanding. The code and model will be publicly available. | [üîó Paper](http://arxiv.org/abs/2505.23727v1) |
| [SC-LoRA: Balancing Efficient Fine-tuning and Knowledge Preservation via
  Subspace-Constrained LoRA](http://arxiv.org/abs/2505.23724v1) | Minrui Luo, Fuhang Kuang, Yu Wang, Zirui Liu, Tianxing He | 2025-05-29 | Optimization, Fine-Tuning | Parameter-Efficient Fine-Tuning (PEFT) methods, particularly Low-Rank Adaptation (LoRA), are indispensable for efficiently customizing Large Language Models (LLMs). However, vanilla LoRA suffers from slow convergence speed and knowledge forgetting problems. Recent studies have leveraged the power of designed LoRA initialization, to enhance the fine-tuning efficiency, or to preserve knowledge in the pre-trained LLM. However, none of these works can address the two cases at the same time. To this end, we introduce Subspace-Constrained LoRA (SC-LoRA), a novel LoRA initialization framework engineered to navigate the trade-off between efficient fine-tuning and knowledge preservation. We achieve this by constraining the output of trainable LoRA adapters in a low-rank subspace, where the context information of fine-tuning data is most preserved while the context information of preserved knowledge is least retained, in a balanced way. Such constraint enables the trainable weights to primarily focus on the main features of fine-tuning data while avoiding damaging the preserved knowledge features. We provide theoretical analysis on our method, and conduct extensive experiments including safety preservation and world knowledge preservation, on various downstream tasks. In our experiments, SC-LoRA succeeds in delivering superior fine-tuning performance while markedly diminishing knowledge forgetting, surpassing contemporary LoRA initialization methods. | [üîó Paper](http://arxiv.org/abs/2505.23724v1) |
| [Label-Guided In-Context Learning for Named Entity Recognition](http://arxiv.org/abs/2505.23722v1) | Fan Bai, Hamid Hassanzadeh, Ardavan Saeedi, Mark Dredze | 2025-05-29 | Optimization, Responsible AI, Fine-Tuning, Model Evaluation, Training & Evaluation | In-context learning (ICL) enables large language models (LLMs) to perform new tasks using only a few demonstrations. In Named Entity Recognition (NER), demonstrations are typically selected based on semantic similarity to the test instance, ignoring training labels and resulting in suboptimal performance. We introduce DEER, a new method that leverages training labels through token-level statistics to improve ICL performance. DEER first enhances example selection with a label-guided, token-based retriever that prioritizes tokens most informative for entity recognition. It then prompts the LLM to revisit error-prone tokens, which are also identified using label statistics, and make targeted corrections. Evaluated on five NER datasets using four different LLMs, DEER consistently outperforms existing ICL methods and approaches the performance of supervised fine-tuning. Further analysis shows its effectiveness on both seen and unseen entities and its robustness in low-resource settings. | [üîó Paper](http://arxiv.org/abs/2505.23722v1) |
## üîπ Scaling Laws

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Anisotropic conformal Carroll field theories and their gravity duals](http://arxiv.org/abs/2505.23755v1) | Emilie Despontin, Stephane Detournay, Sudipta Dutta, Dima Fontaine | 2025-05-29 | Scaling Laws | We investigate anisotropic conformal Carroll field theories and their holographic duals. On the field theory side, we focus on the case with scaling exponent $z=0$ in two and three spacetime dimensions. These theories exhibit infinite-dimensional symmetry algebras, including supertranslations and superrotations, and are closely related to, but distinct from, Warped Conformal Field Theories. We construct the associated Carrollian stress tensor, derive its transformation properties, and analyse the structure of correlation functions under different choices of vacua. On the gravity side, we identify three and four-dimensional plane wave geometries whose isometry algebras realise the two- and three-dimensional Carroll algebra and anisotropic scale transformations. We propose, for each scaling exponent, a phase space of asymptotically-plane wave spacetimes and show that the residual diffeomorphisms reproduce the expected conformal Carroll field theory algebra, establishing a framework for anisotropic Carrollian holography. | [üîó Paper](http://arxiv.org/abs/2505.23755v1) |
## üîπ Model Evaluation

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Differential Information: An Information-Theoretic Perspective on
  Preference Optimization](http://arxiv.org/abs/2505.23761v1) | Yunjae Won, Hyunji Lee, Hyeonbin Hwang, Minjoon Seo | 2025-05-29 | Model Evaluation, Optimization, Responsible AI | Direct Preference Optimization (DPO) has become a standard technique for aligning language models with human preferences in a supervised manner. Despite its empirical success, the theoretical justification behind its log-ratio reward parameterization remains incomplete. In this work, we address this gap by utilizing the Differential Information Distribution (DID): a distribution over token sequences that captures the information gained during policy updates. First, we show that when preference labels encode the differential information required to transform a reference policy into a target policy, the log-ratio reward in DPO emerges as the uniquely optimal form for learning the target policy via preference optimization. This result naturally yields a closed-form expression for the optimal sampling distribution over rejected responses. Second, we find that the condition for preferences to encode differential information is fundamentally linked to an implicit assumption regarding log-margin ordered policies-an inductive bias widely used in preference optimization yet previously unrecognized. Finally, by analyzing the entropy of the DID, we characterize how learning low-entropy differential information reinforces the policy distribution, while high-entropy differential information induces a smoothing effect, which explains the log-likelihood displacement phenomenon. We validate our theoretical findings in synthetic experiments and extend them to real-world instruction-following datasets. Our results suggest that learning high-entropy differential information is crucial for general instruction-following, while learning low-entropy differential information benefits knowledge-intensive question answering. Overall, our work presents a unifying perspective on the DPO objective, the structure of preference data, and resulting policy behaviors through the lens of differential information. | [üîó Paper](http://arxiv.org/abs/2505.23761v1) |
| [Boosting Domain Incremental Learning: Selecting the Optimal Parameters
  is All You Need](http://arxiv.org/abs/2505.23744v1) | Qiang Wang, Xiang Song, Yuhang He, Jizhou Han, Chenhao Ding, Xinyuan Gao, Yihong Gong | 2025-05-29 | Model Evaluation, Optimization, Responsible AI, Fine-Tuning | Deep neural networks (DNNs) often underperform in real-world, dynamic settings where data distributions change over time. Domain Incremental Learning (DIL) offers a solution by enabling continual model adaptation, with Parameter-Isolation DIL (PIDIL) emerging as a promising paradigm to reduce knowledge conflicts. However, existing PIDIL methods struggle with parameter selection accuracy, especially as the number of domains and corresponding classes grows. To address this, we propose SOYO, a lightweight framework that improves domain selection in PIDIL. SOYO introduces a Gaussian Mixture Compressor (GMC) and Domain Feature Resampler (DFR) to store and balance prior domain data efficiently, while a Multi-level Domain Feature Fusion Network (MDFN) enhances domain feature extraction. Our framework supports multiple Parameter-Efficient Fine-Tuning (PEFT) methods and is validated across tasks such as image classification, object detection, and speech enhancement. Experimental results on six benchmarks demonstrate SOYO's consistent superiority over existing baselines, showcasing its robustness and adaptability in complex, evolving environments. The codes will be released in https://github.com/qwangcv/SOYO. | [üîó Paper](http://arxiv.org/abs/2505.23744v1) |
| [ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS](http://arxiv.org/abs/2505.23734v2) | Weijie Wang, Donny Y. Chen, Zeyu Zhang, Duochao Shi, Akide Liu, Bohan Zhuang | 2025-05-29 | Model Evaluation, Optimization, Responsible AI, Multimodal AI | Feed-forward 3D Gaussian Splatting (3DGS) models have recently emerged as a promising solution for novel view synthesis, enabling one-pass inference without the need for per-scene 3DGS optimization. However, their scalability is fundamentally constrained by the limited capacity of their encoders, leading to degraded performance or excessive memory consumption as the number of input views increases. In this work, we analyze feed-forward 3DGS frameworks through the lens of the Information Bottleneck principle and introduce ZPressor, a lightweight architecture-agnostic module that enables efficient compression of multi-view inputs into a compact latent state $Z$ that retains essential scene information while discarding redundancy. Concretely, ZPressor enables existing feed-forward 3DGS models to scale to over 100 input views at 480P resolution on an 80GB GPU, by partitioning the views into anchor and support sets and using cross attention to compress the information from the support views into anchor views, forming the compressed latent state $Z$. We show that integrating ZPressor into several state-of-the-art feed-forward 3DGS models consistently improves performance under moderate input views and enhances robustness under dense view settings on two large-scale benchmarks DL3DV-10K and RealEstate10K. The video results, code and trained models are available on our project page: https://lhmd.top/zpressor. | [üîó Paper](http://arxiv.org/abs/2505.23734v2) |
## üîπ Prompt Engineering

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Argus: Vision-Centric Reasoning with Grounded Chain-of-Thought](http://arxiv.org/abs/2505.23766v1) | Yunze Man, De-An Huang, Guilin Liu, Shiwei Sheng, Shilong Liu, Liang-Yan Gui, Jan Kautz, Yu-Xiong Wang, Zhiding Yu | 2025-05-29 | Prompt Engineering, Multimodal AI | Recent advances in multimodal large language models (MLLMs) have demonstrated remarkable capabilities in vision-language tasks, yet they often struggle with vision-centric scenarios where precise visual focus is needed for accurate reasoning. In this paper, we introduce Argus to address these limitations with a new visual attention grounding mechanism. Our approach employs object-centric grounding as visual chain-of-thought signals, enabling more effective goal-conditioned visual attention during multimodal reasoning tasks. Evaluations on diverse benchmarks demonstrate that Argus excels in both multimodal reasoning tasks and referring object grounding tasks. Extensive analysis further validates various design choices of Argus, and reveals the effectiveness of explicit language-guided visual region-of-interest engagement in MLLMs, highlighting the importance of advancing multimodal intelligence from a visual-centric perspective. Project page: https://yunzeman.github.io/argus/ | [üîó Paper](http://arxiv.org/abs/2505.23766v1) |
| [To Trust Or Not To Trust Your Vision-Language Model's Prediction](http://arxiv.org/abs/2505.23745v1) | Hao Dong, Moru Liu, Jian Liang, Eleni Chatzi, Olga Fink | 2025-05-29 | Prompt Engineering, Multimodal AI | Vision-Language Models (VLMs) have demonstrated strong capabilities in aligning visual and textual modalities, enabling a wide range of applications in multimodal understanding and generation. While they excel in zero-shot and transfer learning scenarios, VLMs remain susceptible to misclassification, often yielding confident yet incorrect predictions. This limitation poses a significant risk in safety-critical domains, where erroneous predictions can lead to severe consequences. In this work, we introduce TrustVLM, a training-free framework designed to address the critical challenge of estimating when VLM's predictions can be trusted. Motivated by the observed modality gap in VLMs and the insight that certain concepts are more distinctly represented in the image embedding space, we propose a novel confidence-scoring function that leverages this space to improve misclassification detection. We rigorously evaluate our approach across 17 diverse datasets, employing 4 architectures and 2 VLMs, and demonstrate state-of-the-art performance, with improvements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95 compared to existing baselines. By improving the reliability of the model without requiring retraining, TrustVLM paves the way for safer deployment of VLMs in real-world applications. The code will be available at https://github.com/EPFL-IMOS/TrustVLM. | [üîó Paper](http://arxiv.org/abs/2505.23745v1) |
| [FMG-Det: Foundation Model Guided Robust Object Detection](http://arxiv.org/abs/2505.23726v1) | Darryl Hannan, Timothy Doster, Henry Kvinge, Adam Attarian, Yijing Watkins | 2025-05-29 | Prompt Engineering | Collecting high quality data for object detection tasks is challenging due to the inherent subjectivity in labeling the boundaries of an object. This makes it difficult to not only collect consistent annotations across a dataset but also to validate them, as no two annotators are likely to label the same object using the exact same coordinates. These challenges are further compounded when object boundaries are partially visible or blurred, which can be the case in many domains. Training on noisy annotations significantly degrades detector performance, rendering them unusable, particularly in few-shot settings, where just a few corrupted annotations can impact model performance. In this work, we propose FMG-Det, a simple, efficient methodology for training models with noisy annotations. More specifically, we propose combining a multiple instance learning (MIL) framework with a pre-processing pipeline that leverages powerful foundation models to correct labels prior to training. This pre-processing pipeline, along with slight modifications to the detector head, results in state-of-the-art performance across a number of datasets, for both standard and few-shot scenarios, while being much simpler and more efficient than other approaches. | [üîó Paper](http://arxiv.org/abs/2505.23726v1) |
## üîπ Responsible AI

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Exposing the Impact of GenAI for Cybercrime: An Investigation into the
  Dark Side](http://arxiv.org/abs/2505.23733v1) | Truong, Luu, Binny M. Samuel | 2025-05-29 | Responsible AI, Security & Adversarial ML | In recent years, the rapid advancement and democratization of generative AI models have sparked significant debate over safety, ethical risks, and dual-use concerns, particularly in the context of cybersecurity. While anecdotally known, this paper provides empirical evidence regarding generative AI's association with malicious internet-related activities and cybercrime by examining the phenomenon through psychological frameworks of technological amplification and affordance theory. Using a quasi-experimental design with interrupted time series analysis, we analyze two datasets, one general and one cryptocurrency-focused, to empirically assess generative AI's role in cybercrime. The findings contribute to ongoing discussions about AI governance by balancing control and fostering innovation, underscoring the need for strategies to guide policymakers, inform AI developers and cybersecurity professionals, and educate the public to maximize AI's benefits while mitigating its risks. | [üîó Paper](http://arxiv.org/abs/2505.23733v1) |
## üîπ General AI

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Turbulence in Primordial Dark Matter Halos and Its Impact on the First
  Star Formation](http://arxiv.org/abs/2505.23768v1) | Meng-Yuan Ho, Ke-Jung Chen, Pei-Cheng Tung | 2025-05-29 | General AI | We present high-resolution simulations of the first star-forming clouds in 15 minihalos with masses ranging from $\sim 10^5$ to $10^7\ \text{M}_{\odot}$ at redshifts $z \sim 17 - 20$, using the \texttt{GIZMO} code. Our simulations incorporate detailed primordial gas physics and adopt initial conditions from the state-of-the-art TNG cosmological simulations. To achieve the required resolution, we apply a particle-splitting technique that increases the resolution of the original TNG data by a factor of $\sim 10^5$, reaching gas and dark matter particle masses of $0.2\ \text{M}_{\odot}$ and $80\ \text{M}_{\odot}$, respectively. This enables us to resolve gas accretion during the early assembly of minihalos and to capture the emergence of strong turbulent flows. We find that turbulence, driven by gas infall into the dark matter potential wells, is predominantly supersonic, with characteristic Mach numbers ranging from $1.8$ to $4.2$, increasing with halo mass. The supersonic turbulence effectively fragments the central gas cloud into multiple dense clumps, some of which form gravitationally bound cores and begin to collapse into the first stars. Our results suggest that supersonic turbulence is a common feature in minihalos and plays a key role in generating clumpy star-forming clouds, with important implications for the initial mass function of the first stars. | [üîó Paper](http://arxiv.org/abs/2505.23768v1) |
| [On completely monotonic functions](http://arxiv.org/abs/2505.23767v1) | Mostafa Najafi, Ali Morassaei | 2025-05-29 | General AI | Let $ f:(0,\infty)\rightarrow \Bbb{R} $ be a completely monotonic function. In this paper, we present some properties of this functions and several new classes of completely monotonic functions. We also give some special functions such that its have completely monotonic condition. | [üîó Paper](http://arxiv.org/abs/2505.23767v1) |
| [From Chat Logs to Collective Insights: Aggregative Question Answering](http://arxiv.org/abs/2505.23765v1) | Wentao Zhang, Woojeong Kim, Yuntian Deng | 2025-05-29 | General AI | Conversational agents powered by large language models (LLMs) are rapidly becoming integral to our daily interactions, generating unprecedented amounts of conversational data. Such datasets offer a powerful lens into societal interests, trending topics, and collective concerns. Yet, existing approaches typically treat these interactions as independent and miss critical insights that could emerge from aggregating and reasoning across large-scale conversation logs. In this paper, we introduce Aggregative Question Answering, a novel task requiring models to reason explicitly over thousands of user-chatbot interactions to answer aggregative queries, such as identifying emerging concerns among specific demographics. To enable research in this direction, we construct a benchmark, WildChat-AQA, comprising 6,027 aggregative questions derived from 182,330 real-world chatbot conversations. Experiments show that existing methods either struggle to reason effectively or incur prohibitive computational costs, underscoring the need for new approaches capable of extracting collective insights from large-scale conversational data. | [üîó Paper](http://arxiv.org/abs/2505.23765v1) |
| [Efficient sampling for sparse Bayesian learning using hierarchical prior
  normalization](http://arxiv.org/abs/2505.23753v1) | Jan Glaubitz, Youssef Marzouk | 2025-05-29 | General AI | We introduce an approach for efficient Markov chain Monte Carlo (MCMC) sampling for challenging high-dimensional distributions in sparse Bayesian learning (SBL). The core innovation involves using hierarchical prior-normalizing transport maps (TMs), which are deterministic couplings that transform the sparsity-promoting SBL prior into a standard normal one. We analytically derive these prior-normalizing TMs by leveraging the product-like form of SBL priors and Knothe--Rosenblatt (KR) rearrangements. These transform the complex target posterior into a simpler reference distribution equipped with a standard normal prior that can be sampled more efficiently. Specifically, one can leverage the standard normal prior by using more efficient, structure-exploiting samplers. Our numerical experiments on various inverse problems -- including signal deblurring, inverting the non-linear inviscid Burgers equation, and recovering an impulse image -- demonstrate significant performance improvements for standard MCMC techniques. | [üîó Paper](http://arxiv.org/abs/2505.23753v1) |
| [REOrdering Patches Improves Vision Models](http://arxiv.org/abs/2505.23751v1) | Declan Kutscher, David M. Chan, Yutong Bai, Trevor Darrell, Ritwik Gupta | 2025-05-29 | General AI | Sequence models such as transformers require inputs to be represented as one-dimensional sequences. In vision, this typically involves flattening images using a fixed row-major (raster-scan) order. While full self-attention is permutation-equivariant, modern long-sequence transformers increasingly rely on architectural approximations that break this invariance and introduce sensitivity to patch ordering. We show that patch order significantly affects model performance in such settings, with simple alternatives like column-major or Hilbert curves yielding notable accuracy shifts. Motivated by this, we propose REOrder, a two-stage framework for discovering task-optimal patch orderings. First, we derive an information-theoretic prior by evaluating the compressibility of various patch sequences. Then, we learn a policy over permutations by optimizing a Plackett-Luce policy using REINFORCE. This approach enables efficient learning in a combinatorial permutation space. REOrder improves top-1 accuracy over row-major ordering on ImageNet-1K by up to 3.01% and Functional Map of the World by 13.35%. | [üîó Paper](http://arxiv.org/abs/2505.23751v1) |
| [Perturbative Likelihoods for Large-Scale Structure of the Universe](http://arxiv.org/abs/2505.23750v1) | Rodrigo Voivodic | 2025-05-29 | General AI | This work presents a formalism for deriving likelihoods of the cosmological density field directly from first principles within Perturbation Theory (PT). By assuming a perturbative expansion around the Gaussian initial density field and additional stochastic components, we analytically compute two forms of the likelihood. Full marginalization over all underlying fields yields the likelihood of the observed density field, expressed in terms of its summary statistics (such as the power spectrum and bispectrum), which are naturally given by the formalism, and conditioned on model parameters. Marginalizing only over the stochastic fields results in the field-level likelihood. A key strength of this method is its ability to automatically specify the precise combinations of initial field covariances and PT expansion kernels required at each perturbative order (e.g., tree-level power spectrum and bispectrum, and the 1-loop power spectrum). This guarantees that the resulting likelihoods are fully consistent with PT at the chosen order of accuracy, avoiding ad-hoc choices in constructing the statistical model. | [üîó Paper](http://arxiv.org/abs/2505.23750v1) |
| [Brunn-Minkowski and Reverse Isoperimetric Inequalities for Dual
  Quermassintegrals](http://arxiv.org/abs/2505.23748v1) | Shay Sadovsky, Gaoyong Zhang | 2025-05-29 | General AI | This paper establishes two new geometric inequalities in the dual Brunn-Minkowski theory. The first, originally conjectured by Lutwak, is the Brunn-Minkowski inequality for dual quermassintegrals of origin-symmetric convex bodies. The second, generalizing Ball's volume ratio inequality, is a reverse isoperimetric inequality: among all origin-symmetric convex bodies in John's position, the cube maximizes the dual quermassintegrals. | [üîó Paper](http://arxiv.org/abs/2505.23748v1) |
| [Comparative of Genetic Fuzzy regression techniques for aeroacoustic
  phenomenons](http://arxiv.org/abs/2505.23746v1) | Hugo Henry, Kelly Cohen | 2025-05-29 | General AI | This study investigates the application of Genetic Fuzzy Systems (GFS) to model the self-noise generated by airfoils, a key issue in aeroaccoustics with significant implications for aerospace, automotive and drone applications. Using the publicly available Airfoil Self Noise dataset, various Fuzzy regression strategies are explored and compared. The paper evaluates a brute force Takagi Sugeno Kang (TSK) fuzzy system with high rule density, a cascading Geneti Fuzzy Tree (GFT) architecture and a novel clustered approach based on Fuzzy C-means (FCM) to reduce the model's complexity. This highlights the viability of clustering assisted fuzzy inference as an effective regression tool for complex aero accoustic phenomena. Keywords : Fuzzy logic, Regression, Cascading systems, Clustering and AI. | [üîó Paper](http://arxiv.org/abs/2505.23746v1) |
| [Fermion parity and quantum capacitance oscillation with partially
  separated Majorana and quasi-Majorana modes](http://arxiv.org/abs/2505.23741v1) | Tudor D. Stanescu, Sumanta Tewari | 2025-05-29 | General AI | In a recent experiment, flux dependent oscillations of the quantum capacitance were observed in a one dimensional spin-orbit coupled semiconductor superconductor heterostructure connected end to end via a quantum dot and threaded by a magnetic flux. In the topological superconducting phase of the heterostructure, the oscillations corresponding to different fermion parity sectors are shifted by half a period and can serve as a mechanism for fermion parity readout or fusion operations involving a pair of localized, well separated Majorana modes. In this work, we demonstrate that flux induced fermion parity dependent oscillations of the quantum capacitance in a disordered semiconductor superconductor quantum dot system can originate not only from topologically protected, spatially well separated Majorana zero modes (MZMs) localized at the wire ends, but also, generically, from partially separated Majorana modes with significant overlap, as well as from quasi-Majorana modes in the topologically trivial phase, which can be viewed as Andreev bound states whose constituent Majorana wave functions are slightly shifted relative to each other and have nonzero amplitude at opposite ends of the wire. Therefore, while the detection of flux dependent oscillations of quantum capacitance marks an important experimental advance, such observations alone do not constitute evidence of the presence of topological Majorana zero modes. | [üîó Paper](http://arxiv.org/abs/2505.23741v1) |
| [Apache Point rapid response characterization of primitive pre-impact
  detection asteroid 2024 RW$_1$](http://arxiv.org/abs/2505.23736v1) | Carl Ingebretsen, Bryce T. Bolin, Robert Jedicke, Peter Vere≈°, Christine H. Chen, Carey M. Lisse, Russet McMillan, Torrie Sutherland, Amanda J. Townsend | 2025-05-29 | General AI | Pre-impact detection asteroids (PIDAs) may be detected only a few hours before their impact with Earth, providing a brief opportunity to characterize them before impact. We describe the characterization of PIDA 2024 RW$_1$, which was discovered by the Catalina Sky Survey on 2024 September 4 at 05:43 UTC, before it entered the atmosphere near the northern Philippines at 16:39 UTC. We observed 2024 RW$_1$ with the Astrophysical Research Consortium Telescope Imaging Camera on the Apache Point Astrophysical Research Consortium's 3.5-m telescope on 2024 September 4 10:16 UTC. We obtained g, r, i, and z photometry of 2024 RW$_1$, yielding color indices of g-r = 0.47$\pm$0.04, r-i = 0.13$\pm$0.04, i-z = -0.11$\pm$0.07, and g-i = 0.60$\pm$0.04, corresponding to a spectral slope of 0.67$\pm$0.40~$\%$/100 nm. The closest match to an asteroid spectral type is with B-type asteroids from the C-complex. We detect variations in the time series photometry of the asteroid with an amplitude of $\sim$0.75, and a double-peaked rotation period of $\sim$1900 s. Assuming a visible albedo of 0.07 and a density of $\sim$1500 kg/m$^3$, and using the derived absolute magnitude of 32.2$\pm$0.5, we calculate that the asteroid has a diameter of 1.8$\pm$0.4 m and a total mass of $\sim$5000 kg. The most likely source of 2024 RW$_1$ is the 3:1 mean motion resonance followed by the $\nu_6$ resonance, according to NEOMOD3. | [üîó Paper](http://arxiv.org/abs/2505.23736v1) |
| [DTBIA: An Immersive Visual Analytics System for Brain-Inspired Research](http://arxiv.org/abs/2505.23730v1) | Jun-Hsiang Yao, Mingzheng Li, Jiayi Liu, Yuxiao Li, Jielin Feng, Jun Han, Qibao Zheng, Jianfeng Feng, Siming Chen | 2025-05-29 | General AI | The Digital Twin Brain (DTB) is an advanced artificial intelligence framework that integrates spiking neurons to simulate complex cognitive functions and collaborative behaviors. For domain experts, visualizing the DTB's simulation outcomes is essential to understanding complex cognitive activities. However, this task poses significant challenges due to DTB data's inherent characteristics, including its high-dimensionality, temporal dynamics, and spatial complexity. To address these challenges, we developed DTBIA, an Immersive Visual Analytics System for Brain-Inspired Research. In collaboration with domain experts, we identified key requirements for effectively visualizing spatiotemporal and topological patterns at multiple levels of detail. DTBIA incorporates a hierarchical workflow - ranging from brain regions to voxels and slice sections - along with immersive navigation and a 3D edge bundling algorithm to enhance clarity and provide deeper insights into both functional (BOLD) and structural (DTI) brain data. The utility and effectiveness of DTBIA are validated through two case studies involving with brain research experts. The results underscore the system's role in enhancing the comprehension of complex neural behaviors and interactions. | [üîó Paper](http://arxiv.org/abs/2505.23730v1) |
| [The ambiguous AT2022rze: Changing-look AGN mimicking a supernova in a
  merging galaxy system](http://arxiv.org/abs/2505.23731v1) | P. J. Pessi, R. Lunnan, J. Sollerman, L. Yan, A. Le Reste, Y. Yao, S. Nordblom, Y. Sharma, M. Gilfanov, R. Sunyaev, S. Schulze, J. Johansson, A. Gangopadhyay, K. Tristram, M. Hayes, C. Fransson, Y. Hu, S. J. Brennan, S. Rose, K. De, P. Charalampopoulos, A. Gkini, M. J. Graham, C. P. Guti√©rrez, S. Mattila, T. Nagao, I. P√©rez-Fournon, F. Poidevin, J. S. Bloom, J. Brugger, T. X. Chen, M. M. Kasliwal, F. J. Masci, J. N. Purdum | 2025-05-29 | General AI | AT2022rze is a luminous, ambiguous transient located South-East of the geometric center of its host galaxy at redshift z = 0.08. The host appears to be formed by a merging galaxy system. The observed characteristics of AT2022rze are reminiscent of active galactic nuclei (AGN), tidal disruption events (TDEs), and superluminous supernovae (SLSNe). The transient reached a peak absolute magnitude of -20.2 +- 0.2 mag, showing a sharp rise (trise,1/e = 27.5 +- 0.6 days) followed by a slow decline (tdec,1/e = 382.9 +- 0.6). Its bumpy light curve and narrow Balmer lines indicate the presence of gas (and dust). Its light curve shows rather red colors, indicating that the transient could be affected by significant host extinction. The spectra reveal coronal lines, indicative of high-energy (X-ray/UV) emission. Archival data reveal no prior activity at this location, disfavoring a steady-state AGN, although an optical spectrum obtained prior to the transient is consistent with an AGN classification of the host. Based on this, we conclude that the transient most likely represents a Changing-look AGN at the center of the smallest component of the merging system. | [üîó Paper](http://arxiv.org/abs/2505.23731v1) |
| [Group Convolutional Neural Network Ground State of the Quantum Dimer
  Model](http://arxiv.org/abs/2505.23728v1) | Ojasvi Sharma, Sandipan Manna, Prashant Shekhar Rao, G J Sreejith | 2025-05-29 | General AI | We estimate the ground state of the square lattice Quantum Dimer Model in a $\rm{p4m}$-symmetric Group Convolutional Neural Network (GCNN) representation and show that results in agreement with exact diagonalization (ED) and quantum Monte Carlo (QMC) can be obtained with a $\mathcal{L}=2$ layer network. In systems of linear size $L=8$ with Hilbert space dimension $3.1\times 10^8$, GCNN shows fidelity as high as $0.99999$ with ED. For $12\leq L\leq 32$, we find excellent agreement with QMC estimates of energy, order parameters and correlation functions. The network is optimized by minimizing the energy estimated from a Metropolis algorithm assisted by a directed loop sampler. We analyze the quantum geometric tensor at the minima for $\mathcal{L}=1,2$ and $3$ and show that the empirical quantum dimension saturates with increasing network complexity due to Metropolis sampling constraints. | [üîó Paper](http://arxiv.org/abs/2505.23728v1) |
| [COBRA: Contextual Bandit Algorithm for Ensuring Truthful Strategic
  Agents](http://arxiv.org/abs/2505.23720v1) | Arun Verma, Indrajit Saha, Makoto Yokoo, Bryan Kian Hsiang Low | 2025-05-29 | General AI | This paper considers a contextual bandit problem involving multiple agents, where a learner sequentially observes the contexts and the agent's reported arms, and then selects the arm that maximizes the system's overall reward. Existing work in contextual bandits assumes that agents truthfully report their arms, which is unrealistic in many real-life applications. For instance, consider an online platform with multiple sellers; some sellers may misrepresent product quality to gain an advantage, such as having the platform preferentially recommend their products to online users. To address this challenge, we propose an algorithm, COBRA, for contextual bandit problems involving strategic agents that disincentivize their strategic behavior without using any monetary incentives, while having incentive compatibility and a sub-linear regret guarantee. Our experimental results also validate the different performance aspects of our proposed algorithm. | [üîó Paper](http://arxiv.org/abs/2505.23720v1) |
