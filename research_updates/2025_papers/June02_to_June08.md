# 📌 AI Research Papers (June02 to June08)

## 🔹 LLM

| 📄 Title | 🖊 Authors | 📅 Date | 🏷 Tags | 📜 Summary | 🔗 Link |
|---------|---------|---------|---------|---------|---------|
| [Inference-Time Hyper-Scaling with KV Cache Compression](http://arxiv.org/abs/2506.05345v1) | Adrian Łańcucki, Konrad Staniszewski, Piotr Nawrot, Edoardo M. Ponti | 2025-06-05 | LLM, Scaling Laws | Inference-time scaling trades efficiency for increased reasoning accuracy by generating longer or more parallel sequences. However, in Transformer LLMs, generation cost is bottlenecked by the size of the key-value (KV) cache, rather than the number of generated tokens. Hence, we explore inference-time hyper-scaling: by compressing the KV cache, we can generate more tokens within the same compute budget and further improve the accuracy of scaled inference. The success of this approach, however, hinges on the ability of compression methods to preserve accuracy even at high compression ratios. To make hyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a novel method for sparsifying KV caches that only requires 1K training steps to achieve 8$\times$ compression, while maintaining better accuracy than training-free sparse attention. Instead of prematurely discarding cached tokens, DMS delays token eviction, implicitly merging representations and preserving critical information. We demonstrate the effectiveness of inference-time hyper-scaling with DMS on multiple families of LLMs, showing that it boosts accuracy for comparable inference runtime and memory load. For instance, we enhance Qwen-R1 32B by an average of 9.1 points on AIME 24, 7.6 on GPQA, and 9.6 on LiveCodeBench across compute budgets. | [🔗 Paper](http://arxiv.org/abs/2506.05345v1) |
| [Exploring Diffusion Transformer Designs via Grafting](http://arxiv.org/abs/2506.05340v2) | Keshigeyan Chandrasegaran, Michael Poli, Daniel Y. Fu, Dongjun Kim, Lea M. Hadzic, Manling Li, Agrim Gupta, Stefano Massaroli, Azalia Mirhoseini, Juan Carlos Niebles, Stefano Ermon, Li Fei-Fei | 2025-06-05 | LLM, Diffusion Models | Designing model architectures requires decisions such as selecting operators (e.g., attention, convolution) and configurations (e.g., depth, width). However, evaluating the impact of these decisions on model quality requires costly pretraining, limiting architectural investigation. Inspired by how new software is built on existing code, we ask: can new architecture designs be studied using pretrained models? To this end, we present grafting, a simple approach for editing pretrained diffusion transformers (DiTs) to materialize new architectures under small compute budgets. Informed by our analysis of activation behavior and attention locality, we construct a testbed based on the DiT-XL/2 design to study the impact of grafting on model quality. Using this testbed, we develop a family of hybrid designs via grafting: replacing softmax attention with gated convolution, local attention, and linear attention, and replacing MLPs with variable expansion ratio and convolutional variants. Notably, many hybrid designs achieve good quality (FID: 2.38-2.64 vs. 2.27 for DiT-XL/2) using <2% pretraining compute. We then graft a text-to-image model (PixArt-Sigma), achieving a 1.43x speedup with less than a 2% drop in GenEval score. Finally, we present a case study that restructures DiT-XL/2 by converting every pair of sequential transformer blocks into parallel blocks via grafting. This reduces model depth by 2x and yields better quality (FID: 2.77) than other models of comparable depth. Together, we show that new diffusion model designs can be explored by grafting pretrained DiTs, with edits ranging from operator replacement to architecture restructuring. Code and grafted models: https://grafting.stanford.edu | [🔗 Paper](http://arxiv.org/abs/2506.05340v2) |
| [Revisiting Depth Representations for Feed-Forward 3D Gaussian Splatting](http://arxiv.org/abs/2506.05327v1) | Duochao Shi, Weijie Wang, Donny Y. Chen, Zeyu Zhang, Jia-Wang Bian, Bohan Zhuang, Chunhua Shen | 2025-06-05 | LLM | Depth maps are widely used in feed-forward 3D Gaussian Splatting (3DGS) pipelines by unprojecting them into 3D point clouds for novel view synthesis. This approach offers advantages such as efficient training, the use of known camera poses, and accurate geometry estimation. However, depth discontinuities at object boundaries often lead to fragmented or sparse point clouds, degrading rendering quality -- a well-known limitation of depth-based representations. To tackle this issue, we introduce PM-Loss, a novel regularization loss based on a pointmap predicted by a pre-trained transformer. Although the pointmap itself may be less accurate than the depth map, it effectively enforces geometric smoothness, especially around object boundaries. With the improved depth map, our method significantly improves the feed-forward 3DGS across various architectures and scenes, delivering consistently better rendering results. Our project page: https://aim-uofa.github.io/PMLoss | [🔗 Paper](http://arxiv.org/abs/2506.05327v1) |
| [Generalizable, real-time neural decoding with hybrid state-space models](http://arxiv.org/abs/2506.05320v1) | Avery Hee-Woon Ryoo, Nanda H. Krishna, Ximeng Mao, Mehdi Azabou, Eva L. Dyer, Matthew G. Perich, Guillaume Lajoie | 2025-06-05 | LLM, Production and Deployment | Real-time decoding of neural activity is central to neuroscience and neurotechnology applications, from closed-loop experiments to brain-computer interfaces, where models are subject to strict latency constraints. Traditional methods, including simple recurrent neural networks, are fast and lightweight but often struggle to generalize to unseen data. In contrast, recent Transformer-based approaches leverage large-scale pretraining for strong generalization performance, but typically have much larger computational requirements and are not always suitable for low-resource or real-time settings. To address these shortcomings, we present POSSM, a novel hybrid architecture that combines individual spike tokenization via a cross-attention module with a recurrent state-space model (SSM) backbone to enable (1) fast and causal online prediction on neural activity and (2) efficient generalization to new sessions, individuals, and tasks through multi-dataset pretraining. We evaluate POSSM's decoding performance and inference speed on intracortical decoding of monkey motor tasks, and show that it extends to clinical applications, namely handwriting and speech decoding in human subjects. Notably, we demonstrate that pretraining on monkey motor-cortical recordings improves decoding performance on the human handwriting task, highlighting the exciting potential for cross-species transfer. In all of these tasks, we find that POSSM achieves decoding accuracy comparable to state-of-the-art Transformers, at a fraction of the inference cost (up to 9x faster on GPU). These results suggest that hybrid SSMs are a promising approach to bridging the gap between accuracy, inference speed, and generalization when training neural decoders for real-time, closed-loop applications. | [🔗 Paper](http://arxiv.org/abs/2506.05320v1) |
| [The Arm Qubit: A Superconducting Qubit Co-Designed for Coherence and
  Coupling](http://arxiv.org/abs/2506.05315v1) | Jeremy B. Kline, Alec Yen, Stanley Chen, Kevin P. O'Brien | 2025-06-05 | LLM | We present a superconducting qubit which consists of two strongly coupled modes: one for data storage and one for coupling, allowing faster, higher-fidelity entangling gates and readout. The use of a dedicated coupling mode allows nonlinear couplings of several hundred MHz between the data mode and other elements, with minimal linear coupling to the data mode. Including decoherence, simulations show that this architecture enables microwave-only CZ gates with an infidelity of $8.6\times10^{-5}$ in 17 ns and always-on ZZ interaction less than 0.4 kHz. Numerical simulations also show readout with state assignment error of $1\times10^{-4}$ in 27 ns (assuming quantum efficiency $\eta=0.5$), Purcell-limited lifetime of 167 ms without a Purcell filter, and a mechanism to suppress shot-noise dephasing ($1/\Gamma_{\phi}=15.8$ ms). Single-qubit gate infidelities are below $1\times10^{-5}$ including decoherence. These beyond experimental state-of-the-art gate and readout fidelities rely only on capacitive coupling between arm qubits, making the arm qubit a promising scalable building block for fault-tolerant quantum computers. | [🔗 Paper](http://arxiv.org/abs/2506.05315v1) |
## 🔹 Diffusion Models

| 📄 Title | 🖊 Authors | 📅 Date | 🏷 Tags | 📜 Summary | 🔗 Link |
|---------|---------|---------|---------|---------|---------|
| [Contrastive Flow Matching](http://arxiv.org/abs/2506.05350v1) | George Stoica, Vivek Ramanujan, Xiang Fan, Ali Farhadi, Ranjay Krishna, Judy Hoffman | 2025-06-05 | Diffusion Models | Unconditional flow-matching trains diffusion models to transport samples from a source distribution to a target distribution by enforcing that the flows between sample pairs are unique. However, in conditional settings (e.g., class-conditioned models), this uniqueness is no longer guaranteed--flows from different conditions may overlap, leading to more ambiguous generations. We introduce Contrastive Flow Matching, an extension to the flow matching objective that explicitly enforces uniqueness across all conditional flows, enhancing condition separation. Our approach adds a contrastive objective that maximizes dissimilarities between predicted flows from arbitrary sample pairs. We validate Contrastive Flow Matching by conducting extensive experiments across varying model architectures on both class-conditioned (ImageNet-1k) and text-to-image (CC3M) benchmarks. Notably, we find that training models with Contrastive Flow Matching (1) improves training speed by a factor of up to 9x, (2) requires up to 5x fewer de-noising steps and (3) lowers FID by up to 8.9 compared to training the same models with flow matching. We release our code at: https://github.com/gstoica27/DeltaFM.git. | [🔗 Paper](http://arxiv.org/abs/2506.05350v1) |
| [Heterogeneous response and non-Markovianity in the microrheology of
  semisolid viscoelastic materials](http://arxiv.org/abs/2506.05311v1) | T. N. Azevedo, L. G. Rizzi | 2025-06-05 | Diffusion Models | Recent works indicate that heterogeneous response and non-Markovianity may yield recognizable hallmarks in the microrheology of semisolid viscoelastic materials. Here we perform numerical simulations using a non-Markovian overdamped Langevin approach to explore how the microrheology experienced by probe particles immersed in an effective semisolid material can be influenced by its micro-heterogeneities. Our results show that, besides affecting the mean squared displacement, the time-dependent diffusion coefficient, and the shear moduli, the micro-heterogeneities lead to displacement distributions that deviate from the usual Gaussian behavior. In addition, our study provides an analytical way to characterize the micro-heterogeneities of semisolid viscoelastic materials through their microrheology. | [🔗 Paper](http://arxiv.org/abs/2506.05311v1) |
| [Learning normalized image densities via dual score matching](http://arxiv.org/abs/2506.05310v1) | Florentin Guth, Zahra Kadkhodaie, Eero P Simoncelli | 2025-06-05 | Diffusion Models | Learning probability models from data is at the heart of many machine learning endeavors, but is notoriously difficult due to the curse of dimensionality. We introduce a new framework for learning \emph{normalized} energy (log probability) models that is inspired from diffusion generative models, which rely on networks optimized to estimate the score. We modify a score network architecture to compute an energy while preserving its inductive biases. The gradient of this energy network with respect to its input image is the score of the learned density, which can be optimized using a denoising objective. Importantly, the gradient with respect to the noise level provides an additional score that can be optimized with a novel secondary objective, ensuring consistent and normalized energies across noise levels. We train an energy network with this \emph{dual} score matching objective on the ImageNet64 dataset, and obtain a cross-entropy (negative log likelihood) value comparable to the state of the art. We further validate our approach by showing that our energy model \emph{strongly generalizes}: estimated log probabilities are nearly independent of the specific images in the training set. Finally, we demonstrate that both image probability and dimensionality of local neighborhoods vary significantly with image content, in contrast with traditional assumptions such as concentration of measure or support on a low-dimensional manifold. | [🔗 Paper](http://arxiv.org/abs/2506.05310v1) |
## 🔹 RLHF

| 📄 Title | 🖊 Authors | 📅 Date | 🏷 Tags | 📜 Summary | 🔗 Link |
|---------|---------|---------|---------|---------|---------|
| [Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity
  Analysis Between Alignment and Fine-tuning Datasets](http://arxiv.org/abs/2506.05346v1) | Lei Hsiung, Tianyu Pang, Yung-Chen Tang, Linyue Song, Tsung-Yi Ho, Pin-Yu Chen, Yaoqing Yang | 2025-06-05 | RLHF, Optimization, Security & Adversarial ML | Recent advancements in large language models (LLMs) have underscored their vulnerability to safety alignment jailbreaks, particularly when subjected to downstream fine-tuning. However, existing mitigation strategies primarily focus on reactively addressing jailbreak incidents after safety guardrails have been compromised, removing harmful gradients during fine-tuning, or continuously reinforcing safety alignment throughout fine-tuning. As such, they tend to overlook a critical upstream factor: the role of the original safety-alignment data. This paper therefore investigates the degradation of safety guardrails through the lens of representation similarity between upstream alignment datasets and downstream fine-tuning tasks. Our experiments demonstrate that high similarity between these datasets significantly weakens safety guardrails, making models more susceptible to jailbreaks. Conversely, low similarity between these two types of datasets yields substantially more robust models and thus reduces harmfulness score by up to 10.33%. By highlighting the importance of upstream dataset design in the building of durable safety guardrails and reducing real-world vulnerability to jailbreak attacks, these findings offer actionable insights for fine-tuning service providers. | [🔗 Paper](http://arxiv.org/abs/2506.05346v1) |
| [Direct Numerical Layout Generation for 3D Indoor Scene Synthesis via
  Spatial Reasoning](http://arxiv.org/abs/2506.05341v1) | Xingjian Ran, Yixuan Li, Linning Xu, Mulin Yu, Bo Dai | 2025-06-05 | RLHF, Prompt Engineering | Realistic 3D indoor scene synthesis is vital for embodied AI and digital content creation. It can be naturally divided into two subtasks: object generation and layout generation. While recent generative models have significantly advanced object-level quality and controllability, layout generation remains challenging due to limited datasets. Existing methods either overfit to these datasets or rely on predefined constraints to optimize numerical layout that sacrifice flexibility. As a result, they fail to generate scenes that are both open-vocabulary and aligned with fine-grained user instructions. We introduce DirectLayout, a framework that directly generates numerical 3D layouts from text descriptions using generalizable spatial reasoning of large language models (LLMs). DirectLayout decomposes the generation into three stages: producing a Bird's-Eye View (BEV) layout, lifting it into 3D space, and refining object placements. To enable explicit spatial reasoning and help the model grasp basic principles of object placement, we employ Chain-of-Thought (CoT) Activation based on the 3D-Front dataset. Additionally, we design CoT-Grounded Generative Layout Reward to enhance generalization and spatial planning. During inference, DirectLayout addresses asset-layout mismatches via Iterative Asset-Layout Alignment through in-context learning. Extensive experiments demonstrate that DirectLayout achieves impressive semantic consistency, generalization and physical plausibility. | [🔗 Paper](http://arxiv.org/abs/2506.05341v1) |
| [Flattery, Fluff, and Fog: Diagnosing and Mitigating Idiosyncratic Biases
  in Preference Models](http://arxiv.org/abs/2506.05339v1) | Anirudh Bharadwaj, Chaitanya Malaviya, Nitish Joshi, Mark Yatskar | 2025-06-05 | RLHF, Model Evaluation, Training & Evaluation, Responsible AI | Language models serve as proxies for human preference judgements in alignment and evaluation, yet they exhibit systematic miscalibration, prioritizing superficial patterns over substantive qualities. This bias manifests as overreliance on features like length, structure, and style, leading to issues like reward hacking and unreliable evaluations. Evidence suggests these biases originate in artifacts in human training data. In this work, we systematically investigate the relationship between training data biases and preference model miscalibration across five idiosyncratic features of language model generations: length, structure, jargon, sycophancy and vagueness. Using controlled counterfactual pairs, we first quantify the extent to which preference models favor responses with magnified biases (skew), finding this preference occurs in >60% of instances, and model preferences show high miscalibration (~40%) compared to human preferences. Notably, bias features only show mild negative correlations to human preference labels (mean r_human = -0.12) but show moderately strong positive correlations with labels from a strong reward model (mean r_model = +0.36), suggesting that models may overrely on spurious cues. To mitigate these issues, we propose a simple post-training method based on counterfactual data augmentation (CDA) using synthesized contrastive examples. Finetuning models with CDA reduces average miscalibration from 39.4% to 32.5% and average absolute skew difference from 20.5% to 10.0%, while maintaining overall RewardBench performance, showing that targeted debiasing is effective for building reliable preference models. | [🔗 Paper](http://arxiv.org/abs/2506.05339v1) |
| [Seeing the Invisible: Machine learning-Based QPI Kernel Extraction via
  Latent Alignment](http://arxiv.org/abs/2506.05325v1) | Yingshuai Ji, Haomin Zhuang, Matthew Toole, James McKenzie, Xiaolong Liu, Xiangliang Zhang | 2025-06-05 | RLHF | Quasiparticle interference (QPI) imaging is a powerful tool for probing electronic structures in quantum materials, but extracting the single-scatterer QPI pattern (i.e., the kernel) from a multi-scatterer image remains a fundamentally ill-posed inverse problem. In this work, we propose the first AI-based framework for QPI kernel extraction. We introduce a two-step learning strategy that decouples kernel representation learning from observation-to-kernel inference. In the first step, we train a variational autoencoder to learn a compact latent space of scattering kernels. In the second step, we align the latent representation of QPI observations with those of the pre-learned kernels using a dedicated encoder. This design enables the model to infer kernels robustly even under complex, entangled scattering conditions. We construct a diverse and physically realistic QPI dataset comprising 100 unique kernels and evaluate our method against a direct one-step baseline. Experimental results demonstrate that our approach achieves significantly higher extraction accuracy, and improved generalization to unseen kernels. | [🔗 Paper](http://arxiv.org/abs/2506.05325v1) |
| [Improving Data Efficiency for LLM Reinforcement Fine-tuning Through
  Difficulty-targeted Online Data Selection and Rollout Replay](http://arxiv.org/abs/2506.05316v1) | Yifan Sun, Jingyan Shen, Yibin Wang, Tianyu Chen, Zhendong Wang, Mingyuan Zhou, Huan Zhang | 2025-06-05 | RLHF, Optimization | Reinforcement learning (RL) has become an effective approach for fine-tuning large language models (LLMs), particularly to enhance their reasoning capabilities. However, RL fine-tuning remains highly resource-intensive, and existing work has largely overlooked the problem of data efficiency. In this paper, we propose two techniques to improve data efficiency in LLM RL fine-tuning: difficulty-targeted online data selection and rollout replay. We introduce the notion of adaptive difficulty to guide online data selection, prioritizing questions of moderate difficulty that are more likely to yield informative learning signals. To estimate adaptive difficulty efficiently, we develop an attention-based framework that requires rollouts for only a small reference set of questions. The adaptive difficulty of the remaining questions is then estimated based on their similarity to this set. To further reduce rollout cost, we introduce a rollout replay mechanism that reuses recent rollouts, lowering per-step computation while maintaining stable updates. Extensive experiments across 6 LLM-dataset combinations show that our method reduces RL fine-tuning time by 25% to 65% to reach the same level of performance as the original GRPO algorithm. | [🔗 Paper](http://arxiv.org/abs/2506.05316v1) |
## 🔹 Multimodal AI

| 📄 Title | 🖊 Authors | 📅 Date | 🏷 Tags | 📜 Summary | 🔗 Link |
|---------|---------|---------|---------|---------|---------|
| [VideoMathQA: Benchmarking Mathematical Reasoning via Multimodal
  Understanding in Videos](http://arxiv.org/abs/2506.05349v1) | Hanoona Rasheed, Abdelrahman Shaker, Anqi Tang, Muhammad Maaz, Ming-Hsuan Yang, Salman Khan, Fahad Khan | 2025-06-05 | Multimodal AI, Model Evaluation, Training & Evaluation | Mathematical reasoning in real-world video settings presents a fundamentally different challenge than in static images or text. It requires interpreting fine-grained visual information, accurately reading handwritten or digital text, and integrating spoken cues, often dispersed non-linearly over time. In such multimodal contexts, success hinges not just on perception, but on selectively identifying and integrating the right contextual details from a rich and noisy stream of content. To this end, we introduce VideoMathQA, a benchmark designed to evaluate whether models can perform such temporally extended cross-modal reasoning on videos. The benchmark spans 10 diverse mathematical domains, covering videos ranging from 10 seconds to over 1 hour. It requires models to interpret structured visual content, understand instructional narratives, and jointly ground concepts across visual, audio, and textual modalities. We employ graduate-level experts to ensure high quality, totaling over $920$ man-hours of annotation. To reflect real-world scenarios, questions are designed around three core reasoning challenges: direct problem solving, where answers are grounded in the presented question; conceptual transfer, which requires applying learned methods to new problems; and deep instructional comprehension, involving multi-step reasoning over extended explanations and partially worked-out solutions. Each question includes multi-step reasoning annotations, enabling fine-grained diagnosis of model capabilities. Through this benchmark, we highlight the limitations of existing approaches and establish a systematic evaluation framework for models that must reason, rather than merely perceive, across temporally extended and modality-rich mathematical problem settings. Our benchmark and evaluation code are available at: https://mbzuai-oryx.github.io/VideoMathQA | [🔗 Paper](http://arxiv.org/abs/2506.05349v1) |
| [SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs](http://arxiv.org/abs/2506.05344v1) | Jiahui Wang, Zuyan Liu, Yongming Rao, Jiwen Lu | 2025-06-05 | Multimodal AI, Optimization | Multimodal Large Language Models (MLLMs) are commonly derived by extending pre-trained Large Language Models (LLMs) with visual capabilities. In this work, we investigate how MLLMs process visual inputs by analyzing their attention mechanisms. We reveal a surprising sparsity phenomenon: only a small subset (approximately less than 5%) of attention heads in LLMs actively contribute to visual understanding, termed visual heads. To identify these heads efficiently, we design a training-free framework that quantifies head-level visual relevance through targeted response analysis. Building on this discovery, we introduce SparseMM, a KV-Cache optimization strategy that allocates asymmetric computation budgets to heads in LLMs based on their visual scores, leveraging the sparity of visual heads for accelerating the inference of MLLMs. Compared with prior KV-Cache acceleration methods that ignore the particularity of visual, SparseMM prioritizes stress and retaining visual semantics during decoding. Extensive evaluations across mainstream multimodal benchmarks demonstrate that SparseMM achieves superior accuracy-efficiency trade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52% memory reduction during generation while maintaining performance parity on efficiency test. Our project is open sourced at https://github.com/CR400AF-A/SparseMM. | [🔗 Paper](http://arxiv.org/abs/2506.05344v1) |
| [ContentV: Efficient Training of Video Generation Models with Limited
  Compute](http://arxiv.org/abs/2506.05343v1) | Wenfeng Lin, Renjie Chen, Boyuan Liu, Shiyue Yan, Ruoyu Feng, Jiangchuan Wei, Yichen Zhang, Yimeng Zhou, Chao Feng, Jiao Ran, Qi Wu, Zuotao Liu, Mingyu Guo | 2025-06-05 | Multimodal AI, RLHF, Diffusion Models | Recent advances in video generation demand increasingly efficient training recipes to mitigate escalating computational costs. In this report, we present ContentV, an 8B-parameter text-to-video model that achieves state-of-the-art performance (85.14 on VBench) after training on 256 x 64GB Neural Processing Units (NPUs) for merely four weeks. ContentV generates diverse, high-quality videos across multiple resolutions and durations from text prompts, enabled by three key innovations: (1) A minimalist architecture that maximizes reuse of pre-trained image generation models for video generation; (2) A systematic multi-stage training strategy leveraging flow matching for enhanced efficiency; and (3) A cost-effective reinforcement learning with human feedback framework that improves generation quality without requiring additional human annotations. All the code and models are available at: https://contentv.github.io. | [🔗 Paper](http://arxiv.org/abs/2506.05343v1) |
| [Refer to Anything with Vision-Language Prompts](http://arxiv.org/abs/2506.05342v1) | Shengcao Cao, Zijun Wei, Jason Kuen, Kangning Liu, Lingzhi Zhang, Jiuxiang Gu, HyunJoon Jung, Liang-Yan Gui, Yu-Xiong Wang | 2025-06-05 | Multimodal AI, Model Evaluation, Training & Evaluation | Recent image segmentation models have advanced to segment images into high-quality masks for visual entities, and yet they cannot provide comprehensive semantic understanding for complex queries based on both language and vision. This limitation reduces their effectiveness in applications that require user-friendly interactions driven by vision-language prompts. To bridge this gap, we introduce a novel task of omnimodal referring expression segmentation (ORES). In this task, a model produces a group of masks based on arbitrary prompts specified by text only or text plus reference visual entities. To address this new challenge, we propose a novel framework to "Refer to Any Segmentation Mask Group" (RAS), which augments segmentation models with complex multimodal interactions and comprehension via a mask-centric large multimodal model. For training and benchmarking ORES models, we create datasets MaskGroups-2M and MaskGroups-HQ to include diverse mask groups specified by text and reference entities. Through extensive evaluation, we demonstrate superior performance of RAS on our new ORES task, as well as classic referring expression segmentation (RES) and generalized referring expression segmentation (GRES) tasks. Project page: https://Ref2Any.github.io. | [🔗 Paper](http://arxiv.org/abs/2506.05342v1) |
| [VideoMolmo: Spatio-Temporal Grounding Meets Pointing](http://arxiv.org/abs/2506.05336v1) | Ghazi Shazan Ahmad, Ahmed Heakl, Hanan Gani, Abdelrahman Shaker, Zhiqiang Shen, Ranjay Krishna, Fahad Shahbaz Khan, Salman Khan | 2025-06-05 | Multimodal AI, Ongoing Learning, AI Safety | Spatio-temporal localization is vital for precise interactions across diverse domains, from biological research to autonomous navigation and interactive interfaces. Current video-based approaches, while proficient in tracking, lack the sophisticated reasoning capabilities of large language models, limiting their contextual understanding and generalization. We introduce VideoMolmo, a large multimodal model tailored for fine-grained spatio-temporal pointing conditioned on textual descriptions. Building upon the Molmo architecture, VideoMolmo incorporates a temporal module utilizing an attention mechanism to condition each frame on preceding frames, ensuring temporal consistency. Additionally, our novel temporal mask fusion pipeline employs SAM2 for bidirectional point propagation, significantly enhancing coherence across video sequences. This two-step decomposition, i.e., first using the LLM to generate precise pointing coordinates, then relying on a sequential mask-fusion module to produce coherent segmentation, not only simplifies the task for the language model but also enhances interpretability. Due to the lack of suitable datasets, we curate a comprehensive dataset comprising 72k video-caption pairs annotated with 100k object points. To evaluate the generalization of VideoMolmo, we introduce VPoS-Bench, a challenging out-of-distribution benchmark spanning five real-world scenarios: Cell Tracking, Egocentric Vision, Autonomous Driving, Video-GUI Interaction, and Robotics. We also evaluate our model on Referring Video Object Segmentation (Refer-VOS) and Reasoning VOS tasks. In comparison to existing models, VideoMolmo substantially improves spatio-temporal pointing accuracy and reasoning capability. Our code and models are publicly available at https://github.com/mbzuai-oryx/VideoMolmo. | [🔗 Paper](http://arxiv.org/abs/2506.05336v1) |
| [Unleashing Hour-Scale Video Training for Long Video-Language
  Understanding](http://arxiv.org/abs/2506.05332v1) | Jingyang Lin, Jialian Wu, Ximeng Sun, Ze Wang, Jiang Liu, Yusheng Su, Xiaodong Yu, Hao Chen, Jiebo Luo, Zicheng Liu, Emad Barsoum | 2025-06-05 | Multimodal AI | Recent long-form video-language understanding benchmarks have driven progress in video large multimodal models (Video-LMMs). However, the scarcity of well-annotated long videos has left the training of hour-long Video-LLMs underexplored. To close this gap, we present VideoMarathon, a large-scale hour-long video instruction-following dataset. This dataset includes around 9,700 hours of long videos sourced from diverse domains, ranging from 3 to 60 minutes per video. Specifically, it contains 3.3M high-quality QA pairs, spanning six fundamental topics: temporality, spatiality, object, action, scene, and event. Compared to existing video instruction datasets, VideoMarathon significantly extends training video durations up to 1 hour, and supports 22 diverse tasks requiring both short- and long-term video comprehension. Building on VideoMarathon, we propose Hour-LLaVA, a powerful and efficient Video-LMM for hour-scale video-language modeling. It enables hour-long video training and inference at 1-FPS sampling by leveraging a memory augmentation module, which adaptively integrates user question-relevant and spatiotemporal-informative semantics from a cached full video context. In our experiments, Hour-LLaVA achieves the best performance on multiple long video-language benchmarks, demonstrating the high quality of the VideoMarathon dataset and the superiority of the Hour-LLaVA model. | [🔗 Paper](http://arxiv.org/abs/2506.05332v1) |
| [MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical
  Chain-of-Thought Reasoning](http://arxiv.org/abs/2506.05331v1) | Xinyan Chen, Renrui Zhang, Dongzhi Jiang, Aojun Zhou, Shilin Yan, Weifeng Lin, Hongsheng Li | 2025-06-05 | Multimodal AI, Prompt Engineering, LLM | Chain-of-Thought (CoT) has widely enhanced mathematical reasoning in Large Language Models (LLMs), but it still remains challenging for extending it to multimodal domains. Existing works either adopt a similar textual reasoning for image input, or seek to interleave visual signals into mathematical CoT. However, they face three key limitations for math problem-solving: reliance on coarse-grained box-shaped image regions, limited perception of vision encoders on math content, and dependence on external capabilities for visual modification. In this paper, we propose MINT-CoT, introducing Mathematical INterleaved Tokens for Chain-of-Thought visual reasoning. MINT-CoT adaptively interleaves relevant visual tokens into textual reasoning steps via an Interleave Token, which dynamically selects visual regions of any shapes within math figures. To empower this capability, we construct the MINT-CoT dataset, containing 54K mathematical problems aligning each reasoning step with visual regions at the token level, accompanied by a rigorous data generation pipeline. We further present a three-stage MINT-CoT training strategy, progressively combining text-only CoT SFT, interleaved CoT SFT, and interleaved CoT RL, which derives our MINT-CoT-7B model. Extensive experiments demonstrate the effectiveness of our method for effective visual interleaved reasoning in mathematical domains, where MINT-CoT-7B outperforms the baseline model by +34.08% on MathVista, +28.78% on GeoQA, and +23.2% on MMStar, respectively. Our code and data are available at https://github.com/xinyan-cxy/MINT-CoT | [🔗 Paper](http://arxiv.org/abs/2506.05331v1) |
| [AV-Reasoner: Improving and Benchmarking Clue-Grounded Audio-Visual
  Counting for MLLMs](http://arxiv.org/abs/2506.05328v1) | Lidong Lu, Guo Chen, Zhiqi Li, Yicheng Liu, Tong Lu | 2025-06-05 | Multimodal AI, RLHF, Model Evaluation, Training & Evaluation | Despite progress in video understanding, current MLLMs struggle with counting tasks. Existing benchmarks are limited by short videos, close-set queries, lack of clue annotations, and weak multimodal coverage. In this paper, we introduce CG-AV-Counting, a manually-annotated clue-grounded counting benchmark with 1,027 multimodal questions and 5,845 annotated clues over 497 long videos. It supports both black-box and white-box evaluation, serving as a comprehensive testbed for both end-to-end and reasoning-based counting. To explore ways to improve model's counting capability, we propose AV-Reasoner, a model trained with GRPO and curriculum learning to generalize counting ability from related tasks. AV-Reasoner achieves state-of-the-art results across multiple benchmarks, demonstrating the effectiveness of reinforcement learning. However, experiments show that on out-of-domain benchmarks, reasoning in the language space fails to bring performance gains. The code and benchmark have been realeased on https://av-reasoner.github.io. | [🔗 Paper](http://arxiv.org/abs/2506.05328v1) |
| [LSM-2: Learning from Incomplete Wearable Sensor Data](http://arxiv.org/abs/2506.05321v1) | Maxwell A. Xu, Girish Narayanswamy, Kumar Ayush, Dimitris Spathis, Shun Liao, Shyam A. Tailor, Ahmed Metwally, A. Ali Heydari, Yuwei Zhang, Jake Garrison, Samy Abdel-Ghaffar, Xuhai Xu, Ken Gu, Jacob Sunshine, Ming-Zher Poh, Yun Liu, Tim Althoff, Shrikanth Narayanan, Pushmeet Kohli, Mark Malhotra, Shwetak Patel, Yuzhe Yang, James M. Rehg, Xin Liu, Daniel McDuff | 2025-06-05 | Multimodal AI, Scaling Laws, Fine-Tuning | Foundation models, a cornerstone of recent advancements in machine learning, have predominantly thrived on complete and well-structured data. Wearable sensor data frequently suffers from significant missingness, posing a substantial challenge for self-supervised learning (SSL) models that typically assume complete data inputs. This paper introduces the second generation of Large Sensor Model (LSM-2) with Adaptive and Inherited Masking (AIM), a novel SSL approach that learns robust representations directly from incomplete data without requiring explicit imputation. AIM's core novelty lies in its use of learnable mask tokens to model both existing ("inherited") and artificially introduced missingness, enabling it to robustly handle fragmented real-world data during inference. Pre-trained on an extensive dataset of 40M hours of day-long multimodal sensor data, our LSM-2 with AIM achieves the best performance across a diverse range of tasks, including classification, regression and generative modeling. Furthermore, LSM-2 with AIM exhibits superior scaling performance, and critically, maintains high performance even under targeted missingness scenarios, reflecting clinically coherent patterns, such as the diagnostic value of nighttime biosignals for hypertension prediction. This makes AIM a more reliable choice for real-world wearable data applications. | [🔗 Paper](http://arxiv.org/abs/2506.05321v1) |
| [Does Your 3D Encoder Really Work? When Pretrain-SFT from 2D VLMs Meets
  3D VLMs](http://arxiv.org/abs/2506.05318v2) | Haoyuan Li, Yanpeng Zhou, Yufei Gao, Tao Tang, Jianhua Han, Yujie Yuan, Dave Zhenyu Chen, Jiawang Bian, Hang Xu, Xiaodan Liang | 2025-06-05 | Multimodal AI, Scaling Laws, LLM, RLHF, Training & Evaluation | Remarkable progress in 2D Vision-Language Models (VLMs) has spurred interest in extending them to 3D settings for tasks like 3D Question Answering, Dense Captioning, and Visual Grounding. Unlike 2D VLMs that typically process images through an image encoder, 3D scenes, with their intricate spatial structures, allow for diverse model architectures. Based on their encoder design, this paper categorizes recent 3D VLMs into 3D object-centric, 2D image-based, and 3D scene-centric approaches. Despite the architectural similarity of 3D scene-centric VLMs to their 2D counterparts, they have exhibited comparatively lower performance compared with the latest 3D object-centric and 2D image-based approaches. To understand this gap, we conduct an in-depth analysis, revealing that 3D scene-centric VLMs show limited reliance on the 3D scene encoder, and the pre-train stage appears less effective than in 2D VLMs. Furthermore, we observe that data scaling benefits are less pronounced on larger datasets. Our investigation suggests that while these models possess cross-modal alignment capabilities, they tend to over-rely on linguistic cues and overfit to frequent answer distributions, thereby diminishing the effective utilization of the 3D encoder. To address these limitations and encourage genuine 3D scene understanding, we introduce a novel 3D Relevance Discrimination QA dataset designed to disrupt shortcut learning and improve 3D understanding. Our findings highlight the need for advanced evaluation and improved strategies for better 3D understanding in 3D VLMs. | [🔗 Paper](http://arxiv.org/abs/2506.05318v2) |
| [MARBLE: Material Recomposition and Blending in CLIP-Space](http://arxiv.org/abs/2506.05313v1) | Ta-Ying Cheng, Prafull Sharma, Mark Boss, Varun Jampani | 2025-06-05 | Multimodal AI, Diffusion Models | Editing materials of objects in images based on exemplar images is an active area of research in computer vision and graphics. We propose MARBLE, a method for performing material blending and recomposing fine-grained material properties by finding material embeddings in CLIP-space and using that to control pre-trained text-to-image models. We improve exemplar-based material editing by finding a block in the denoising UNet responsible for material attribution. Given two material exemplar-images, we find directions in the CLIP-space for blending the materials. Further, we can achieve parametric control over fine-grained material attributes such as roughness, metallic, transparency, and glow using a shallow network to predict the direction for the desired material attribute change. We perform qualitative and quantitative analysis to demonstrate the efficacy of our proposed method. We also present the ability of our method to perform multiple edits in a single forward pass and applicability to painting.   Project Page: https://marblecontrol.github.io/ | [🔗 Paper](http://arxiv.org/abs/2506.05313v1) |
| [Perceive Anything: Recognize, Explain, Caption, and Segment Anything in
  Images and Videos](http://arxiv.org/abs/2506.05302v1) | Weifeng Lin, Xinyu Wei, Ruichuan An, Tianhe Ren, Tingwei Chen, Renrui Zhang, Ziyu Guo, Wentao Zhang, Lei Zhang, Hongsheng Li | 2025-06-05 | Multimodal AI | We present Perceive Anything Model (PAM), a conceptually straightforward and efficient framework for comprehensive region-level visual understanding in images and videos. Our approach extends the powerful segmentation model SAM 2 by integrating Large Language Models (LLMs), enabling simultaneous object segmentation with the generation of diverse, region-specific semantic outputs, including categories, label definition, functional explanations, and detailed captions. A key component, Semantic Perceiver, is introduced to efficiently transform SAM 2's rich visual features, which inherently carry general vision, localization, and semantic priors into multi-modal tokens for LLM comprehension. To support robust multi-granularity understanding, we also develop a dedicated data refinement and augmentation pipeline, yielding a high-quality dataset of 1.5M image and 0.6M video region-semantic annotations, including novel region-level streaming video caption data. PAM is designed for lightweightness and efficiency, while also demonstrates strong performance across a diverse range of region understanding tasks. It runs 1.2-2.4x faster and consumes less GPU memory than prior approaches, offering a practical solution for real-world applications. We believe that our effective approach will serve as a strong baseline for future research in region-level visual understanding. | [🔗 Paper](http://arxiv.org/abs/2506.05302v1) |
| [SeedVR2: One-Step Video Restoration via Diffusion Adversarial
  Post-Training](http://arxiv.org/abs/2506.05301v1) | Jianyi Wang, Shanchuan Lin, Zhijie Lin, Yuxi Ren, Meng Wei, Zongsheng Yue, Shangchen Zhou, Hao Chen, Yang Zhao, Ceyuan Yang, Xuefeng Xiao, Chen Change Loy, Lu Jiang | 2025-06-05 | Multimodal AI, Optimization, Diffusion Models | Recent advances in diffusion-based video restoration (VR) demonstrate significant improvement in visual quality, yet yield a prohibitive computational cost during inference. While several distillation-based approaches have exhibited the potential of one-step image restoration, extending existing approaches to VR remains challenging and underexplored, particularly when dealing with high-resolution video in real-world settings. In this work, we propose a one-step diffusion-based VR model, termed as SeedVR2, which performs adversarial VR training against real data. To handle the challenging high-resolution VR within a single step, we introduce several enhancements to both model architecture and training procedures. Specifically, an adaptive window attention mechanism is proposed, where the window size is dynamically adjusted to fit the output resolutions, avoiding window inconsistency observed under high-resolution VR using window attention with a predefined window size. To stabilize and improve the adversarial post-training towards VR, we further verify the effectiveness of a series of losses, including a proposed feature matching loss without significantly sacrificing training efficiency. Extensive experiments show that SeedVR2 can achieve comparable or even better performance compared with existing VR approaches in a single step. | [🔗 Paper](http://arxiv.org/abs/2506.05301v1) |
## 🔹 Optimization

| 📄 Title | 🖊 Authors | 📅 Date | 🏷 Tags | 📜 Summary | 🔗 Link |
|---------|---------|---------|---------|---------|---------|
| [ProJo4D: Progressive Joint Optimization for Sparse-View Inverse Physics
  Estimation](http://arxiv.org/abs/2506.05317v1) | Daniel Rho, Jun Myeong Choi, Biswadip Dey, Roni Sengupta | 2025-06-05 | Optimization | Neural rendering has made significant strides in 3D reconstruction and novel view synthesis. With the integration with physics, it opens up new applications. The inverse problem of estimating physics from visual data, however, still remains challenging, limiting its effectiveness for applications like physically accurate digital twin creation in robotics and XR. Existing methods that incorporate physics into neural rendering frameworks typically require dense multi-view videos as input, making them impractical for scalable, real-world use. When presented with sparse multi-view videos, the sequential optimization strategy used by existing approaches introduces significant error accumulation, e.g., poor initial 3D reconstruction leads to bad material parameter estimation in subsequent stages. Instead of sequential optimization, directly optimizing all parameters at the same time also fails due to the highly non-convex and often non-differentiable nature of the problem. We propose ProJo4D, a progressive joint optimization framework that gradually increases the set of jointly optimized parameters guided by their sensitivity, leading to fully joint optimization over geometry, appearance, physical state, and material property. Evaluations on PAC-NeRF and Spring-Gaus datasets show that ProJo4D outperforms prior work in 4D future state prediction, novel view rendering of future state, and material parameter estimation, demonstrating its effectiveness in physically grounded 4D scene understanding. For demos, please visit the project webpage: https://daniel03c1.github.io/ProJo4D/ | [🔗 Paper](http://arxiv.org/abs/2506.05317v1) |
| [Constrained Entropic Unlearning: A Primal-Dual Framework for Large
  Language Models](http://arxiv.org/abs/2506.05314v1) | Taha Entesari, Arman Hatami, Rinat Khaziev, Anil Ramakrishna, Mahyar Fazlyab | 2025-06-05 | Optimization | Large Language Models (LLMs) deployed in real-world settings increasingly face the need to unlearn sensitive, outdated, or proprietary information. Existing unlearning methods typically formulate forgetting and retention as a regularized trade-off, combining both objectives into a single scalarized loss. This often leads to unstable optimization and degraded performance on retained data, especially under aggressive forgetting. We propose a new formulation of LLM unlearning as a constrained optimization problem: forgetting is enforced via a novel logit-margin flattening loss that explicitly drives the output distribution toward uniformity on a designated forget set, while retention is preserved through a hard constraint on a separate retain set. Compared to entropy-based objectives, our loss is softmax-free, numerically stable, and maintains non-vanishing gradients, enabling more efficient and robust optimization. We solve the constrained problem using a scalable primal-dual algorithm that exposes the trade-off between forgetting and retention through the dynamics of the dual variable. Evaluations on the TOFU and MUSE benchmarks across diverse LLM architectures demonstrate that our approach consistently matches or exceeds state-of-the-art baselines, effectively removing targeted information while preserving downstream utility. | [🔗 Paper](http://arxiv.org/abs/2506.05314v1) |
| [Do It Yourself: Learning Semantic Correspondence from Pseudo-Labels](http://arxiv.org/abs/2506.05312v1) | Olaf Dünkel, Thomas Wimmer, Christian Theobalt, Christian Rupprecht, Adam Kortylewski | 2025-06-05 | Optimization | Finding correspondences between semantically similar points across images and object instances is one of the everlasting challenges in computer vision. While large pre-trained vision models have recently been demonstrated as effective priors for semantic matching, they still suffer from ambiguities for symmetric objects or repeated object parts. We propose to improve semantic correspondence estimation via 3D-aware pseudo-labeling. Specifically, we train an adapter to refine off-the-shelf features using pseudo-labels obtained via 3D-aware chaining, filtering wrong labels through relaxed cyclic consistency, and 3D spherical prototype mapping constraints. While reducing the need for dataset specific annotations compared to prior work, we set a new state-of-the-art on SPair-71k by over 4% absolute gain and by over 7% against methods with similar supervision requirements. The generality of our proposed approach simplifies extension of training to other data sources, which we demonstrate in our experiments. | [🔗 Paper](http://arxiv.org/abs/2506.05312v1) |
## 🔹 Scaling Laws

| 📄 Title | 🖊 Authors | 📅 Date | 🏷 Tags | 📜 Summary | 🔗 Link |
|---------|---------|---------|---------|---------|---------|
| [Kinetics: Rethinking Test-Time Scaling Laws](http://arxiv.org/abs/2506.05333v2) | Ranajoy Sadhukhan, Zhuoming Chen, Haizhong Zheng, Yang Zhou, Emma Strubell, Beidi Chen | 2025-06-05 | Scaling Laws | We rethink test-time scaling laws from a practical efficiency perspective, revealing that the effectiveness of smaller models is significantly overestimated. Prior work, grounded in compute-optimality, overlooks critical memory access bottlenecks introduced by inference-time strategies (e.g., Best-of-$N$, long CoTs). Our holistic analysis, spanning models from 0.6B to 32B parameters, reveals a new Kinetics Scaling Law that better guides resource allocation by incorporating both computation and memory access costs. Kinetics Scaling Law suggests that test-time compute is more effective when used on models above a threshold than smaller ones. A key reason is that in TTS, attention, rather than parameter count, emerges as the dominant cost factor. Motivated by this, we propose a new scaling paradigm centered on sparse attention, which lowers per-token cost and enables longer generations and more parallel samples within the same resource budget. Empirically, we show that sparse attention models consistently outperform dense counterparts, achieving over 60 points gains in low-cost regimes and over 5 points gains in high-cost regimes for problem-solving accuracy on AIME, encompassing evaluations on state-of-the-art MoEs. These results suggest that sparse attention is essential and increasingly important with more computing invested, for realizing the full potential of test-time scaling where, unlike training, accuracy has yet to saturate as a function of computation, and continues to improve through increased generation. The code is available at https://github.com/Infini-AI-Lab/Kinetics. | [🔗 Paper](http://arxiv.org/abs/2506.05333v2) |
## 🔹 Model Evaluation

| 📄 Title | 🖊 Authors | 📅 Date | 🏷 Tags | 📜 Summary | 🔗 Link |
|---------|---------|---------|---------|---------|---------|
| [Estimation of Treatment Effects Under Nonstationarity via Truncated
  Difference-in-Q's](http://arxiv.org/abs/2506.05308v1) | Ramesh Johari, Tianyi Peng, Wenqian Xing | 2025-06-05 | Model Evaluation, Training & Evaluation, Responsible AI | Randomized controlled experiments (''A/B testing'') are fundamental for assessing interventions in dynamic technology-driven environments, such as recommendation systems, online marketplaces, and digital health interventions. In these systems, interventions typically impact not only the current state of the system, but also future states; therefore, accurate estimation of the global average treatment effect (or GATE) from experiments requires accounting for the dynamic temporal behavior of the system. To address this, recent literature has analyzed a range of estimators applied to Bernoulli randomized experiments in stationary environments, ranging from the standard difference-in-means (DM) estimator to methods building on reinforcement learning techniques, such as off-policy evaluation and the recently proposed difference-in-Q's (DQ) estimator. However, all these estimators exhibit high bias and variance when the environment is nonstationary. This paper addresses the challenge of estimation under nonstationarity. We show that a simple extension of the DM estimator using differences in truncated outcome trajectories yields favorable bias and variance in nonstationary Markovian settings. Our theoretical analysis establishes this result by first showing that the truncated estimator is in fact estimating an appropriate policy gradient that can be expressed as a difference in Q-values; thus we refer to our estimator as the truncated DQ estimator (by analogy to the DQ estimator). We then show that the corresponding policy gradient is a first-order approximation to the GATE. Combining these insights yields our bias and variance bounds. We validate our results through synthetic and realistic simulations-including hospital and ride-sharing settings-and show that a well-calibrated truncated DQ estimator achieves low bias and variance even in nonstationary environments. | [🔗 Paper](http://arxiv.org/abs/2506.05308v1) |
## 🔹 Responsible AI

| 📄 Title | 🖊 Authors | 📅 Date | 🏷 Tags | 📜 Summary | 🔗 Link |
|---------|---------|---------|---------|---------|---------|
| [Defurnishing with X-Ray Vision: Joint Removal of Furniture from
  Panoramas and Mesh](http://arxiv.org/abs/2506.05338v2) | Alan Dolhasz, Chen Ma, Dave Gausebeck, Kevin Chen, Gregor Miller, Lucas Hayne, Gunnar Hovden, Azwad Sabik, Olaf Brandt, Mira Slavcheva | 2025-06-05 | Responsible AI | We present a pipeline for generating defurnished replicas of indoor spaces represented as textured meshes and corresponding multi-view panoramic images. To achieve this, we first segment and remove furniture from the mesh representation, extend planes, and fill holes, obtaining a simplified defurnished mesh (SDM). This SDM acts as an ``X-ray'' of the scene's underlying structure, guiding the defurnishing process. We extract Canny edges from depth and normal images rendered from the SDM. We then use these as a guide to remove the furniture from panorama images via ControlNet inpainting. This control signal ensures the availability of global geometric information that may be hidden from a particular panoramic view by the furniture being removed. The inpainted panoramas are used to texture the mesh. We show that our approach produces higher quality assets than methods that rely on neural radiance fields, which tend to produce blurry low-resolution images, or RGB-D inpainting, which is highly susceptible to hallucinations. | [🔗 Paper](http://arxiv.org/abs/2506.05338v2) |
## 🔹 Autonomous Agents

| 📄 Title | 🖊 Authors | 📅 Date | 🏷 Tags | 📜 Summary | 🔗 Link |
|---------|---------|---------|---------|---------|---------|
| [ProRefine: Inference-time Prompt Refinement with Textual Feedback](http://arxiv.org/abs/2506.05305v1) | Deepak Pandita, Tharindu Cyril Weerasooriya, Ankit Parag Shah, Christopher M. Homan, Wei Wei | 2025-06-05 | Autonomous Agents, Prompt Engineering, Optimization | Agentic workflows, where multiple AI agents collaborate to accomplish complex tasks like reasoning or planning, are becoming increasingly prevalent. However, these workflows often suffer from error propagation and sub-optimal performance, largely due to poorly designed prompts that fail to effectively guide individual agents. This is a critical problem because it limits the reliability and scalability of these powerful systems. We introduce ProRefine, an innovative inference-time prompt optimization method that leverages textual feedback from large language models (LLMs) to address this challenge. ProRefine dynamically refines prompts for multi-step reasoning tasks without additional training or ground truth labels. Evaluated on five benchmark mathematical reasoning datasets, ProRefine significantly surpasses zero-shot Chain-of-Thought baselines by 3 to 37 percentage points. This approach not only boosts accuracy but also allows smaller models to match the performance of larger ones, highlighting its potential for efficient and scalable AI deployment, and democratizing access to high-performing AI. | [🔗 Paper](http://arxiv.org/abs/2506.05305v1) |
## 🔹 General AI

| 📄 Title | 🖊 Authors | 📅 Date | 🏷 Tags | 📜 Summary | 🔗 Link |
|---------|---------|---------|---------|---------|---------|
| [FreeTimeGS: Free Gaussian Primitives at Anytime and Anywhere for Dynamic
  Scene Reconstruction](http://arxiv.org/abs/2506.05348v2) | Yifan Wang, Peishan Yang, Zhen Xu, Jiaming Sun, Zhanhua Zhang, Yong Chen, Hujun Bao, Sida Peng, Xiaowei Zhou | 2025-06-05 | General AI | This paper addresses the challenge of reconstructing dynamic 3D scenes with complex motions. Some recent works define 3D Gaussian primitives in the canonical space and use deformation fields to map canonical primitives to observation spaces, achieving real-time dynamic view synthesis. However, these methods often struggle to handle scenes with complex motions due to the difficulty of optimizing deformation fields. To overcome this problem, we propose FreeTimeGS, a novel 4D representation that allows Gaussian primitives to appear at arbitrary time and locations. In contrast to canonical Gaussian primitives, our representation possesses the strong flexibility, thus improving the ability to model dynamic 3D scenes. In addition, we endow each Gaussian primitive with an motion function, allowing it to move to neighboring regions over time, which reduces the temporal redundancy. Experiments results on several datasets show that the rendering quality of our method outperforms recent methods by a large margin. Project page: https://zju3dv.github.io/freetimegs/ . | [🔗 Paper](http://arxiv.org/abs/2506.05348v2) |
| [Neural Inverse Rendering from Propagating Light](http://arxiv.org/abs/2506.05347v1) | Anagh Malik, Benjamin Attal, Andrew Xie, Matthew O'Toole, David B. Lindell | 2025-06-05 | General AI | We present the first system for physically based, neural inverse rendering from multi-viewpoint videos of propagating light. Our approach relies on a time-resolved extension of neural radiance caching -- a technique that accelerates inverse rendering by storing infinite-bounce radiance arriving at any point from any direction. The resulting model accurately accounts for direct and indirect light transport effects and, when applied to captured measurements from a flash lidar system, enables state-of-the-art 3D reconstruction in the presence of strong indirect light. Further, we demonstrate view synthesis of propagating light, automatic decomposition of captured measurements into direct and indirect components, as well as novel capabilities such as multi-view time-resolved relighting of captured scenes. | [🔗 Paper](http://arxiv.org/abs/2506.05347v1) |
| [Momentum fraction and hard scale dependence of double parton scattering](http://arxiv.org/abs/2506.05337v1) | Joao Vitor C. Lovato, Edgar Huayra, Emmanuel G. de Oliveira | 2025-06-05 | General AI | The effective cross section of double parton scattering in high-energy hadron collisions has been measured in proton--proton collisions, with significant variation among final-state observables, contrary to the idea of a universal value. Building upon our previous work, we incorporate the dependence on both the parton longitudinal momentum fraction $x$ and the process energy hard scale $\mu$ into the transverse part of the double parton distributions, using a Gaussian profile. Employing the experimental data from the LHC and Tevatron experiments (covering different processes, kinematic configurations, and center--of--mass energies), we perform a global fit of the model, extracting the parameters that describe the proton structure. With this result, it becomes possible to calculate the effective cross section for others observables, and we provide predictions for future measurements at the LHC. | [🔗 Paper](http://arxiv.org/abs/2506.05337v1) |
| [Search Arena: Analyzing Search-Augmented LLMs](http://arxiv.org/abs/2506.05334v1) | Mihran Miroyan, Tsung-Han Wu, Logan King, Tianle Li, Jiayi Pan, Xinyan Hu, Wei-Lin Chiang, Anastasios N. Angelopoulos, Trevor Darrell, Narges Norouzi, Joseph E. Gonzalez | 2025-06-05 | General AI | Search-augmented language models combine web search with Large Language Models (LLMs) to improve response groundedness and freshness. However, analyzing these systems remains challenging: existing datasets are limited in scale and narrow in scope, often constrained to static, single-turn, fact-checking questions. In this work, we introduce Search Arena, a crowd-sourced, large-scale, human-preference dataset of over 24,000 paired multi-turn user interactions with search-augmented LLMs. The dataset spans diverse intents and languages, and contains full system traces with around 12,000 human preference votes. Our analysis reveals that user preferences are influenced by the number of citations, even when the cited content does not directly support the attributed claims, uncovering a gap between perceived and actual credibility. Furthermore, user preferences vary across cited sources, revealing that community-driven platforms are generally preferred and static encyclopedic sources are not always appropriate and reliable. To assess performance across different settings, we conduct cross-arena analyses by testing search-augmented LLMs in a general-purpose chat environment and conventional LLMs in search-intensive settings. We find that web search does not degrade and may even improve performance in non-search settings; however, the quality in search settings is significantly affected if solely relying on the model's parametric knowledge. We open-sourced the dataset to support future research in this direction. Our dataset and code are available at: https://github.com/lmarena/search-arena. | [🔗 Paper](http://arxiv.org/abs/2506.05334v1) |
| [Upper bound for the Holevo quantity arising from the fundamental
  entropic inequality](http://arxiv.org/abs/2506.05335v1) | M. E. Shirokov | 2025-06-05 | General AI | We show how the fundamental entropic inequality proved recently in [arXiv:2408.15306] can be used to obtain a quite accurate upper bound on the Holevo quantity of a discrete ensemble of quantum states expressed via the probabilities and the metric characteristics of this ensembles. | [🔗 Paper](http://arxiv.org/abs/2506.05335v1) |
| [Spinless and spinful charge excitations in moiré Fractional Chern
  Insulators](http://arxiv.org/abs/2506.05330v1) | Miguel Gonçalves, Juan Felipe Mendez-Valderrama, Jonah Herzog-Arbeitman, Jiabin Yu, Xiaodong Xu, Di Xiao, B. Andrei Bernevig, Nicolas Regnault | 2025-06-05 | General AI | Fractionally charged elementary excitations, the quasi-electron and quasi-hole, are one of the hallmarks of the fractional Chern insulator (FCI). In this work, we observe that spontaneous spin polarization in twisted MoTe$_2$ leads to multiple species of low-energy quasi-particles distinguished by their spin quantum numbers. We perform large-scale exact diagonalization (ED) calculations to investigate the nature of these excitations and develop a method to extract their fundamental energetic properties. Focusing on $\theta = 3.7^{\circ}$ and filling factor $\nu = -2/3$ relevant to recent experiments, we show that spin-preserving (spinless) charge excitations have smaller gap than spin-flipping (spinful) excitations both with and without band mixing. This result is in qualitative agreement with the measured magnetic field dependence of the transport gaps. Beyond the spinless and spinful quasi-particle gaps, we extract the full quasi-electron and quasi-hole ``band structure'' and find significant dispersion with emergent magnetic translation symmetry -- a fundamental departure from the immobile excitations of the quantum Hall fluid. Our results establish a framework for computing the properties of novel elementary excitations in FCIs. | [🔗 Paper](http://arxiv.org/abs/2506.05330v1) |
| [Admissibility of Completely Randomized Trials: A Large-Deviation
  Approach](http://arxiv.org/abs/2506.05329v1) | Guido Imbens, Chao Qin, Stefan Wager | 2025-06-05 | General AI | When an experimenter has the option of running an adaptive trial, is it admissible to ignore this option and run a non-adaptive trial instead? We provide a negative answer to this question in the best-arm identification problem, where the experimenter aims to allocate measurement efforts judiciously to confidently deploy the most effective treatment arm. We find that, whenever there are at least three treatment arms, there exist simple adaptive designs that universally and strictly dominate non-adaptive completely randomized trials. This dominance is characterized by a notion called efficiency exponent, which quantifies a design's statistical efficiency when the experimental sample is large. Our analysis focuses on the class of batched arm elimination designs, which progressively eliminate underperforming arms at pre-specified batch intervals. We characterize simple sufficient conditions under which these designs universally and strictly dominate completely randomized trials. These results resolve the second open problem posed in Qin [2022]. | [🔗 Paper](http://arxiv.org/abs/2506.05329v1) |
| [Searching for Hidden Sector Particles at Neutrino Telescopes](http://arxiv.org/abs/2506.05326v1) | Sagar Airen, Zackaria Chacko, Can Kilic, Ram Purandhar Reddy Sudha | 2025-06-05 | General AI | We explore the possibility of directly detecting light, long-lived hidden sector particles at the IceCube neutrino telescope. Such particles frequently arise in non-minimal hidden sectors that couple to the Standard Model through portal operators. We consider two distinct scenarios. In the first scenario, which arises from a neutrino portal interaction, a hidden sector particle is produced inside the detector by the collision of an energetic neutrino with a nucleon, giving rise to a visible cascade. This new state then decays into a hidden sector daughter, which can naturally be long-lived. The eventual decay of the daughter particle back to Standard Model states gives rise to a second cascade inside the detector. This scenario therefore gives rise to a characteristic "double bang" signal arising from the two distinct cascades. In the second scenario, which arises from a hypercharge portal interaction, a hidden sector particle is produced outside the detector by the collision of an atmospheric muon with a nucleon. This new state promptly decays into a pair of hidden sector daughters that are long-lived. If both daughters decay into Standard Model states inside the detector, we again obtain a double-bang signal from the two distinct cascades. We explore the reach of IceCube for these two scenarios and show that it has the potential to significantly improve the sensitivity to hidden sector models in the mass range from about a GeV to about 20 GeV. | [🔗 Paper](http://arxiv.org/abs/2506.05326v1) |
| [A 2D-CFT Factory: Critical Lattice Models from Competing Anyon
  Condensation Processes in SymTO/SymTFT](http://arxiv.org/abs/2506.05324v1) | Ling-Yan Hung, Kaixin Ji, Ce Shen, Yidun Wan, Yu Zhao | 2025-06-05 | General AI | In this paper, we introduce a ``CFT factory'' : a novel algorithm of methodically generating 2D lattice models that would flow to 2D conformal fixed points in the infrared. These 2D models are realised by giving critical boundary conditions to 3D topological orders (symTOs/symTFTs) described by string-net models, often called the strange correlators. We engineer these critical boundary conditions by introducing a commensurate amount of non-commuting anyon condensates. The non-invertible symmetries preserved at the critical point can be controlled by studying a novel ``refined condensation tree''. Our structured method generates an infinite family of critical lattice models, including the A-series minimal models, and uncovers previously unknown critical points. Notably, we find at least three novel critical points (c$\approx 1.3$, $1.8$, and $2.5$ respectively) preserving the Haagerup symmetries, in addition to recovering previously reported ones. The condensation tree, together with a generalised Kramers-Wannier duality, predicts precisely large swathes of phase boundaries, fixes almost completely the global phase diagram, and sieves out second order phase transitions. This is not only illustrated in well-known examples (such as the 8-vertex model related to the $A_5$ category) but also further verified with precision numerics, using our improved (non-invertible) symmetry-preserving tensor-network RG, in novel examples involving the Haagerup symmetries. We show that critical couplings can be precisely encoded in the categorical data (Frobenius algebras and quantum dimensions in unitary fusion categories), thus establishing a powerful, systematic route to discovering and potentially classifying new conformal field theories. | [🔗 Paper](http://arxiv.org/abs/2506.05324v1) |
| [Non-Perturbative Topological Gadgets for Many-Body Coupling](http://arxiv.org/abs/2506.05323v1) | David Headley, Nicholas Chancellor | 2025-06-05 | General AI | Continuous-time quantum hardware implementations generally lack the native capability to implement high-order terms that would facilitate efficient compilation of quantum algorithms. This limitation has, in part, motivated the development of perturbative gadgets -- multi-qubit constructions used to effect a desired Hamiltonian using engineered low-energy subspaces of a larger system constructed using simpler, usually two-body, primitives. In this work, we demonstrate how a class of non-perturbative gadgets can produce high-order multi-body interactions by taking advantage of the odd-even properties of topological defect subspaces. The simplest example is based on domain-wall defects forming an effective Ising spin-chain based on three-body coupling with linear connectivity, alongside three-, or five-body driving terms depending on the intended use. Although this will be the main focus of the paper due to conceptual simplicity, there exist systems constructed with only two-body couplings where the boundaries determine whether there are an odd or even number of defects, namely ice-like systems which may yield more complex gadget-like constructions. | [🔗 Paper](http://arxiv.org/abs/2506.05323v1) |
| [Equilibrium Computation in First-Price Auctions with Correlated Priors](http://arxiv.org/abs/2506.05322v1) | Aris Filos-Ratsikas, Yiannis Giannakopoulos, Alexandros Hollender, Charalampos Kokkalis | 2025-06-05 | General AI | We consider the computational complexity of computing Bayes-Nash equilibria in first-price auctions, where the bidders' values for the item are drawn from a general (possibly correlated) joint distribution. We show that when the values and the bidding space are discrete, determining the existence of a pure Bayes-Nash equilibrium is NP-hard. This is the first hardness result in the literature of the problem that does not rely on assumptions of subjectivity of the priors, or convoluted tie-breaking rules. We then present two main approaches for achieving positive results, via bid sparsification and via bid densification. The former is more combinatorial and is based on enumeration techniques, whereas the latter makes use of the continuous theory of the problem developed in the economics literature. Using these approaches, we develop polynomial-time approximation algorithms for computing equilibria in symmetric settings or settings with a fixed number of bidders, for different (discrete or continuous) variants of the auction. | [🔗 Paper](http://arxiv.org/abs/2506.05322v1) |
| [Landau-Ginzburg Paradigm of Topological Phases](http://arxiv.org/abs/2506.05319v1) | Yu Zhao, Yidun Wan | 2025-06-05 | General AI | Topologically ordered matter phases have been regarded as beyond the Landau-Ginzburg symmetry breaking paradigm of matter phases. Recent studies of anyon condensation in topological phases, however, may fit topological phases back in the Landau-Ginzburg paradigm. To truly do so, we realized that the string-net model of topological phases is in fact an effective lattice gauge theory coupled with anyonic matter once two modifications are made: (1) We reinterpret anyons as matter fields coupled to lattice gauge fields, thus extending the HGW model to a genuine Hamiltonian lattice gauge theory. (2) By explicitly incorporating the internal degrees of freedom of anyons, we construct an enlarged Hilbert space that supports well-defined gauge transformations and covariant coupling, restoring the analogy with conventional lattice gauge field theory. In this modified string-net model, topological phase transitions induced by anyon condensation and their consequent phenomena, such as order parameter fields, coherent states, Goldstone modes, and gapping gauge degrees of freedom, can be formulated exactly as Landau's effective theory of the Higgs mechanism. To facilitate the understanding, we also compare anyon condensation to/with the Higgs boson condensation in the electroweak theory and the Cooper pair condensation. | [🔗 Paper](http://arxiv.org/abs/2506.05319v1) |
| [Time to Talk: LLM Agents for Asynchronous Group Communication in Mafia
  Games](http://arxiv.org/abs/2506.05309v1) | Niv Eckhaus, Uri Berger, Gabriel Stanovsky | 2025-06-05 | General AI | LLMs are used predominantly in synchronous communication, where a human user and a model communicate in alternating turns. In contrast, many real-world settings are inherently asynchronous. For example, in group chats, online team meetings, or social games, there is no inherent notion of turns; therefore, the decision of when to speak forms a crucial part of the participant's decision making. In this work, we develop an adaptive asynchronous LLM-agent which, in addition to determining what to say, also decides when to say it. To evaluate our agent, we collect a unique dataset of online Mafia games, including both human participants, as well as our asynchronous agent. Overall, our agent performs on par with human players, both in game performance, as well as in its ability to blend in with the other human players. Our analysis shows that the agent's behavior in deciding when to speak closely mirrors human patterns, although differences emerge in message content. We release all our data and code to support and encourage further research for more realistic asynchronous communication between LLM agents. This work paves the way for integration of LLMs into realistic human group settings, from assistance in team discussions to educational and professional environments where complex social dynamics must be navigated. | [🔗 Paper](http://arxiv.org/abs/2506.05309v1) |
| [Erasure cost of a quantum process: A thermodynamic meaning of the
  dynamical min-entropy](http://arxiv.org/abs/2506.05307v1) | Himanshu Badhani, Dhanuja G S, Swati Choudhary, Vishal Anand, Siddhartha Das | 2025-06-05 | General AI | The erasure of information is fundamentally an irreversible logical operation, carrying profound consequences for the energetics of computation and information processing. In this work, we investigate the thermodynamic costs associated with erasing (and preparing) quantum processes. Specifically, we analyze an arbitrary bipartite unitary gate acting on logical and ancillary input-output systems, where the ancillary input is always initialized in the ground state. We focus on the adversarial erasure cost of the reduced dynamics~\textemdash~that is, the minimal thermodynamic work required to erase the logical output of the gate for any logical input, assuming full access to the ancilla but no access to any purifying reference of the logical input state. We determine that this adversarial erasure cost is directly proportional to the negative min-entropy of the reduced dynamics, thereby giving the dynamical min-entropy a clear operational meaning. A key foundation of this result is the quantum process decoupling theorem, which quantitatively relates the decoupling ability of a process with its min-entropy. This insight bridges thermodynamics, information theory, and the fundamental limits of quantum computation. | [🔗 Paper](http://arxiv.org/abs/2506.05307v1) |
| [Full characterization of measurement-induced transitions of a
  superconducting qubit](http://arxiv.org/abs/2506.05306v1) | Thomas Connolly, Pavel D. Kurilovich, Vladislav D. Kurilovich, Charlotte G. L. Bøttcher, Sumeru Hazra, Wei Dai, Andy Z. Ding, Vidul R. Joshi, Heekun Nho, Spencer Diamond, Daniel K. Weiss, Valla Fatemi, Luigi Frunzio, Leonid I. Glazman, Michel H. Devoret | 2025-06-05 | General AI | Repeated quantum non-demolition measurement is a cornerstone of quantum error correction protocols. In superconducting qubits, the speed of dispersive state readout can be enhanced by increasing the power of the readout tone. However, such an increase has been found to result in additional qubit state transitions that violate the desired quantum non-demolition character of the measurement. Recently, the readout of a transmon superconducting qubit was improved by using a tone with frequency much larger than the qubit frequency. Here, we experimentally identify the mechanisms of readout-induced transitions in this regime. In the dominant mechanism, the energy of an incoming readout photon is partially absorbed by the transmon and partially returned to the transmission line as a photon with lower frequency. Other mechanisms involve the excitation of unwanted package modes, decay via material defects, and, at higher qubit frequencies, the activation of undesired resonances in the transmon spectrum. Our work provides a comprehensive characterization of superconducting qubit state transitions caused by a strong drive. | [🔗 Paper](http://arxiv.org/abs/2506.05306v1) |
| [Cryogenic Optical Lattice Clock with $1.7\times 10^{-20}$ Blackbody
  Radiation Stark Uncertainty](http://arxiv.org/abs/2506.05304v1) | Youssef S. Hassan, Kyle Beloy, Jacob L. Siegel, Takumi Kobayashi, Eric Swiler, Tanner Grogan, Roger C. Brown, Tristan Rojo, Tobias Bothwell, Benjamin D. Hunt, Adam Halaoui, Andrew D. Ludlow | 2025-06-05 | General AI | Controlling the Stark perturbation from ambient thermal radiation is key to advancing the performance of many atomic frequency standards, including state-of-the-art optical lattice clocks (OLCs). We demonstrate a cryogenic OLC that utilizes a dynamically actuated radiation shield to control the perturbation at $1.7\times10^{-20}$ fractional frequency, a factor of $\sim$40 beyond the best OLC to date. Our shield furnishes the atoms with a near-ideal cryogenic blackbody radiation (BBR) environment by rejecting external thermal radiation at the part-per-million level during clock spectroscopy, overcoming a key limitation with previous cryogenic BBR control solutions in OLCs. While the lowest BBR shift uncertainty is realized with cryogenic operation, we further exploit the radiation control that the shield offers over a wide range of temperatures to directly measure and verify the leading BBR Stark dynamic correction coefficient for ytterbium. This independent measurement reduces the literature-combined uncertainty of this coefficient by 30%, thus benefiting state-of-the-art Yb OLCs operated at room temperature. We verify the static BBR coefficient for Yb at the low $10^{-18}$ level. | [🔗 Paper](http://arxiv.org/abs/2506.05304v1) |
| [Transient dynamics of associative memory models](http://arxiv.org/abs/2506.05303v1) | David G. Clark | 2025-06-05 | General AI | Associative memory models such as the Hopfield network and its dense generalizations with higher-order interactions exhibit a "blackout catastrophe"--a discontinuous transition where stable memory states abruptly vanish when the number of stored patterns exceeds a critical capacity. This transition is often interpreted as rendering networks unusable beyond capacity limits. We argue that this interpretation is largely an artifact of the equilibrium perspective. We derive dynamical mean-field equations using a bipartite cavity approach for graded-activity dense associative memory models, with the Hopfield model as a special case, and solve them using a numerical scheme. We show that patterns can be transiently retrieved with high accuracy above capacity despite the absence of stable attractors. This occurs because slow regions persist in the above-capacity energy landscape as shallow, unstable remnants of below-capacity stable basins. The same transient-retrieval effect occurs in below-capacity networks initialized outside basins of attraction. "Transient-recovery curves" provide a concise visual summary of these effects, revealing graceful, non-catastrophic changes in retrieval behavior above capacity and allowing us to compare the behavior across interaction orders. This dynamical perspective reveals rich energy landscape structure obscured by equilibrium analysis and suggests biological neural circuits may exploit transient dynamics for memory retrieval. Furthermore, our approach suggests ways of understanding computational properties of neural circuits without reference to fixed points, advances the technical repertoire of numerical mean-field solution methods for recurrent neural networks, and yields new theoretical results on generalizations of the Hopfield model. | [🔗 Paper](http://arxiv.org/abs/2506.05303v1) |
