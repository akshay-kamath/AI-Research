# üìå AI Research Papers (February03 to February09)

## üîπ LLM

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Fillerbuster: Multi-View Scene Completion for Casual Captures](http://arxiv.org/abs/2502.05175v1) | Ethan Weber, Norman M√ºller, Yash Kant, Vasu Agrawal, Michael Zollh√∂fer, Angjoo Kanazawa, Christian Richardt | 2025-02-07 | LLM, Diffusion Models | We present Fillerbuster, a method that completes unknown regions of a 3D scene by utilizing a novel large-scale multi-view latent diffusion transformer. Casual captures are often sparse and miss surrounding content behind objects or above the scene. Existing methods are not suitable for handling this challenge as they focus on making the known pixels look good with sparse-view priors, or on creating the missing sides of objects from just one or two photos. In reality, we often have hundreds of input frames and want to complete areas that are missing and unobserved from the input frames. Additionally, the images often do not have known camera parameters. Our solution is to train a generative model that can consume a large context of input frames while generating unknown target views and recovering image poses when desired. We show results where we complete partial captures on two existing datasets. We also present an uncalibrated scene completion task where our unified model predicts both poses and creates new content. Our model is the first to predict many images and poses together for scene completion. | [üîó Paper](http://arxiv.org/abs/2502.05175v1) |
| [Joint MoE Scaling Laws: Mixture of Experts Can Be Memory Efficient](http://arxiv.org/abs/2502.05172v1) | Jan Ludziejewski, Maciej Pi√≥ro, Jakub Krajewski, Maciej Stefaniak, Micha≈Ç Krutul, Jan Ma≈Ça≈õnicki, Marek Cygan, Piotr Sankowski, Kamil Adamczewski, Piotr Mi≈Ço≈õ, Sebastian Jaszczur | 2025-02-07 | LLM, Scaling Laws | Mixture of Experts (MoE) architectures have significantly increased computational efficiency in both research and real-world applications of large-scale machine learning models. However, their scalability and efficiency under memory constraints remain relatively underexplored. In this work, we present joint scaling laws for dense and MoE models, incorporating key factors such as the number of active parameters, dataset size, and the number of experts. Our findings provide a principled framework for selecting the optimal MoE configuration under fixed memory and compute budgets. Surprisingly, we show that MoE models can be more memory-efficient than dense models, contradicting conventional wisdom. To derive and validate the theoretical predictions of our scaling laws, we conduct over 280 experiments with up to 2.7B active parameters and up to 5B total parameters. These results offer actionable insights for designing and deploying MoE models in practical large-scale training scenarios. | [üîó Paper](http://arxiv.org/abs/2502.05172v1) |
| [NoLiMa: Long-Context Evaluation Beyond Literal Matching](http://arxiv.org/abs/2502.05167v1) | Ali Modarressi, Hanieh Deilamsalehy, Franck Dernoncourt, Trung Bui, Ryan A. Rossi, Seunghyun Yoon, Hinrich Sch√ºtze | 2025-02-07 | LLM, Training & Evaluation | Recent large language models (LLMs) support long contexts ranging from 128K to 1M tokens. A popular method for evaluating these capabilities is the needle-in-a-haystack (NIAH) test, which involves retrieving a "needle" (relevant information) from a "haystack" (long irrelevant context). Extensions of this approach include increasing distractors, fact chaining, and in-context reasoning. However, in these benchmarks, models can exploit existing literal matches between the needle and haystack to simplify the task. To address this, we introduce NoLiMa, a benchmark extending NIAH with a carefully designed needle set, where questions and needles have minimal lexical overlap, requiring models to infer latent associations to locate the needle within the haystack. We evaluate 12 popular LLMs that claim to support contexts of at least 128K tokens. While they perform well in short contexts (<1K), performance degrades significantly as context length increases. At 32K, for instance, 10 models drop below 50% of their strong short-length baselines. Even GPT-4o, one of the top-performing exceptions, experiences a reduction from an almost-perfect baseline of 99.3% to 69.7%. Our analysis suggests these declines stem from the increased difficulty the attention mechanism faces in longer contexts when literal matches are absent, making it harder to retrieve relevant information. | [üîó Paper](http://arxiv.org/abs/2502.05167v1) |
| [In-context denoising with one-layer transformers: connections between
  attention and associative memory retrieval](http://arxiv.org/abs/2502.05164v1) | Matthew Smart, Alberto Bietti, Anirvan M. Sengupta | 2025-02-07 | LLM, Diffusion Models, Training & Evaluation | We introduce in-context denoising, a task that refines the connection between attention-based architectures and dense associative memory (DAM) networks, also known as modern Hopfield networks. Using a Bayesian framework, we show theoretically and empirically that certain restricted denoising problems can be solved optimally even by a single-layer transformer. We demonstrate that a trained attention layer processes each denoising prompt by performing a single gradient descent update on a context-aware DAM energy landscape, where context tokens serve as associative memories and the query token acts as an initial state. This one-step update yields better solutions than exact retrieval of either a context token or a spurious local minimum, providing a concrete example of DAM networks extending beyond the standard retrieval paradigm. Overall, this work solidifies the link between associative memory and attention mechanisms first identified by Ramsauer et al., and demonstrates the relevance of associative memory models in the study of in-context learning. | [üîó Paper](http://arxiv.org/abs/2502.05164v1) |
| [A Lightweight Method to Disrupt Memorized Sequences in LLM](http://arxiv.org/abs/2502.05159v1) | Parjanya Prajakta Prashant, Kaustubh Ponkshe, Babak Salimi | 2025-02-07 | LLM | Large language models (LLMs) demonstrate impressive capabilities across many tasks yet risk reproducing copyrighted content verbatim, raising legal and ethical concerns. Although methods like differential privacy or neuron editing can reduce memorization, they typically require costly retraining or direct access to model weights and may degrade performance. To address these challenges, we propose TokenSwap, a lightweight, post-hoc approach that replaces the probabilities of grammar-related tokens with those from a small auxiliary model (e.g., DistilGPT-2). We run extensive experiments on commercial grade models such as Pythia-6.9b and LLaMA-3-8b and demonstrate that our method effectively reduces well-known cases of memorized generation by upto 10x with little to no impact on downstream tasks. Our approach offers a uniquely accessible and effective solution to users of real-world systems. | [üîó Paper](http://arxiv.org/abs/2502.05159v1) |
## üîπ Diffusion Models

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [FlashVideo:Flowing Fidelity to Detail for Efficient High-Resolution
  Video Generation](http://arxiv.org/abs/2502.05179v1) | Shilong Zhang, Wenbo Li, Shoufa Chen, Chongjian Ge, Peize Sun, Yida Zhang, Yi Jiang, Zehuan Yuan, Binyue Peng, Ping Luo | 2025-02-07 | Diffusion Models, Multimodal AI | DiT diffusion models have achieved great success in text-to-video generation, leveraging their scalability in model capacity and data scale. High content and motion fidelity aligned with text prompts, however, often require large model parameters and a substantial number of function evaluations (NFEs). Realistic and visually appealing details are typically reflected in high resolution outputs, further amplifying computational demands especially for single stage DiT models. To address these challenges, we propose a novel two stage framework, FlashVideo, which strategically allocates model capacity and NFEs across stages to balance generation fidelity and quality. In the first stage, prompt fidelity is prioritized through a low resolution generation process utilizing large parameters and sufficient NFEs to enhance computational efficiency. The second stage establishes flow matching between low and high resolutions, effectively generating fine details with minimal NFEs. Quantitative and visual results demonstrate that FlashVideo achieves state-of-the-art high resolution video generation with superior computational efficiency. Additionally, the two-stage design enables users to preview the initial output before committing to full resolution generation, thereby significantly reducing computational costs and wait times as well as enhancing commercial viability . | [üîó Paper](http://arxiv.org/abs/2502.05179v1) |
| [Latent Swap Joint Diffusion for Long-Form Audio Generation](http://arxiv.org/abs/2502.05130v1) | Yusheng Dai, Chenxi Wang, Chang Li, Chen Wang, Jun Du, Kewei Li, Ruoyu Wang, Jiefeng Ma, Lei Sun, Jianqing Gao | 2025-02-07 | Diffusion Models, Multimodal AI | Previous work on long-form audio generation using global-view diffusion or iterative generation demands significant training or inference costs. While recent advancements in multi-view joint diffusion for panoramic generation provide an efficient option, they struggle with spectrum generation with severe overlap distortions and high cross-view consistency costs. We initially explore this phenomenon through the connectivity inheritance of latent maps and uncover that averaging operations excessively smooth the high-frequency components of the latent map. To address these issues, we propose Swap Forward (SaFa), a frame-level latent swap framework that synchronizes multiple diffusions to produce a globally coherent long audio with more spectrum details in a forward-only manner. At its core, the bidirectional Self-Loop Latent Swap is applied between adjacent views, leveraging stepwise diffusion trajectory to adaptively enhance high-frequency components without disrupting low-frequency components. Furthermore, to ensure cross-view consistency, the unidirectional Reference-Guided Latent Swap is applied between the reference and the non-overlap regions of each subview during the early stages, providing centralized trajectory guidance. Quantitative and qualitative experiments demonstrate that SaFa significantly outperforms existing joint diffusion methods and even training-based long audio generation models. Moreover, we find that it also adapts well to panoramic generation, achieving comparable state-of-the-art performance with greater efficiency and model generalizability. Project page is available at https://swapforward.github.io/. | [üîó Paper](http://arxiv.org/abs/2502.05130v1) |
## üîπ RLHF

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive
  Multimodal Understanding and Generation](http://arxiv.org/abs/2502.05178v1) | Yue Zhao, Fuzhao Xue, Scott Reed, Linxi Fan, Yuke Zhu, Jan Kautz, Zhiding Yu, Philipp Kr√§henb√ºhl, De-An Huang | 2025-02-07 | RLHF, Prompt Engineering, Multimodal AI, Optimization | We introduce Quantized Language-Image Pretraining (QLIP), a visual tokenization method that combines state-of-the-art reconstruction quality with state-of-the-art zero-shot image understanding. QLIP trains a binary-spherical-quantization-based autoencoder with reconstruction and language-image alignment objectives. We are the first to show that the two objectives do not need to be at odds. We balance the two loss terms dynamically during training and show that a two-stage training pipeline effectively mixes the large-batch requirements of image-language pre-training with the memory bottleneck imposed by the reconstruction objective. We validate the effectiveness of QLIP for multimodal understanding and text-conditioned image generation with a single model. Specifically, QLIP serves as a drop-in replacement for the visual encoder for LLaVA and the image tokenizer for LlamaGen with comparable or even better performance. Finally, we demonstrate that QLIP enables a unified mixed-modality auto-regressive model for understanding and generation. | [üîó Paper](http://arxiv.org/abs/2502.05178v1) |
| [Long-VITA: Scaling Large Multi-modal Models to 1 Million Tokens with
  Leading Short-Context Accuray](http://arxiv.org/abs/2502.05177v1) | Yunhang Shen, Chaoyou Fu, Shaoqi Dong, Xiong Wang, Peixian Chen, Mengdan Zhang, Haoyu Cao, Ke Li, Xiawu Zheng, Yan Zhang, Yiyi Zhou, Rongrong Ji, Xing Sun | 2025-02-07 | RLHF, Multimodal AI, Optimization, Scaling Laws | Establishing the long-context capability of large vision-language models is crucial for video understanding, high-resolution image understanding, multi-modal agents and reasoning. We introduce Long-VITA, a simple yet effective large multi-modal model for long-context visual-language understanding tasks. It is adept at concurrently processing and analyzing modalities of image, video, and text over 4K frames or 1M tokens while delivering advanced performances on short-context multi-modal tasks. We propose an effective multi-modal training schema that starts with large language models and proceeds through vision-language alignment, general knowledge learning, and two sequential stages of long-sequence fine-tuning. We further implement context-parallelism distributed inference and logits-masked language modeling head to scale Long-VITA to infinitely long inputs of images and texts during model inference. Regarding training data, Long-VITA is built on a mix of $17$M samples from public datasets only and demonstrates the state-of-the-art performance on various multi-modal benchmarks, compared against recent cutting-edge models with internal data. Long-VITA is fully reproducible and supports both NPU and GPU platforms for training and testing. We hope Long-VITA can serve as a competitive baseline and offer valuable insights for the open-source community in advancing long-context multi-modal understanding. | [üîó Paper](http://arxiv.org/abs/2502.05177v1) |
| [AuraFusion360: Augmented Unseen Region Alignment for Reference-based
  360¬∞ Unbounded Scene Inpainting](http://arxiv.org/abs/2502.05176v1) | Chung-Ho Wu, Yang-Jung Chen, Ying-Huan Chen, Jie-Ying Lee, Bo-Hsu Ke, Chun-Wei Tuan Mu, Yi-Chuan Huang, Chin-Yang Lin, Min-Hung Chen, Yen-Yu Lin, Yu-Lun Liu | 2025-02-07 | RLHF, Diffusion Models, Prompt Engineering, Multimodal AI | Three-dimensional scene inpainting is crucial for applications from virtual reality to architectural visualization, yet existing methods struggle with view consistency and geometric accuracy in 360{\deg} unbounded scenes. We present AuraFusion360, a novel reference-based method that enables high-quality object removal and hole filling in 3D scenes represented by Gaussian Splatting. Our approach introduces (1) depth-aware unseen mask generation for accurate occlusion identification, (2) Adaptive Guided Depth Diffusion, a zero-shot method for accurate initial point placement without requiring additional training, and (3) SDEdit-based detail enhancement for multi-view coherence. We also introduce 360-USID, the first comprehensive dataset for 360{\deg} unbounded scene inpainting with ground truth. Extensive experiments demonstrate that AuraFusion360 significantly outperforms existing methods, achieving superior perceptual quality while maintaining geometric accuracy across dramatic viewpoint changes. See our project page for video results and the dataset at https://kkennethwu.github.io/aurafusion360/. | [üîó Paper](http://arxiv.org/abs/2502.05176v1) |
| [DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM
  Guardrails](http://arxiv.org/abs/2502.05163v1) | Yihe Deng, Yu Yang, Junkai Zhang, Wei Wang, Bo Li | 2025-02-07 | RLHF | The rapid advancement of large language models (LLMs) has increased the need for guardrail models to ensure responsible use, particularly in detecting unsafe and illegal content. While substantial safety data exist in English, multilingual guardrail modeling remains underexplored due to the scarcity of open-source safety data in other languages. To address this gap, we propose a novel two-player Reinforcement Learning (RL) framework, where a generator and a guardrail model co-evolve adversarially to produce high-quality synthetic data for multilingual guardrail training. We theoretically formalize this interaction as a two-player game, proving convergence to a Nash equilibrium. Empirical evaluations show that our model \ours outperforms state-of-the-art models, achieving nearly 10% improvement over LlamaGuard3 (8B) on English benchmarks while being 4.5x faster at inference with a significantly smaller model (0.5B). We achieve substantial advancements in multilingual safety tasks, particularly in addressing the imbalance for lower-resource languages in a collected real dataset. Ablation studies emphasize the critical role of synthetic data generation in bridging the imbalance in open-source data between English and other languages. These findings establish a scalable and efficient approach to synthetic data generation, paving the way for improved multilingual guardrail models to enhance LLM safety. Code, model, and data will be open-sourced at https://github.com/yihedeng9/DuoGuard. | [üîó Paper](http://arxiv.org/abs/2502.05163v1) |
| [Hummingbird: High Fidelity Image Generation via Multimodal Context
  Alignment](http://arxiv.org/abs/2502.05153v1) | Minh-Quan Le, Gaurav Mittal, Tianjian Meng, A S M Iftekhar, Vishwas Suryanarayanan, Barun Patra, Dimitris Samaras, Mei Chen | 2025-02-07 | RLHF, Diffusion Models, Multimodal AI | While diffusion models are powerful in generating high-quality, diverse synthetic data for object-centric tasks, existing methods struggle with scene-aware tasks such as Visual Question Answering (VQA) and Human-Object Interaction (HOI) Reasoning, where it is critical to preserve scene attributes in generated images consistent with a multimodal context, i.e. a reference image with accompanying text guidance query. To address this, we introduce Hummingbird, the first diffusion-based image generator which, given a multimodal context, generates highly diverse images w.r.t. the reference image while ensuring high fidelity by accurately preserving scene attributes, such as object interactions and spatial relationships from the text guidance. Hummingbird employs a novel Multimodal Context Evaluator that simultaneously optimizes our formulated Global Semantic and Fine-grained Consistency Rewards to ensure generated images preserve the scene attributes of reference images in relation to the text guidance while maintaining diversity. As the first model to address the task of maintaining both diversity and fidelity given a multimodal context, we introduce a new benchmark formulation incorporating MME Perception and Bongard HOI datasets. Benchmark experiments show Hummingbird outperforms all existing methods by achieving superior fidelity while maintaining diversity, validating Hummingbird's potential as a robust multimodal context-aligned image generator in complex visual tasks. | [üîó Paper](http://arxiv.org/abs/2502.05153v1) |
## üîπ Multimodal AI

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Multitwine: Multi-Object Compositing with Text and Layout Control](http://arxiv.org/abs/2502.05165v1) | Gemma Canet Tarr√©s, Zhe Lin, Zhifei Zhang, He Zhang, Andrew Gilbert, John Collomosse, Soo Ye Kim | 2025-02-07 | Multimodal AI | We introduce the first generative model capable of simultaneous multi-object compositing, guided by both text and layout. Our model allows for the addition of multiple objects within a scene, capturing a range of interactions from simple positional relations (e.g., next to, in front of) to complex actions requiring reposing (e.g., hugging, playing guitar). When an interaction implies additional props, like `taking a selfie', our model autonomously generates these supporting objects. By jointly training for compositing and subject-driven generation, also known as customization, we achieve a more balanced integration of textual and visual inputs for text-driven object compositing. As a result, we obtain a versatile model with state-of-the-art performance in both tasks. We further present a data generation pipeline leveraging visual and language models to effortlessly synthesize multimodal, aligned training data. | [üîó Paper](http://arxiv.org/abs/2502.05165v1) |
| [Transforming Science with Large Language Models: A Survey on AI-assisted
  Scientific Discovery, Experimentation, Content Generation, and Evaluation](http://arxiv.org/abs/2502.05151v1) | Steffen Eger, Yong Cao, Jennifer D'Souza, Andreas Geiger, Christian Greisinger, Stephanie Gross, Yufang Hou, Brigitte Krenn, Anne Lauscher, Yizhi Li, Chenghua Lin, Nafise Sadat Moosavi, Wei Zhao, Tristan Miller | 2025-02-07 | Multimodal AI, Training & Evaluation | With the advent of large multimodal language models, science is now at a threshold of an AI-based technological transformation. Recently, a plethora of new AI models and tools has been proposed, promising to empower researchers and academics worldwide to conduct their research more effectively and efficiently. This includes all aspects of the research cycle, especially (1) searching for relevant literature; (2) generating research ideas and conducting experimentation; generating (3) text-based and (4) multimodal content (e.g., scientific figures and diagrams); and (5) AI-based automatic peer review. In this survey, we provide an in-depth overview over these exciting recent developments, which promise to fundamentally alter the scientific research process for good. Our survey covers the five aspects outlined above, indicating relevant datasets, methods and results (including evaluation) as well as limitations and scope for future research. Ethical concerns regarding shortcomings of these tools and potential for misuse (fake science, plagiarism, harms to research integrity) take a particularly prominent place in our discussion. We hope that our survey will not only become a reference guide for newcomers to the field but also a catalyst for new AI-based initiatives in the area of "AI4Science". | [üîó Paper](http://arxiv.org/abs/2502.05151v1) |
| [Counting Fish with Temporal Representations of Sonar Video](http://arxiv.org/abs/2502.05129v1) | Kai Van Brunt, Justin Kay, Timm Haucke, Pietro Perona, Grant Van Horn, Sara Beery | 2025-02-07 | Multimodal AI | Accurate estimates of salmon escapement - the number of fish migrating upstream to spawn - are key data for conservation and fishery management. Existing methods for salmon counting using high-resolution imaging sonar hardware are non-invasive and compatible with computer vision processing. Prior work in this area has utilized object detection and tracking based methods for automated salmon counting. However, these techniques remain inaccessible to many sonar deployment sites due to limited compute and connectivity in the field. We propose an alternative lightweight computer vision method for fish counting based on analyzing echograms - temporal representations that compress several hundred frames of imaging sonar video into a single image. We predict upstream and downstream counts within 200-frame time windows directly from echograms using a ResNet-18 model, and propose a set of domain-specific image augmentations and a weakly-supervised training protocol to further improve results. We achieve a count error of 23% on representative data from the Kenai River in Alaska, demonstrating the feasibility of our approach. | [üîó Paper](http://arxiv.org/abs/2502.05129v1) |
## üîπ Optimization

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Data-Parallel Neural Network Training via Nonlinearly Preconditioned
  Trust-Region Method](http://arxiv.org/abs/2502.05133v1) | Samuel A. Cruz Alegr√≠a, Ken Trotti, Alena Kopaniƒç√°kov√°, Rolf Krause | 2025-02-07 | Optimization | Parallel training methods are increasingly relevant in machine learning (ML) due to the continuing growth in model and dataset sizes. We propose a variant of the Additively Preconditioned Trust-Region Strategy (APTS) for training deep neural networks (DNNs). The proposed APTS method utilizes a data-parallel approach to construct a nonlinear preconditioner employed in the nonlinear optimization strategy. In contrast to the common employment of Stochastic Gradient Descent (SGD) and Adaptive Moment Estimation (Adam), which are both variants of gradient descent (GD) algorithms, the APTS method implicitly adjusts the step sizes in each iteration, thereby removing the need for costly hyperparameter tuning. We demonstrate the performance of the proposed APTS variant using the MNIST and CIFAR-10 datasets. The results obtained indicate that the APTS variant proposed here achieves comparable validation accuracy to SGD and Adam, all while allowing for parallel training and obviating the need for expensive hyperparameter tuning. | [üîó Paper](http://arxiv.org/abs/2502.05133v1) |
## üîπ Scaling Laws

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Impulse measurements enhanced with squeezed readout light](http://arxiv.org/abs/2502.05168v1) | Tsai-Chen Lee, Jacob L. Beckey, Giacomo Marocco, Daniel Carney | 2025-02-07 | Scaling Laws | We quantify how squeezed light can reduce quantum measurement noise to levels below the standard quantum limit in impulse measurements with mechanical detectors. The broadband nature of the signal implies that frequency-dependent squeezing performs better than frequency-independent squeezing. We calculate the optimal scaling of the impulse sensitivity with the squeezing strength, and quantify degradations due to photodetection losses. Even for lossless measurement, we find there exists a fundamental limit to the benefit of squeezing that depends only on the system's mechanical properties. | [üîó Paper](http://arxiv.org/abs/2502.05168v1) |
## üîπ Training & Evaluation

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [MELON: Indirect Prompt Injection Defense via Masked Re-execution and
  Tool Comparison](http://arxiv.org/abs/2502.05174v1) | Kaijie Zhu, Xianjun Yang, Jindong Wang, Wenbo Guo, William Yang Wang | 2025-02-07 | Training & Evaluation, Security & Adversarial ML | Recent research has explored that LLM agents are vulnerable to indirect prompt injection (IPI) attacks, where malicious tasks embedded in tool-retrieved information can redirect the agent to take unauthorized actions. Existing defenses against IPI have significant limitations: either require essential model training resources, lack effectiveness against sophisticated attacks, or harm the normal utilities. We present MELON (Masked re-Execution and TooL comparisON), a novel IPI defense. Our approach builds on the observation that under a successful attack, the agent's next action becomes less dependent on user tasks and more on malicious tasks. Following this, we design MELON to detect attacks by re-executing the agent's trajectory with a masked user prompt modified through a masking function. We identify an attack if the actions generated in the original and masked executions are similar. We also include three key designs to reduce the potential false positives and false negatives. Extensive evaluation on the IPI benchmark AgentDojo demonstrates that MELON outperforms SOTA defenses in both attack prevention and utility preservation. Moreover, we show that combining MELON with a SOTA prompt augmentation defense (denoted as MELON-Aug) further improves its performance. We also conduct a detailed ablation study to validate our key designs. | [üîó Paper](http://arxiv.org/abs/2502.05174v1) |
## üîπ Model Evaluation

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [VideoRoPE: What Makes for Good Video Rotary Position Embedding?](http://arxiv.org/abs/2502.05173v1) | Xilin Wei, Xiaoran Liu, Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Jian Tong, Haodong Duan, Qipeng Guo, Jiaqi Wang, Xipeng Qiu, Dahua Lin | 2025-02-07 | Model Evaluation, Multimodal AI | While Rotary Position Embedding (RoPE) and its variants are widely adopted for their long-context capabilities, the extension of the 1D RoPE to video, with its complex spatio-temporal structure, remains an open challenge. This work first introduces a comprehensive analysis that identifies four key characteristics essential for the effective adaptation of RoPE to video, which have not been fully considered in prior work. As part of our analysis, we introduce a challenging V-NIAH-D (Visual Needle-In-A-Haystack with Distractors) task, which adds periodic distractors into V-NIAH. The V-NIAH-D task demonstrates that previous RoPE variants, lacking appropriate temporal dimension allocation, are easily misled by distractors. Based on our analysis, we introduce \textbf{VideoRoPE}, with a \textit{3D structure} designed to preserve spatio-temporal relationships. VideoRoPE features \textit{low-frequency temporal allocation} to mitigate periodic oscillations, a \textit{diagonal layout} to maintain spatial symmetry, and \textit{adjustable temporal spacing} to decouple temporal and spatial indexing. VideoRoPE consistently surpasses previous RoPE variants, across diverse downstream tasks such as long video retrieval, video understanding, and video hallucination. Our code will be available at \href{https://github.com/Wiselnn570/VideoRoPE}{https://github.com/Wiselnn570/VideoRoPE}. | [üîó Paper](http://arxiv.org/abs/2502.05173v1) |
| [Relationship between 2D and 3D Galaxy Stellar Mass and Correlations with
  Halo Mass](http://arxiv.org/abs/2502.05158v1) | Conghao Zhou, Alexie Leauthaud, Shuo Xu, Benedikt Diemer, Song Huang, Katya Leidig, Tesla Jeltema, Marco Gatti, Yifei Luo, Carlo Cannarozzo, Sven Heydenreich | 2025-02-07 | Model Evaluation, Responsible AI | Recent studies suggest that the stars in the outer regions of massive galaxies trace halo mass better than the inner regions and that an annular stellar mass provides a low scatter method of selecting galaxy clusters. However, we can only observe galaxies as projected two-dimensional objects on the sky. In this paper, we use a sample of simulated galaxies to study how well galaxy stellar mass profiles in three dimensions correlate with halo mass, and what effects arise when observationally projecting stellar profiles into two dimensions. We compare 2D and 3D outer stellar mass selections and find that they have similar performance as halo mass proxies and that, surprisingly, a 2D selection sometimes has marginally better performance. We also investigate whether the weak lensing profiles around galaxies selected by 2D outer stellar mass suffer from projection effects. We find that the lensing profiles of samples selected by 2D and 3D definitions are nearly identical, suggesting that the 2D selection does not create a bias. These findings underscore the promise of using outer stellar mass as a tool for identifying galaxy clusters. | [üîó Paper](http://arxiv.org/abs/2502.05158v1) |
| [Meta Audiobox Aesthetics: Unified Automatic Quality Assessment for
  Speech, Music, and Sound](http://arxiv.org/abs/2502.05139v1) | Andros Tjandra, Yi-Chiao Wu, Baishan Guo, John Hoffman, Brian Ellis, Apoorv Vyas, Bowen Shi, Sanyuan Chen, Matt Le, Nick Zacharov, Carleigh Wood, Ann Lee, Wei-Ning Hsu | 2025-02-07 | Model Evaluation, Multimodal AI, Training & Evaluation | The quantification of audio aesthetics remains a complex challenge in audio processing, primarily due to its subjective nature, which is influenced by human perception and cultural context. Traditional methods often depend on human listeners for evaluation, leading to inconsistencies and high resource demands. This paper addresses the growing need for automated systems capable of predicting audio aesthetics without human intervention. Such systems are crucial for applications like data filtering, pseudo-labeling large datasets, and evaluating generative audio models, especially as these models become more sophisticated. In this work, we introduce a novel approach to audio aesthetic evaluation by proposing new annotation guidelines that decompose human listening perspectives into four distinct axes. We develop and train no-reference, per-item prediction models that offer a more nuanced assessment of audio quality. Our models are evaluated against human mean opinion scores (MOS) and existing methods, demonstrating comparable or superior performance. This research not only advances the field of audio aesthetics but also provides open-source models and datasets to facilitate future work and benchmarking. We release our code and pre-trained model at: https://github.com/facebookresearch/audiobox-aesthetics | [üîó Paper](http://arxiv.org/abs/2502.05139v1) |
## üîπ Prompt Engineering

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth
  Approach](http://arxiv.org/abs/2502.05171v1) | Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, Tom Goldstein | 2025-02-07 | Prompt Engineering, Scaling Laws | We study a novel language model architecture that is capable of scaling test-time computation by implicitly reasoning in latent space. Our model works by iterating a recurrent block, thereby unrolling to arbitrary depth at test-time. This stands in contrast to mainstream reasoning models that scale up compute by producing more tokens. Unlike approaches based on chain-of-thought, our approach does not require any specialized training data, can work with small context windows, and can capture types of reasoning that are not easily represented in words. We scale a proof-of-concept model to 3.5 billion parameters and 800 billion tokens. We show that the resulting model can improve its performance on reasoning benchmarks, sometimes dramatically, up to a computation load equivalent to 50 billion parameters. | [üîó Paper](http://arxiv.org/abs/2502.05171v1) |
## üîπ AI Safety

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Efficient distributional regression trees learning algorithms for
  calibrated non-parametric probabilistic forecasts](http://arxiv.org/abs/2502.05157v1) | Duchemin Quentin, Obozinski Guillaume | 2025-02-07 | AI Safety | The perspective of developing trustworthy AI for critical applications in science and engineering requires machine learning techniques that are capable of estimating their own uncertainty. In the context of regression, instead of estimating a conditional mean, this can be achieved by producing a predictive interval for the output, or to even learn a model of the conditional probability $p(y x)$ of an output $y$ given input features $x$. While this can be done under parametric assumptions with, e.g. generalized linear model, these are typically too strong, and non-parametric models offer flexible alternatives. In particular, for scalar outputs, learning directly a model of the conditional cumulative distribution function of $y$ given $x$ can lead to more precise probabilistic estimates, and the use of proper scoring rules such as the weighted interval score (WIS) and the continuous ranked probability score (CRPS) lead to better coverage and calibration properties.   This paper introduces novel algorithms for learning probabilistic regression trees for the WIS or CRPS loss functions. These algorithms are made computationally efficient thanks to an appropriate use of known data structures - namely min-max heaps, weight-balanced binary trees and Fenwick trees. Through numerical experiments, we demonstrate that the performance of our methods is competitive with alternative approaches. Additionally, our methods benefit from the inherent interpretability and explainability of trees. As a by-product, we show how our trees can be used in the context of conformal prediction and explain why they are particularly well-suited for achieving group-conditional coverage guarantees. | [üîó Paper](http://arxiv.org/abs/2502.05157v1) |
## üîπ Ongoing Learning

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Chest X-ray Foundation Model with Global and Local Representations
  Integration](http://arxiv.org/abs/2502.05142v1) | Zefan Yang, Xuanang Xu, Jiajin Zhang, Ge Wang, Mannudeep K. Kalra, Pingkun Yan | 2025-02-07 | Ongoing Learning | Chest X-ray (CXR) is the most frequently ordered imaging test, supporting diverse clinical tasks from thoracic disease detection to postoperative monitoring. However, task-specific classification models are limited in scope, require costly labeled data, and lack generalizability to out-of-distribution datasets. To address these challenges, we introduce CheXFound, a self-supervised vision foundation model that learns robust CXR representations and generalizes effectively across a wide range of downstream tasks. We pretrain CheXFound on a curated CXR-1M dataset, comprising over one million unique CXRs from publicly available sources. We propose a Global and Local Representations Integration (GLoRI) module for downstream adaptations, by incorporating disease-specific local features with global image features for enhanced performance in multilabel classification. Our experimental results show that CheXFound outperforms state-of-the-art models in classifying 40 disease findings across different prevalence levels on the CXR-LT 24 dataset and exhibits superior label efficiency on downstream tasks with limited training data. Additionally, CheXFound achieved significant improvements on new tasks with out-of-distribution datasets, including opportunistic cardiovascular disease risk estimation and mortality prediction. These results highlight CheXFound's strong generalization capabilities, enabling diverse adaptations with improved label efficiency. The project source code is publicly available at https://github.com/RPIDIAL/CheXFound. | [üîó Paper](http://arxiv.org/abs/2502.05142v1) |
## üîπ General AI

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Observation of a dynamic magneto-chiral instability in photoexcited
  tellurium](http://arxiv.org/abs/2502.05170v1) | Yijing Huang, Nick Abboud, Yinchuan Lv, Penghao Zhu, Azel Murzabekova, Changjun Lee, Emma A. Pappas, Dominic Petruzzi, Jason Y. Yan, Dipanjan Chauduri, Peter Abbamonte, Daniel P. Shoemaker, Rafael M. Fernandes, Jorge Noronha, Fahad Mahmood | 2025-02-07 | General AI | In a system of charged chiral fermions driven out of equilibrium, an electric current parallel to the magnetic field can generate a dynamic instability by which electromagnetic waves become amplified. Whether a similar instability can occur in chiral solid-state systems remains an open question. Using time-domain terahertz (THz) emission spectroscopy, we detect signatures of what we dub a ``dynamic magneto-chiral instability" in elemental tellurium, a structurally chiral crystal. Upon transient photoexcitation in a moderate external magnetic field, tellurium emits THz radiation consisting of coherent modes that amplify over time. An explanation for this amplification is proposed using a theoretical model based on a dynamic instability of electromagnetic waves interacting with infrared-active oscillators of impurity acceptor states in tellurium to form an amplifying polariton. Our work not only uncovers the presence of a magneto-chiral instability but also highlights its promise for THz-wave amplification in chiral materials. | [üîó Paper](http://arxiv.org/abs/2502.05170v1) |
| [Flopping for FLOPs: Leveraging equivariance for computational efficiency](http://arxiv.org/abs/2502.05169v1) | Georg B√∂kman, David Nordstr√∂m, Fredrik Kahl | 2025-02-07 | General AI | Incorporating geometric invariance into neural networks enhances parameter efficiency but typically increases computational costs. This paper introduces new equivariant neural networks that preserve symmetry while maintaining a comparable number of floating-point operations (FLOPs) per parameter to standard non-equivariant networks. We focus on horizontal mirroring (flopping) invariance, common in many computer vision tasks. The main idea is to parametrize the feature spaces in terms of mirror-symmetric and mirror-antisymmetric features, i.e., irreps of the flopping group. This decomposes the linear layers to be block-diagonal, requiring half the number of FLOPs. Our approach reduces both FLOPs and wall-clock time, providing a practical solution for efficient, scalable symmetry-aware architectures. | [üîó Paper](http://arxiv.org/abs/2502.05169v1) |
| [Stirring supercooled colloidal liquids at the particle scale](http://arxiv.org/abs/2502.05166v1) | Piotr Habdas, Eric R. Weeks | 2025-02-07 | General AI | We study the decay of tangential velocity profiles with distance from a local disturbance in hard-sphere colloidal suspensions as the colloidal glass transition is approached. The disturbance, generated by a dimer of superparamagnetic particles rotated by an external magnetic field, enables a precise characterization of the system's response through confocal microscopy and tracking of individual particle dynamics. The tangential velocity profiles exhibit nearly exponential decay with distance. As particle density increases toward the colloidal glass transition, the characteristic length scale derived from exponential fits grows. We also observe that the colloidal particles slip against the rotating dimer, with less slip in samples which are closer to the glass transition. | [üîó Paper](http://arxiv.org/abs/2502.05166v1) |
| [Ramsey Theory on the Integer Grid: The "L" Problem](http://arxiv.org/abs/2502.05162v1) | Isaac Mammel, William Smith, Carl Yerger | 2025-02-07 | General AI | In an $[n] \times [n]$ integer grid, a monochromatic $L$ is any set of points $\{(i, j), (i, j+t), (i+t, j+t)\}$ for some positive integer $t$, where $1 \leq i, j, i+t, j+t \leq n$. In this paper, we investigate the upper bound for the smallest integer $n$ such that a $3$-colored $n \times n$ grid is guaranteed to contain a monochromatic $L$. We use various methods, such as counting intervals on the main diagonal and using Golomb rulers, to improve the upper bound. This bound originally sat at 2593, and we improve it first to 1803, then to 1573, then to 772, and finally to 493. In the latter part of this paper, we discuss the lower bound and our attempts to improve it using SAT solvers. | [üîó Paper](http://arxiv.org/abs/2502.05162v1) |
| [Estimated Roadway Segment Traffic Data by Vehicle Class for the United
  States: A Machine Learning Approach](http://arxiv.org/abs/2502.05161v1) | Brittany Antonczak, Meg Fay, Aviral Chawla, Gregory Rowangould | 2025-02-07 | General AI | The Highway Performance Monitoring System, managed by the Federal Highway Administration, provides essential data on average annual daily traffic across U.S. roadways, but it has limited representation of medium- and heavy-duty vehicles on non-interstate roads. This gap limits research and policy analysis on the impacts of truck traffic, especially concerning air quality and public health. To address this, we use random forest regression to estimate medium- and heavy-duty vehicle traffic volumes in areas with sparse data. This results in a more comprehensive dataset, which enables the estimation of traffic density at the census block level as a proxy for traffic-related air pollution exposure. Our high-resolution spatial data products, rigorously validated, provide a more accurate representation of truck traffic and its environmental and health impacts. These datasets are valuable for transportation planning, public health research, and policy decisions aimed at mitigating the effects of truck traffic on vulnerable communities exposed to air pollution. | [üîó Paper](http://arxiv.org/abs/2502.05161v1) |
| [A parameter study for LLL and BKZ with application to shortest vector
  problems](http://arxiv.org/abs/2502.05160v1) | Tobias K√∂ppl, Ren√© Zander, Louis Henkel, Nikolay Tcholtchev | 2025-02-07 | General AI | In this work, we study the solution of shortest vector problems (SVPs) arising in terms of learning with error problems (LWEs). LWEs are linear systems of equations over a modular ring, where a perturbation vector is added to the right-hand side. This type of problem is of great interest, since LWEs have to be solved in order to be able to break lattice-based cryptosystems as the Module-Lattice-Based Key-Encapsulation Mechanism published by NIST in 2024. Due to this fact, several classical and quantum-based algorithms have been studied to solve SVPs. Two well-known algorithms that can be used to simplify a given SVP are the Lenstra-Lenstra-Lov\'asz (LLL) algorithm and the Block Korkine-Zolotarev (BKZ) algorithm. LLL and BKZ construct bases that can be used to compute or approximate solutions of the SVP. We study the performance of both algorithms for SVPs with different sizes and modular rings. Thereby, application of LLL or BKZ to a given SVP is considered to be successful if they produce bases containing a solution vector of the SVP. | [üîó Paper](http://arxiv.org/abs/2502.05160v1) |
| [Tractable description of hydrodynamic limits of a class of interacting
  jump processes on sparse graphs](http://arxiv.org/abs/2502.05156v1) | Juniper Cocomello, Michel Davydov, Kavita Ramanan | 2025-02-07 | General AI | We consider dynamics of the empirical measure of vertex neighborhood states of Markov interacting jump processes on sparse random graphs, in a suitable asymptotic limit as the graph size goes to infinity. Under the assumption of a certain acyclic structure on single-particle transitions, we provide a tractable autonomous description of the evolution of this hydrodynamic limit in terms of a finite coupled system of ordinary differential equations. Key ingredients of the proof include a characterization of the hydrodynamic limit of the neighborhood empirical measure in terms of a certain local-field equation, well-posedness of its Markovian projection, and a Markov random field property of the time-marginals, which may be of independent interest. We also show how our results lead to principled approximations for classes of interacting jump processes and illustrate its efficacy via simulations on several examples, including an idealized model of seizure spread in the brain. | [üîó Paper](http://arxiv.org/abs/2502.05156v1) |
| [Deep Dynamic Probabilistic Canonical Correlation Analysis](http://arxiv.org/abs/2502.05155v1) | Shiqin Tang, Shujian Yu, Yining Dong, S. Joe Qin | 2025-02-07 | General AI | This paper presents Deep Dynamic Probabilistic Canonical Correlation Analysis (D2PCCA), a model that integrates deep learning with probabilistic modeling to analyze nonlinear dynamical systems. Building on the probabilistic extensions of Canonical Correlation Analysis (CCA), D2PCCA captures nonlinear latent dynamics and supports enhancements such as KL annealing for improved convergence and normalizing flows for a more flexible posterior approximation. D2PCCA naturally extends to multiple observed variables, making it a versatile tool for encoding prior knowledge about sequential datasets and providing a probabilistic understanding of the system's dynamics. Experimental validation on real financial datasets demonstrates the effectiveness of D2PCCA and its extensions in capturing latent dynamics. | [üîó Paper](http://arxiv.org/abs/2502.05155v1) |
| [Extreme-Scale EV Charging Infrastructure Planning for Last-Mile Delivery
  Using High-Performance Parallel Computing](http://arxiv.org/abs/2502.05152v1) | Waquar Kaleem, Taner Cokyasar, Jeffrey Larson, Omer Verbas, Tanveer Hossain Bhuiyan, Anirudh Subramanyam | 2025-02-07 | General AI | This paper addresses stochastic charger location and allocation (SCLA) problems under queue congestion for last-mile delivery using electric vehicles (EVs). The objective is to decide where to open charging stations and how many chargers of each type to install, subject to budgetary and waiting-time constraints. We formulate the problem as a mixed-integer non-linear program, where each station-charger pair is modeled as a multiserver queue with stochastic arrivals and service times to capture the notion of waiting in fleet operations. The model is extremely large, with billions of variables and constraints for a typical metropolitan area; and even loading the model in solver memory is difficult, let alone solving it. To address this challenge, we develop a Lagrangian-based dual decomposition framework that decomposes the problem by station and leverages parallelization on high-performance computing systems, where the subproblems are solved by using a cutting plane method and their solutions are collected at the master level. We also develop a three-step rounding heuristic to transform the fractional subproblem solutions into feasible integral solutions. Computational experiments on data from the Chicago metropolitan area with hundreds of thousands of households and thousands of candidate stations show that our approach produces high-quality solutions in cases where existing exact methods cannot even load the model in memory. We also analyze various policy scenarios, demonstrating that combining existing depots with newly built stations under multiagency collaboration substantially reduces costs and congestion. These findings offer a scalable and efficient framework for developing sustainable large-scale EV charging networks. | [üîó Paper](http://arxiv.org/abs/2502.05152v1) |
| [Nonlocal perimeters and variations: Extremality and decomposability for
  finite and infinite horizons](http://arxiv.org/abs/2502.05149v1) | Marcello Carioni, Leonardo Del Grande, Jos√© A. Iglesias, Hidde Sch√∂nberger | 2025-02-07 | General AI | We analyze the extremality and decomposability properties with respect to two types of nonlocal perimeters available in the literature, the Gagliardo perimeter based on the eponymous seminorms and the nonlocal distributional Caccioppoli perimeter, both with finite and infinite interaction ranges. A nonlocal notion of indecomposability associated to these perimeters is introduced, and we prove that in both cases it can be characterized solely in terms of the interaction range or horizon $\varepsilon$. Utilizing this, we show that it is possible to uniquely decompose a set into its $\varepsilon$-connected components, establishing a nonlocal analogue of the decomposition theorem of Ambrosio, Caselles, Masnou and Morel. Moreover, the extreme points of the balls induced by the Gagliardo and nonlocal total variation seminorm are identified, which naturally correspond to the two nonlocal perimeters. Surprisingly, while the extreme points in the former case are normalized indicator functions of $\varepsilon$-simple sets, akin to the classical TV-ball, in the latter case they are instead obtained from a nonlocal transformation applied to the extreme points of the TV-ball. Finally, we explore the nonlocal-to-local transition via a $\Gamma$-limit as $\varepsilon \rightarrow 0$ for both perimeters, recovering the classical Caccioppoli perimeter. | [üîó Paper](http://arxiv.org/abs/2502.05149v1) |
| [CodeSCM: Causal Analysis for Multi-Modal Code Generation](http://arxiv.org/abs/2502.05150v1) | Mukur Gupta, Noopur Bhatt, Suman Jana | 2025-02-07 | General AI | In this paper, we propose CodeSCM, a Structural Causal Model (SCM) for analyzing multi-modal code generation using large language models (LLMs). By applying interventions to CodeSCM, we measure the causal effects of different prompt modalities, such as natural language, code, and input-output examples, on the model. CodeSCM introduces latent mediator variables to separate the code and natural language semantics of a multi-modal code generation prompt. Using the principles of Causal Mediation Analysis on these mediators we quantify direct effects representing the model's spurious leanings. We find that, in addition to natural language instructions, input-output examples significantly influence code generation. | [üîó Paper](http://arxiv.org/abs/2502.05150v1) |
| [An Annotated Reading of 'The Singer of Tales' in the LLM Era](http://arxiv.org/abs/2502.05148v1) | Kush R. Varshney | 2025-02-07 | General AI | The Parry-Lord oral-formulaic theory was a breakthrough in understanding how oral narrative poetry is learned, composed, and transmitted by illiterate bards. In this paper, we provide an annotated reading of the mechanism underlying this theory from the lens of large language models (LLMs) and generative artificial intelligence (AI). We point out the the similarities and differences between oral composition and LLM generation, and comment on the implications to society and AI policy. | [üîó Paper](http://arxiv.org/abs/2502.05148v1) |
| [LP-DETR: Layer-wise Progressive Relations for Object Detection](http://arxiv.org/abs/2502.05147v1) | Zhengjian Kang, Ye Zhang, Xiaoyu Deng, Xintao Li, Yongzhe Zhang | 2025-02-07 | General AI | This paper presents LP-DETR (Layer-wise Progressive DETR), a novel approach that enhances DETR-based object detection through multi-scale relation modeling. Our method introduces learnable spatial relationships between object queries through a relation-aware self-attention mechanism, which adaptively learns to balance different scales of relations (local, medium and global) across decoder layers. This progressive design enables the model to effectively capture evolving spatial dependencies throughout the detection pipeline. Extensive experiments on COCO 2017 dataset demonstrate that our method improves both convergence speed and detection accuracy compared to standard self-attention module. The proposed method achieves competitive results, reaching 52.3\% AP with 12 epochs and 52.5\% AP with 24 epochs using ResNet-50 backbone, and further improving to 58.0\% AP with Swin-L backbone. Furthermore, our analysis reveals an interesting pattern: the model naturally learns to prioritize local spatial relations in early decoder layers while gradually shifting attention to broader contexts in deeper layers, providing valuable insights for future research in object detection. | [üîó Paper](http://arxiv.org/abs/2502.05147v1) |
| [Torsion pairs and 3-fold flops](http://arxiv.org/abs/2502.05146v1) | Parth Shimpi | 2025-02-07 | General AI | This paper classifies t-structures on the local derived category of a 3-fold flopping contraction, that are intermediate with respect to the heart of perverse coherent sheaves. Equivalently, this describes the complete lattice of torsion classes for the associated modification algebra. The intermediate hearts are (1) categories of coherent sheaves on birational models and tilts thereof in skyscrapers, (2) algebraic t-structures described in the homological minimal model programme, or (3) combinations of the above over appropriate open covers. An analogous classification is also proved for minimal (and partial) resolutions of Kleinian singularities, thus providing a description of all torsion pairs in the module categories of (contracted) affine preprojective algebras. The results have immediate applications to the classification of spherical modules and (semi)bricks, and are first steps towards describing all t-structures and spherical objects in derived categories of surfaces and 3-folds. | [üîó Paper](http://arxiv.org/abs/2502.05146v1) |
| [From Restless to Contextual: A Thresholding Bandit Approach to Improve
  Finite-horizon Performance](http://arxiv.org/abs/2502.05145v1) | Jiamin Xu, Ivan Nazarov, Aditya Rastogi, √Åfrica Peri√°√±ez, Kyra Gan | 2025-02-07 | General AI | Online restless bandits extend classic contextual bandits by incorporating state transitions and budget constraints, representing each agent as a Markov Decision Process (MDP). This framework is crucial for finite-horizon strategic resource allocation, optimizing limited costly interventions for long-term benefits. However, learning the underlying MDP for each agent poses a major challenge in finite-horizon settings. To facilitate learning, we reformulate the problem as a scalable budgeted thresholding contextual bandit problem, carefully integrating the state transitions into the reward design and focusing on identifying agents with action benefits exceeding a threshold. We establish the optimality of an oracle greedy solution in a simple two-state setting, and propose an algorithm that achieves minimax optimal constant regret in the online multi-state setting with heterogeneous agents and knowledge of outcomes under no intervention. We numerically show that our algorithm outperforms existing online restless bandit methods, offering significant improvements in finite-horizon performance. | [üîó Paper](http://arxiv.org/abs/2502.05145v1) |
| [Revisiting ab-initio excited state forces from many-body Green's
  function formalism: approximations and benchmark](http://arxiv.org/abs/2502.05144v1) | Rafael R. Del Grande, David A. Strubbe | 2025-02-07 | General AI | Ab initio techniques for studying the optical and vibrational properties of materials are well-established, but only a few recent studies have focused on the interaction between excitons and atomic vibrations. In this paper, we revisit the excited state forces method, which integrates results from GW/BSE and DFPT calculations to determine the gradient of the excited state energy. We explore its technical aspects, including convergence and the quality of approximations used. We successfully apply this method to investigate self-trapped excitons in LiF. The excited state forces method provides valuable insights into ionic dynamics in the excited state and the microscopic mechanism of exciton self-trapping. | [üîó Paper](http://arxiv.org/abs/2502.05144v1) |
| [pyMethods2Test: A Dataset of Python Tests Mapped to Focal Methods](http://arxiv.org/abs/2502.05143v1) | Idriss Abdelmadjid, Robert Dyer | 2025-02-07 | General AI | Python is one of the fastest-growing programming languages and currently ranks as the top language in many lists, even recently overtaking JavaScript as the top language on GitHub. Given its importance in data science and machine learning, it is imperative to be able to effectively train LLMs to generate good unit test cases for Python code. This motivates the need for a large dataset to provide training and testing data. To date, while other large datasets exist for languages like Java, none publicly exist for Python. Python poses difficult challenges in generating such a dataset, due to its less rigid naming requirements. In this work, we consider two commonly used Python unit testing frameworks: Pytest and unittest. We analyze a large corpus of over 88K open-source GitHub projects utilizing these testing frameworks. Using a carefully designed set of heuristics, we are able to locate over 22 million test methods. We then analyze the test and non-test code and map individual unit tests to the focal method being tested. This provides an explicit traceability link from the test to the tested method. Our pyMethods2Test dataset contains over 2 million of these focal method mappings, as well as the ability to generate useful context for input to LLMs. The pyMethods2Test dataset is publicly available on Zenodo at: https://doi.org/10.5281/zenodo.14264518 | [üîó Paper](http://arxiv.org/abs/2502.05143v1) |
| [Maximin Share Guarantees for Few Agents with Subadditive Valuations](http://arxiv.org/abs/2502.05141v1) | George Christodoulou, Vasilis Christoforidis, Symeon Mastrakoulis, Alkmini Sgouritsa | 2025-02-07 | General AI | We study the problem of fairly allocating a set of indivisible items among a set of agents. We consider the notion of (approximate) maximin share (MMS) and we provide an improved lower bound of $1/2$ (which is tight) for the case of subadditive valuations when the number of agents is at most four. We also provide a tight lower bound for the case of multiple agents, when they are equipped with one of two possible types of valuations. Moreover, we propose a new model that extends previously studied models in the area of fair division, which will hopefully give rise to further research. We demonstrate the usefulness of this model by employing it as a technical tool to derive our main result, and we provide a thorough analysis for this model for the case of three agents. Finally, we provide an improved impossibility result for the case of three submodular agents. | [üîó Paper](http://arxiv.org/abs/2502.05141v1) |
| [Mass-Optimal Low-Thrust Forced Periodic Trajectories in the Earth-Moon
  CR3BP](http://arxiv.org/abs/2502.05140v1) | Colby C. Merrill, Jackson Kulik, Matthew J. Bryan, Dmitry Savransky | 2025-02-07 | General AI | In Cislunar space, spacecraft are able to exploit naturally periodic orbits, which provide operational reliability. However, these periodic orbits only exist in a limited volume. Enabled by low-thrust propulsion, spacecraft can produce a greater number of periodic trajectories in Cislunar space. We describe a methodology for producing mass-optimal trajectories that enforce periodic structure in the circular-restricted three body problem and study the thrust-limited reachable set around a reference trajectory. In this study, we find that the thrust-limited mass-optimal reachable set is a superset of the energy-limited energy-optimal reachable set in the xy-plane. | [üîó Paper](http://arxiv.org/abs/2502.05140v1) |
| [Collision energy dependence in heavy ion collisions from nonlinear QCD
  evolution](http://arxiv.org/abs/2502.05138v1) | Heikki M√§ntysaari, Bj√∂rn Schenke, Chun Shen, Wenbin Zhao | 2025-02-07 | General AI | We explore the effects of including the energy dependence determined from evolution equations within the color glass condensate framework on observables in ultra-relativistic heavy-ion collisions. This amounts to integrating the JIMWLK evolution equations into the IP-Glasma model, which is then coupled to viscous relativistic hydrodynamics. This methodology allows for a systematic representation of nuclei at specific Bjorken-$x$ values, which are probed at different center-of-mass energies of the collision and rapidities of final state particles. Comparing to the conventional IP-Glasma model, we find significant effects on multiplicity distributions and particle spectra, especially in smaller collision systems at the highest center of mass energies. Our results highlight the importance of incorporating nonlinear QCD evolution in the description of heavy ion collisions at varying center of mass energies, as the precise extraction of transport coefficients will be affected. This work establishes a robust framework for understanding the quark gluon plasma and nuclear structure at high energy, integrating small-$x$ physics into the initial conditions of heavy-ion collisions. | [üîó Paper](http://arxiv.org/abs/2502.05138v1) |
| [Lie algebras with compatible scalar products for non-homogeneous
  Hamiltonian operators](http://arxiv.org/abs/2502.05137v1) | Giorgio Gubbiotti, Francesco Oliveri, Emanuele Sgroi, Pierandrea Vergallo | 2025-02-07 | General AI | We study from an algebraic and geometric viewpoint Hamiltonian operators which are sum of a non-degenerate first-order homogeneous operator and a Poisson tensor. In flat coordinates, also known as Darboux coordinates, these operators are uniquely determined by a triple composed by a Lie algebra, its most general non-degenerate quadratic Casimir and a 2-cocycle. We present some classes of operators associated to Lie algebras with non-degenerate quadratic Casimirs and we give a description of such operators in low dimensions. Finally, motivated by the example of the KdV equation we discuss the conditions of bi-Hamiltonianity of such operators. | [üîó Paper](http://arxiv.org/abs/2502.05137v1) |
| [Quantum Perfect Matchings](http://arxiv.org/abs/2502.05136v1) | David Cui, Laura Manƒçinska, Seyed Sajjad Nezhadi, David E. Roberson | 2025-02-07 | General AI | We investigate quantum and nonsignaling generalizations of perfect matchings in graphs using nonlocal games. Specifically, we introduce nonlocal games that test for $L$-perfect matchings in bipartite graphs, perfect matchings in general graphs and hypergraphs, and fractional perfect matchings. Our definitions come from the fact that these games are classical property tests for the corresponding matching conditions. We use the existence of perfect quantum and nonsignaling strategies for these games to define quantum and nonsignaling versions of perfect matchings. Finally, we provide characterizations of when graphs exhibit these extended properties:   - For nonsignaling matchings, we give a complete combinatorial characterizations. In particular, a graph has a nonsignaling perfect matching if and only if it admits a fractional perfect matching that has bounded value on triangles. \item In bipartite graphs, the nonsignaling $L$-perfect matching property is achieved exactly when the left component of the graph can be split into two disjoint subgraphs: one with a classical $L$-perfect matching and another with left-degree 2.   - In the quantum setting, we show that complete graphs $K_n$ with odd $n \geq 7$ have quantum perfect matchings. We prove that a graph has a quantum perfect matching if and only if the quantum independence number of its line graph is maximal, extending a classical relationship between perfect matchings and line graph independence numbers.   - For bipartite graphs, we establish that the $L$-perfect matching game does not exhibit quantum pseudotelepathy, but we characterize the quantum advantage for complete bipartite graphs $K_{n,2}$.   - Additionally, we prove that deciding quantum perfect matchings in hypergraphs is undecidable and leave open the question of its complexity in graphs. | [üîó Paper](http://arxiv.org/abs/2502.05136v1) |
| [De Sitter quantum gravity within the covariant Lorentzian approach to
  asymptotic safety](http://arxiv.org/abs/2502.05135v1) | Edoardo D'Angelo, Renata Ferrero, Markus B. Fr√∂b | 2025-02-07 | General AI | Recent technical and conceptual advancements in the asymptotic safety approach to quantum gravity have enabled studies of the UV completion of Lorentzian Einstein gravity, emphasizing the role of the state dependence. We present here the first complete investigation of the flow equations of the Einstein-Hilbert action within a cosmological spacetime, namely de Sitter spacetime. Using the newly derived graviton propagator for general gauges and masses in de Sitter spacetime, we analyze the dependence on the gauge and on finite renormalization parameters. Our results provide evidence of a UV fixed point for the most commonly used gauges. | [üîó Paper](http://arxiv.org/abs/2502.05135v1) |
| [Information-Theoretic Guarantees for Recovering Low-Rank Tensors from
  Symmetric Rank-One Measurements](http://arxiv.org/abs/2502.05134v1) | Eren C. Kƒ±zƒ±ldaƒü | 2025-02-07 | General AI | In this paper, we investigate the sample complexity of recovering tensors with low symmetric rank from symmetric rank-one measurements. This setting is particularly motivated by the study of higher-order interactions and the analysis of two-layer neural networks with polynomial activations (polynomial networks). Using a covering numbers argument, we analyze the performance of the symmetric rank minimization program and establish near-optimal sample complexity bounds when the underlying distribution is log-concave. Our measurement model involves random symmetric rank-one tensors, which lead to involved probability calculations. To address these challenges, we employ the Carbery-Wright inequality, a powerful tool for studying anti-concentration properties of random polynomials, and leverage orthogonal polynomials. Additionally, we provide a sample complexity lower bound based on Fano's inequality, and discuss broader implications of our results for two-layer polynomial networks. | [üîó Paper](http://arxiv.org/abs/2502.05134v1) |
| [Fluctuation thermometry of an atom-resolved quantum gas: Beyond the
  fluctuation-dissipation theorem](http://arxiv.org/abs/2502.05132v1) | Maxime Dixmerias, Joris Verstraten, Cyprien Daix, Bruno Peaudecerf, Tim de Jongh, Tarik Yefsah | 2025-02-07 | General AI | Thermometry is essential for studying many-body physics with ultracold atoms. Accurately measuring low temperatures in these systems, however, remains a significant challenge due to the absence of a universal thermometer. Most widely applicable methods, such as fitting of in-situ density profiles or standard fluctuation thermometry, are limited by the requirement of global thermal equilibrium and inapplicability to homogeneous systems. In this work, we introduce a novel in-situ thermometry for quantum gases, leveraging single-atom resolved measurements via quantum gas microscopy, and demonstrate it on an ideal Fermi gas. By analyzing number fluctuations in probe volumes with approximately one atom on average, we extract both global and local temperatures over a broad dynamic range. Unlike traditional fluctuation thermometry, our method does not rely on the fluctuation-dissipation theorem and is based instead on the exact relationship between number fluctuations and density-density correlations. In the low-temperature regime, it allows us to observe significant deviations from fluctuation-dissipation predictions, uncovering sub-extensive fluctuations. Our method is applicable to systems with arbitrary trapping potentials, requiring neither precise trap calibration nor global thermal equilibrium. This nearly universal thermometer for quantum gases overcomes key limitations of existing techniques, paving the way for more accurate and versatile temperature measurements in ultracold quantum systems. | [üîó Paper](http://arxiv.org/abs/2502.05132v1) |
| [Kolmogorov widths of an intersection of anisotropic finite-dimensional
  balls: case $2\le q_j<\infty$](http://arxiv.org/abs/2502.05131v1) | A. A. Vasil'eva | 2025-02-07 | General AI | Order estimates for the Kolmogorov $n$-widths of $\cap _{\alpha\in A}\nu_\alpha B^{\overline{k}}_{\overline{p}}$ in $l^{\overline{k}} _{\overline{q}}$ are obtained; here $\overline{q}=(q_1, \, \dots, \, q_d)$, $2\le q_j<\infty$, $j=1, \, \dots, \, d$. | [üîó Paper](http://arxiv.org/abs/2502.05131v1) |
