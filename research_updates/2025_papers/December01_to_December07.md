# üìå AI Research Papers (December01 to December07)

## üîπ LLM

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Speech World Model: Causal State-Action Planning with Explicit Reasoning for Speech](https://arxiv.org/abs/2512.05933v1) | Xuanru Zhou, Jiachen Lian, Henry Hong, Xinyi Yang, Gopala Anumanchipalli | 2025-12-05 | LLM, AI Safety | Current speech-language models (SLMs) typically use a cascade of speech encoder and large language model, treating speech understanding as a single black box. They analyze the content of speech well but reason weakly about other aspects, especially under sparse supervision. Thus, we argue for explicit reasoning over speech states and actions with modular and transparent decisions. Inspired by cognitive science we adopt a modular perspective and a world model view in which the system learns forward dynamics over latent states. We factorize speech understanding into four modules that communicate through a causal graph, establishing a cognitive state search space. Guided by posterior traces from this space, an instruction-tuned language model produces a concise causal analysis and a user-facing response, enabling counterfactual interventions and interpretability under partial supervision. We present the first graph based modular speech model for explicit reasoning and we will open source the model and data to promote the development of advanced speech understanding. | [üîó Paper](https://arxiv.org/abs/2512.05933v1) |
| [To Err Is Human: Systematic Quantification of Errors in Published AI Papers via LLM Analysis](https://arxiv.org/abs/2512.05925v1) | Federico Bianchi, Yongchan Kwon, Zachary Izzo, Linjun Zhang, James Zou | 2025-12-05 | LLM | How many mistakes do published AI papers contain? Peer-reviewed publications form the foundation upon which new research and knowledge are built. Errors that persist in the literature can propagate unnoticed, creating confusion in follow-up studies and complicating reproducibility. The accelerating pace of research and the increasing demands on the peer-review system make such mistakes harder to detect and avoid. To address this, we developed a Paper Correctness Checker based on GPT-5 to systematically identify mistakes in papers previously published at top AI conferences and journals. Our analysis focuses on objective mistakes-e.g., errors in formulas, derivations, calculations, figures, and tables-that have a clearly verifiable ground truth. We intentionally exclude subjective considerations such as novelty, importance, or writing quality. We find that published papers contain a non-negligible number of objective mistakes and that the average number of mistakes per paper has increased over time-from 3.8 in NeurIPS 2021 to 5.9 in NeurIPS 2025 (55.3% increase); from 4.1 in ICLR 2018 to 5.2 in ICLR 2025; and from 5.0 in TMLR 2022/23 to 5.5 in TMLR 2025. Human experts reviewed 316 potential mistakes identified by the AI Checker and confirmed that 263 were actual mistakes, corresponding to a precision of 83.2%. While most identified issues are relatively minor, correcting them would reduce confusion in the literature and strengthen reproducibility. The AI Checker also surfaced potentially more substantive mistakes that could affect the interpretation of results. Moreover, we show that the AI Checker can propose correct fixes for 75.8% of the identified mistakes. Overall, this study highlights the potential of frontier LLMs to detect and correct objective mistakes in published papers, helping to establish a firmer foundation of knowledge. | [üîó Paper](https://arxiv.org/abs/2512.05925v1) |
| [KQ-SVD: Compressing the KV Cache with Provable Guarantees on Attention Fidelity](https://arxiv.org/abs/2512.05916v1) | Damien Lesens, Beheshteh T. Rakhshan, Guillaume Rabusseau | 2025-12-05 | LLM | The Key-Value (KV) cache is central to the efficiency of transformer-based large language models (LLMs), storing previously computed vectors to accelerate inference. Yet, as sequence length and batch size grow, the cache becomes a major memory bottleneck. Prior compression methods typically apply low-rank decomposition to keys alone or attempt to jointly embed queries and keys, but both approaches neglect that attention fundamentally depends on their inner products. In this work, we prove that such strategies are suboptimal for approximating the attention matrix. We introduce KQ-SVD, a simple and computationally efficient method that directly performs an optimal low-rank decomposition of the attention matrix via a closed-form solution. By targeting the true source of redundancy, KQ-SVD preserves attention outputs with higher fidelity under compression. Extensive evaluations on LLaMA and Mistral models demonstrate that our approach consistently delivers superior projection quality. | [üîó Paper](https://arxiv.org/abs/2512.05916v1) |
## üîπ Diffusion Models

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [EditThinker: Unlocking Iterative Reasoning for Any Image Editor](https://arxiv.org/abs/2512.05965v1) | Hongyu Li, Manyuan Zhang, Dian Zheng, Ziyu Guo, Yimeng Jia, Kaituo Feng, Hao Yu, Yexin Liu, Yan Feng, Peng Pei, Xunliang Cai, Linjiang Huang, Hongsheng Li, Si Liu | 2025-12-05 | Diffusion Models, RLHF | Instruction-based image editing has emerged as a prominent research area, which, benefiting from image generation foundation models, have achieved high aesthetic quality, making instruction-following capability the primary challenge. Existing approaches improve instruction adherence via supervised or reinforcement learning, yet single-turn success rates remain limited due to inherent stochasticity and a lack of deliberation. In this work, we propose a deliberative editing framework to 'think' while they edit, which simulates the human cognitive loop by iteratively executing a Think-while-Edit cycle: Critiquing results and Refining instructions , followed by Repeating the generation until satisfactory. Specifically, we train a single MLLM, EditThinker, to act as the reasoning engine of this framework, which jointly produce the critique score, reasoning process, and refined instructions. We employ reinforcement learning to align the EditThinker's thinking with its editing, thereby generating more targeted instruction improvements. Extensive experiments on four benchmarks demonstrate that our approach significantly improves the instruction-following capability of any image editing model by a large margin. We will release our data construction framework, datasets, and models to benefit the community. | [üîó Paper](https://arxiv.org/abs/2512.05965v1) |
| [A Discontinuous Galerkin Consistent Splitting Method for the Incompressible Navier-Stokes Equations](https://arxiv.org/abs/2512.05919v1) | Dominik Still, Natalia Nebulishvili, Richard Schussnig, Katharina Kormann, Martin Kronbichler | 2025-12-05 | Diffusion Models | This work presents the discontinuous Galerkin discretization of the consistent splitting scheme proposed by Liu [J. Liu, J. Comp. Phys., 228(19), 2009]. The method enforces the divergence-free constraint implicitly, removing velocity--pressure compatibility conditions and eliminating pressure boundary layers. Consistent boundary conditions are imposed, also for settings with open and traction boundaries. Hence, accuracy in time is no longer limited by a splitting error.   The symmetric interior penalty Galerkin method is used for second spatial derivatives. The convective term is treated in a semi-implicit manner, which relaxes the CFL restriction of explicit schemes while avoiding the need to solve nonlinear systems required by fully implicit formulations. For improved mass conservation, Leray projection is combined with divergence and normal continuity penalty terms.   By selecting appropriate fluxes for both the divergence of the velocity field and the divergence of the convective operator, the consistent pressure boundary condition can be shown to reduce to contributions arising solely from the acceleration and the viscous term for the $L^2$ discretization. Per time step, the decoupled nature of the scheme with respect to the velocity and pressure fields leads to a single pressure Poisson equation followed by a single vector-valued convection-diffusion-reaction equation. We verify optimal convergence rates of the method in both space and time and demonstrate compatibility with higher-order time integration schemes. A series of numerical experiments, including the two-dimensional flow around a cylinder benchmark and the three-dimensional Taylor--Green vortex problem, verify the applicability to practically relevant flow problems. | [üîó Paper](https://arxiv.org/abs/2512.05919v1) |
## üîπ RLHF

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Whatever Remains Must Be True: Filtering Drives Reasoning in LLMs, Shaping Diversity](https://arxiv.org/abs/2512.05962v1) | Germ√°n Kruszewski, Pierre Erbacher, Jos Rozen, Marc Dymetman | 2025-12-05 | RLHF | Reinforcement Learning (RL) has become the de facto standard for tuning LLMs to solve tasks involving reasoning. However, growing evidence shows that models trained in such way often suffer from a significant loss in diversity. We argue that this arises because RL implicitly optimizes the "mode-seeking" or "zero-forcing" Reverse KL to a target distribution causing the model to concentrate mass on certain high-probability regions of the target while neglecting others. In this work, we instead begin from an explicit target distribution, obtained by filtering out incorrect answers while preserving the relative probabilities of correct ones. Starting from a pre-trained LLM, we approximate this target distribution using the $Œ±$-divergence family, which unifies prior approaches and enables direct control of the precision-diversity trade-off by interpolating between mode-seeking and mass-covering divergences. On a Lean theorem-proving benchmark, our method achieves state-of-the-art performance along the coverage-precision Pareto frontier, outperforming all prior methods on the coverage axis. | [üîó Paper](https://arxiv.org/abs/2512.05962v1) |
| [Variational Quantum Rainbow Deep Q-Network for Optimizing Resource Allocation Problem](https://arxiv.org/abs/2512.05946v1) | Truong Thanh Hung Nguyen, Truong Thinh Nguyen, Hung Cao | 2025-12-05 | RLHF | Resource allocation remains NP-hard due to combinatorial complexity. While deep reinforcement learning (DRL) methods, such as the Rainbow Deep Q-Network (DQN), improve scalability through prioritized replay and distributional heads, classical function approximators limit their representational power. We introduce Variational Quantum Rainbow DQN (VQR-DQN), which integrates ring-topology variational quantum circuits with Rainbow DQN to leverage quantum superposition and entanglement. We frame the human resource allocation problem (HRAP) as a Markov decision process (MDP) with combinatorial action spaces based on officer capabilities, event schedules, and transition times. On four HRAP benchmarks, VQR-DQN achieves 26.8% normalized makespan reduction versus random baselines and outperforms Double DQN and classical Rainbow DQN by 4.9-13.4%. These gains align with theoretical connections between circuit expressibility, entanglement, and policy quality, demonstrating the potential of quantum-enhanced DRL for large-scale resource allocation. Our implementation is available at: https://github.com/Analytics-Everywhere-Lab/qtrl/. | [üîó Paper](https://arxiv.org/abs/2512.05946v1) |
## üîπ Multimodal AI

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Training-Time Action Conditioning for Efficient Real-Time Chunking](https://arxiv.org/abs/2512.05964v1) | Kevin Black, Allen Z. Ren, Michael Equi, Sergey Levine | 2025-12-05 | Multimodal AI, Production and Deployment | Real-time chunking (RTC) enables vision-language-action models (VLAs) to generate smooth, reactive robot trajectories by asynchronously predicting action chunks and conditioning on previously committed actions via inference-time inpainting. However, this inpainting method introduces computational overhead that increases inference latency. In this work, we propose a simple alternative: simulating inference delay at training time and conditioning on action prefixes directly, eliminating any inference-time overhead. Our method requires no modifications to the model architecture or robot runtime, and can be implemented with only a few additional lines of code. In simulated experiments, we find that training-time RTC outperforms inference-time RTC at higher inference delays. In real-world experiments on box building and espresso making tasks with the $œÄ_{0.6}$ VLA, we demonstrate that training-time RTC maintains both task performance and speed parity with inference-time RTC while being computationally cheaper. Our results suggest that training-time action conditioning is a practical drop-in replacement for inference-time inpainting in real-time robot control. | [üîó Paper](https://arxiv.org/abs/2512.05964v1) |
| [M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG](https://arxiv.org/abs/2512.05959v1) | David Anugraha, Patrick Amadeus Irawan, Anshul Singh, En-Shiun Annie Lee, Genta Indra Winata | 2025-12-05 | Multimodal AI, Training & Evaluation, RAG | Vision-language models (VLMs) have achieved strong performance in visual question answering (VQA), yet they remain constrained by static training data. Retrieval-Augmented Generation (RAG) mitigates this limitation by enabling access to up-to-date, culturally grounded, and multilingual information; however, multilingual multimodal RAG remains largely underexplored. We introduce M4-RAG, a massive-scale benchmark covering 42 languages and 56 regional dialects and registers, comprising over 80,000 culturally diverse image-question pairs for evaluating retrieval-augmented VQA across languages and modalities. To balance realism with reproducibility, we build a controlled retrieval environment containing millions of carefully curated multilingual documents relevant to the query domains, approximating real-world retrieval conditions while ensuring consistent experimentation. Our systematic evaluation reveals that although RAG consistently benefits smaller VLMs, it fails to scale to larger models and often even degrades their performance, exposing a critical mismatch between model size and current retrieval effectiveness. M4-RAG provides a foundation for advancing next-generation RAG systems capable of reasoning seamlessly across languages, modalities, and cultural contexts. | [üîó Paper](https://arxiv.org/abs/2512.05959v1) |
| [SIMPACT: Simulation-Enabled Action Planning using Vision-Language Models](https://arxiv.org/abs/2512.05955v1) | Haowen Liu, Shaoxiong Yao, Haonan Chen, Jiawei Gao, Jiayuan Mao, Jia-Bin Huang, Yilun Du | 2025-12-05 | Multimodal AI | Vision-Language Models (VLMs) exhibit remarkable common-sense and semantic reasoning capabilities. However, they lack a grounded understanding of physical dynamics. This limitation arises from training VLMs on static internet-scale visual-language data that contain no causal interactions or action-conditioned changes. Consequently, it remains challenging to leverage VLMs for fine-grained robotic manipulation tasks that require physical understanding, reasoning, and corresponding action planning. To overcome this, we present SIMPACT, a test-time, SIMulation-enabled ACTion Planning framework that equips VLMs with physical reasoning through simulation-in-the-loop world modeling, without requiring any additional training. From a single RGB-D observation, SIMPACT efficiently constructs physics simulations, enabling the VLM to propose informed actions, observe simulated rollouts, and iteratively refine its reasoning. By integrating language reasoning with physics prediction, our simulation-enabled VLM can understand contact dynamics and action outcomes in a physically grounded way. Our method demonstrates state-of-the-art performance on five challenging, real-world rigid-body and deformable manipulation tasks that require fine-grained physical reasoning, outperforming existing general-purpose robotic manipulation models. Our results demonstrate that embedding physics understanding via efficient simulation into VLM reasoning at test time offers a promising path towards generalizable embodied intelligence. Project webpage can be found at https://simpact-bot.github.io | [üîó Paper](https://arxiv.org/abs/2512.05955v1) |
| [Impugan: Learning Conditional Generative Models for Robust Data Imputation](https://arxiv.org/abs/2512.05950v1) | Zalish Mahmud, Anantaa Kotal, Aritran Piplai | 2025-12-05 | Multimodal AI | Incomplete data are common in real-world applications. Sensors fail, records are inconsistent, and datasets collected from different sources often differ in scale, sampling rate, and quality. These differences create missing values that make it difficult to combine data and build reliable models. Standard imputation methods such as regression models, expectation-maximization, and multiple imputation rely on strong assumptions about linearity and independence. These assumptions rarely hold for complex or heterogeneous data, which can lead to biased or over-smoothed estimates. We propose Impugan, a conditional Generative Adversarial Network (cGAN) for imputing missing values and integrating heterogeneous datasets. The model is trained on complete samples to learn how missing variables depend on observed ones. During inference, the generator reconstructs missing entries from available features, and the discriminator enforces realism by distinguishing true from imputed data. This adversarial process allows Impugan to capture nonlinear and multimodal relationships that conventional methods cannot represent. In experiments on benchmark datasets and a multi-source integration task, Impugan achieves up to 82\% lower Earth Mover's Distance (EMD) and 70\% lower mutual-information deviation (MI) compared to leading baselines. These results show that adversarially trained generative models provide a scalable and principled approach for imputing and merging incomplete, heterogeneous data. Our model is available at: github.com/zalishmahmud/impuganBigData2025 | [üîó Paper](https://arxiv.org/abs/2512.05950v1) |
| [TRACE: A Framework for Analyzing and Enhancing Stepwise Reasoning in Vision-Language Models](https://arxiv.org/abs/2512.05943v1) | Shima Imani, Seungwhan Moon, Lambert Mathias, Lu Zhang, Babak Damavandi | 2025-12-05 | Multimodal AI, Training & Evaluation | Reliable mathematical and scientific reasoning remains an open challenge for large vision-language models. Standard final-answer evaluation often masks reasoning errors, allowing silent failures to persist. To address this gap, we introduce TRACE, a framework for Transparent Reasoning And Consistency Evaluation that diagnoses reasoning trajectories rather than only end results. At its core, TRACE leverages Auxiliary Reasoning Sets, compact sub question answer pairs that decompose complex problems, evaluate intermediate steps through consistency-based metrics, and expose failures overlooked by standard evaluation. Our experiments show that consistency across ARS correlates with final-answer correctness and helps pinpoint the reasoning steps where failures arise, offering actionable signals for model improvement. Furthermore, TRACE defines confidence regions that distinguish reliable from unreliable reasoning paths, supporting effective filtering, debugging, and model refinement. | [üîó Paper](https://arxiv.org/abs/2512.05943v1) |
| [World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty](https://arxiv.org/abs/2512.05927v1) | Zhiting Mei, Tenny Yin, Micah Baker, Ola Shorinwa, Anirudha Majumdar | 2025-12-05 | Multimodal AI, Training & Evaluation, Ongoing Learning, Model Evaluation | Recent advances in generative video models have led to significant breakthroughs in high-fidelity video synthesis, specifically in controllable video generation where the generated video is conditioned on text and action inputs, e.g., in instruction-guided video editing and world modeling in robotics. Despite these exceptional capabilities, controllable video models often hallucinate - generating future video frames that are misaligned with physical reality - which raises serious concerns in many tasks such as robot policy evaluation and planning. However, state-of-the-art video models lack the ability to assess and express their confidence, impeding hallucination mitigation. To rigorously address this challenge, we propose C3, an uncertainty quantification (UQ) method for training continuous-scale calibrated controllable video models for dense confidence estimation at the subpatch level, precisely localizing the uncertainty in each generated video frame. Our UQ method introduces three core innovations to empower video models to estimate their uncertainty. First, our method develops a novel framework that trains video models for correctness and calibration via strictly proper scoring rules. Second, we estimate the video model's uncertainty in latent space, avoiding training instability and prohibitive training costs associated with pixel-space approaches. Third, we map the dense latent-space uncertainty to interpretable pixel-level uncertainty in the RGB space for intuitive visualization, providing high-resolution uncertainty heatmaps that identify untrustworthy regions. Through extensive experiments on large-scale robot learning datasets (Bridge and DROID) and real-world evaluations, we demonstrate that our method not only provides calibrated uncertainty estimates within the training distribution, but also enables effective out-of-distribution detection. | [üîó Paper](https://arxiv.org/abs/2512.05927v1) |
## üîπ Optimization

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Consequences of Kernel Regularity for Bandit Optimization](https://arxiv.org/abs/2512.05957v1) | Madison Lee, Tara Javidi | 2025-12-05 | Optimization | In this work we investigate the relationship between kernel regularity and algorithmic performance in the bandit optimization of RKHS functions. While reproducing kernel Hilbert space (RKHS) methods traditionally rely on global kernel regressors, it is also common to use a smoothness-based approach that exploits local approximations. We show that these perspectives are deeply connected through the spectral properties of isotropic kernels. In particular, we characterize the Fourier spectra of the Mat√©rn, square-exponential, rational-quadratic, $Œ≥$-exponential, piecewise-polynomial, and Dirichlet kernels, and show that the decay rate determines asymptotic regret from both viewpoints. For kernelized bandit algorithms, spectral decay yields upper bounds on the maximum information gain, governing worst-case regret, while for smoothness-based methods, the same decay rates establish H√∂lder space embeddings and Besov space norm-equivalences, enabling local continuity analysis. These connections show that kernel-based and locally adaptive algorithms can be analyzed within a unified framework. This allows us to derive explicit regret bounds for each kernel family, obtaining novel results in several cases and providing improved analysis for others. Furthermore, we analyze LP-GP-UCB, an algorithm that combines both approaches, augmenting global Gaussian process surrogates with local polynomial estimators. While the hybrid approach does not uniformly dominate specialized methods, it achieves order-optimality across multiple kernel families. | [üîó Paper](https://arxiv.org/abs/2512.05957v1) |
| [Designing an Optimal Sensor Network via Minimizing Information Loss](https://arxiv.org/abs/2512.05940v1) | Daniel Waxman, Fernando Llorente, Katia Lamer, Petar M. Djuriƒá | 2025-12-05 | Optimization | Optimal experimental design is a classic topic in statistics, with many well-studied problems, applications, and solutions. The design problem we study is the placement of sensors to monitor spatiotemporal processes, explicitly accounting for the temporal dimension in our modeling and optimization. We observe that recent advancements in computational sciences often yield large datasets based on physics-based simulations, which are rarely leveraged in experimental design. We introduce a novel model-based sensor placement criterion, along with a highly-efficient optimization algorithm, which integrates physics-based simulations and Bayesian experimental design principles to identify sensor networks that "minimize information loss" from simulated data. Our technique relies on sparse variational inference and (separable) Gauss-Markov priors, and thus may adapt many techniques from Bayesian experimental design. We validate our method through a case study monitoring air temperature in Phoenix, Arizona, using state-of-the-art physics-based simulations. Our results show our framework to be superior to random or quasi-random sampling, particularly with a limited number of sensors. We conclude by discussing practical considerations and implications of our framework, including more complex modeling tools and real-world deployments. | [üîó Paper](https://arxiv.org/abs/2512.05940v1) |
| [Qualitative and Quantitative Analysis of Riemannian Optimization Methods for Ground States of Rotating Multicomponent Bose-Einstein Condensates](https://arxiv.org/abs/2512.05939v1) | Martin Hermann, Tatjana Stykel, Mahima Yadav | 2025-12-05 | Optimization | We develop and analyze Riemannian optimization methods for computing ground states of rotating multicomponent Bose-Einstein condensates, defined as minimizers of the Gross-Pitaevskii energy functional. To resolve the non-uniqueness of ground states induced by phase invariance, we work on a quotient manifold endowed with a general Riemannian metric. By introducing an auxiliary phase-aligned iteration and employing fixed-point convergence theory, we establish a unified local convergence framework for Riemannian gradient descent methods and derive explicit convergence rates. Specializing this framework to two metrics tailored to the energy landscape, we study the energy-adaptive and Lagrangian-based Riemannian gradient descent methods. While monotone energy decay and global convergence are established only for the former, a quantified local convergence analysis is provided for both methods. Numerical experiments confirm the theoretical results and demonstrate that the Lagrangian-based method, which incorporates second-order information on the energy functional and mass constraints, achieves faster local convergence than the energy-adaptive scheme. | [üîó Paper](https://arxiv.org/abs/2512.05939v1) |
| [On the Bayes Inconsistency of Disagreement Discrepancy Surrogates](https://arxiv.org/abs/2512.05931v1) | Neil G. Marchant, Andrew C. Cullen, Feng Liu, Sarah M. Erfani | 2025-12-05 | Optimization | Deep neural networks often fail when deployed in real-world contexts due to distribution shift, a critical barrier to building safe and reliable systems. An emerging approach to address this problem relies on \emph{disagreement discrepancy} -- a measure of how the disagreement between two models changes under a shifting distribution. The process of maximizing this measure has seen applications in bounding error under shifts, testing for harmful shifts, and training more robust models. However, this optimization involves the non-differentiable zero-one loss, necessitating the use of practical surrogate losses. We prove that existing surrogates for disagreement discrepancy are not Bayes consistent, revealing a fundamental flaw: maximizing these surrogates can fail to maximize the true disagreement discrepancy. To address this, we introduce new theoretical results providing both upper and lower bounds on the optimality gap for such surrogates. Guided by this theory, we propose a novel disagreement loss that, when paired with cross-entropy, yields a provably consistent surrogate for disagreement discrepancy. Empirical evaluations across diverse benchmarks demonstrate that our method provides more accurate and robust estimates of disagreement discrepancy than existing approaches, particularly under challenging adversarial conditions. | [üîó Paper](https://arxiv.org/abs/2512.05931v1) |
## üîπ Scaling Laws

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Zoom in, Click out: Unlocking and Evaluating the Potential of Zooming for GUI Grounding](https://arxiv.org/abs/2512.05941v1) | Zhiyuan Jiang, Shenghao Xie, Wenyi Li, Wenqiang Zu, Peihang Li, Jiahao Qiu, Siqi Pei, Lei Ma, Tiejun Huang, Mengdi Wang, Shilong Liu | 2025-12-05 | Scaling Laws, Multimodal AI | Grounding is a fundamental capability for building graphical user interface (GUI) agents. Although existing approaches rely on large-scale bounding box supervision, they still face various challenges, such as cross-platform generalization, complex layout analysis, and fine-grained element localization. In this paper, we investigate zoom as a strong yet underexplored prior for GUI grounding, and propose a training-free method, ZoomClick. By characterizing four key properties of zoom (i.e., pre-zoom, depth, shrink size, minimal crop size), we unlock its full capabilities for dynamic spatial focusing and adaptive context switching. Experiments demonstrate that our method significantly boosts the performance of both general vision-language and specialized GUI grounding models, achieving state-of-the-art results on several mainstream benchmarks; for example, UI-Venus-72B attains a 73.1% success rate on ScreenSpot-Pro. Furthermore, we present GUIZoom-Bench, a benchmark for evaluating model adaptability to zoom, aiming to inspire future research on improving zoom for further training and test-time scaling in GUI grounding tasks. | [üîó Paper](https://arxiv.org/abs/2512.05941v1) |
## üîπ Training & Evaluation

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [SymPyBench: A Dynamic Benchmark for Scientific Reasoning with Executable Python Code](https://arxiv.org/abs/2512.05954v1) | Shima Imani, Seungwhan Moon, Adel Ahmadyan, Lu Zhang, Kirmani Ahmed, Babak Damavandi | 2025-12-05 | Training & Evaluation | We introduce, a large-scale synthetic benchmark of 15,045 university-level physics problems (90/10% train/test split). Each problem is fully parameterized, supporting an effectively infinite range of input configurations, and is accompanied by structured, step-by-step reasoning and executable Python code that produces the ground-truth solution for any parameter set. The benchmark contains three question types: MC-Symbolic (multiple-choice with symbolic options), MC-Numerical (multiple-choice with numerical options), and free-form (open-ended responses). These diverse formats test complementary reasoning skills. By leveraging the dynamic, code-driven nature of the benchmark, we introduce three novel evaluation metrics in addition to standard accuracy: Consistency Score, Failure Rate, and Confusion Rate, that quantify variability and uncertainty across problem variants. Experiments with state-of-the-art instruction-tuned language models reveal both strengths and limitations in scientific reasoning, positioning SymPyBench as a foundation for developing more robust and interpretable reasoning systems | [üîó Paper](https://arxiv.org/abs/2512.05954v1) |
| [Adsorption energies are necessary but not sufficient to identify good catalysts](https://arxiv.org/abs/2512.05938v1) | Shahana Chatterjee, Alexander Davis, Lena Podina, Divya Sharma, Yoshua Bengio, Alexandre Duval, Oleksandr Voznyy, Alex Hern√°ndez-Garcia, David Rolnick, F√©lix Therrien | 2025-12-05 | Training & Evaluation | As a core technology for green chemical synthesis and electrochemical energy storage, electrocatalysis is central to decarbonization strategies aimed at combating climate change. In this context, computational and machine learning driven catalyst discovery has emerged as a major research focus. These approaches frequently use the thermodynamic overpotential, calculated from adsorption free energies of reaction intermediates, as a key parameter in their analysis. In this paper, we explore the large-scale applicability of such overpotential estimates for identifying good catalyst candidates by using datasets from the Open Catalyst Project (OC20 and OC22). We start by quantifying the uncertainty in predicting adsorption energies using \textit{ab initio} methods and find that $\sim$0.3-0.5 eV is a conservative estimate for a single adsorption energy prediction. We then compute the overpotential of all materials in the OC20 and OC22 datasets for the hydrogen and oxygen evolution reactions. We find that while the overpotential allows the identification of known good catalysts such as platinum and iridium oxides, the uncertainty is large enough to misclassify a broad fraction of the datasets as ``good'', which limits its value as a screening criterion. These results question the reliance on overpotential estimation as a primary evaluation metric to sort through catalyst candidates and calls for a shift in focus in the computational catalysis and machine learning communities towards other metrics such as synthesizability, stability, lifetime or affordability. | [üîó Paper](https://arxiv.org/abs/2512.05938v1) |
| [A Residual Variance Matching Recursive Least Squares Filter for Real-time UAV Terrain Following](https://arxiv.org/abs/2512.05918v1) | Xiaobo Wu, Youmin Zhang | 2025-12-05 | Training & Evaluation | Accurate real-time waypoints estimation for the UAV-based online Terrain Following during wildfire patrol missions is critical to ensuring flight safety and enabling wildfire detection. However, existing real-time filtering algorithms struggle to maintain accurate waypoints under measurement noise in nonlinear and time-varying systems, posing risks of flight instability and missed wildfire detections during UAV-based terrain following. To address this issue, a Residual Variance Matching Recursive Least Squares (RVM-RLS) filter, guided by a Residual Variance Matching Estimation (RVME) criterion, is proposed to adaptively estimate the real-time waypoints of nonlinear, time-varying UAV-based terrain following systems. The proposed method is validated using a UAV-based online terrain following system within a simulated terrain environment. Experimental results show that the RVM-RLS filter improves waypoints estimation accuracy by approximately 88$\%$ compared with benchmark algorithms across multiple evaluation metrics. These findings demonstrate both the methodological advances in real-time filtering and the practical potential of the RVM-RLS filter for UAV-based online wildfire patrol. | [üîó Paper](https://arxiv.org/abs/2512.05918v1) |
## üîπ Responsible AI

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [AQUA-Net: Adaptive Frequency Fusion and Illumination Aware Network for Underwater Image Enhancement](https://arxiv.org/abs/2512.05960v1) | Munsif Ali, Najmul Hassan, Lucia Ventura, Davide Di Bari, Simonepietro Canese | 2025-12-05 | Responsible AI, Multimodal AI, Training & Evaluation, Model Evaluation | Underwater images often suffer from severe color distortion, low contrast, and a hazy appearance due to wavelength-dependent light absorption and scattering. Simultaneously, existing deep learning models exhibit high computational complexity, which limits their practical deployment for real-time underwater applications. To address these challenges, this paper presents a novel underwater image enhancement model, called Adaptive Frequency Fusion and Illumination Aware Network (AQUA-Net). It integrates a residual encoder decoder with dual auxiliary branches, which operate in the frequency and illumination domains. The frequency fusion encoder enriches spatial representations with frequency cues from the Fourier domain and preserves fine textures and structural details. Inspired by Retinex, the illumination-aware decoder performs adaptive exposure correction through a learned illumination map that separates reflectance from lighting effects. This joint spatial, frequency, and illumination design enables the model to restore color balance, visual contrast, and perceptual realism under diverse underwater conditions. Additionally, we present a high-resolution, real-world underwater video-derived dataset from the Mediterranean Sea, which captures challenging deep-sea conditions with realistic visual degradations to enable robust evaluation and development of deep learning models. Extensive experiments on multiple benchmark datasets show that AQUA-Net performs on par with SOTA in both qualitative and quantitative evaluations while using less number of parameters. Ablation studies further confirm that the frequency and illumination branches provide complementary contributions that improve visibility and color representation. Overall, the proposed model shows strong generalization capability and robustness, and it provides an effective solution for real-world underwater imaging applications. | [üîó Paper](https://arxiv.org/abs/2512.05960v1) |
| [Synset Signset Germany: a Synthetic Dataset for German Traffic Sign Recognition](https://arxiv.org/abs/2512.05936v1) | Anne Sielemann, Lena Loercher, Max-Lion Schumacher, Stefan Wolf, Masoud Roschani, Jens Ziehn | 2025-12-05 | Responsible AI, Model Evaluation | In this paper, we present a synthesis pipeline and dataset for training / testing data in the task of traffic sign recognition that combines the advantages of data-driven and analytical modeling: GAN-based texture generation enables data-driven dirt and wear artifacts, rendering unique and realistic traffic sign surfaces, while the analytical scene modulation achieves physically correct lighting and allows detailed parameterization. In particular, the latter opens up applications in the context of explainable AI (XAI) and robustness tests due to the possibility of evaluating the sensitivity to parameter changes, which we demonstrate with experiments. Our resulting synthetic traffic sign recognition dataset Synset Signset Germany contains a total of 105500 images of 211 different German traffic sign classes, including newly published (2020) and thus comparatively rare traffic signs. In addition to a mask and a segmentation image, we also provide extensive metadata including the stochastically selected environment and imaging effect parameters for each image. We evaluate the degree of realism of Synset Signset Germany on the real-world German Traffic Sign Recognition Benchmark (GTSRB) and in comparison to CATERED, a state-of-the-art synthetic traffic sign recognition dataset. | [üîó Paper](https://arxiv.org/abs/2512.05936v1) |
| [PRiSM: An Agentic Multimodal Benchmark for Scientific Reasoning via Python-Grounded Evaluation](https://arxiv.org/abs/2512.05930v1) | Shima Imani, Seungwhan Moon, Adel Ahmadyan, Lu Zhang, Kirmani Ahmed, Babak Damavandi | 2025-12-05 | Responsible AI, Multimodal AI, Training & Evaluation, Model Evaluation | Evaluating vision-language models (VLMs) in scientific domains like mathematics and physics poses unique challenges that go far beyond predicting final answers. These domains demand conceptual understanding, symbolic reasoning, and adherence to formal laws, requirements that most existing benchmarks fail to address. In particular, current datasets tend to be static, lacking intermediate reasoning steps, robustness to variations, or mechanisms for verifying scientific correctness. To address these limitations, we introduce PRiSM, a synthetic, fully dynamic, and multimodal benchmark for evaluating scientific reasoning via grounded Python code. PRiSM includes over 24,750 university-level physics and math problems, and it leverages our scalable agent-based pipeline, PrismAgent, to generate well-structured problem instances. Each problem contains dynamic textual and visual input, a generated figure, alongside rich structured outputs: executable Python code for ground truth generation and verification, and detailed step-by-step reasoning. The dynamic nature and Python-powered automated ground truth generation of our benchmark allow for fine-grained experimental auditing of multimodal VLMs, revealing failure modes, uncertainty behaviors, and limitations in scientific reasoning. To this end, we propose five targeted evaluation tasks covering generalization, symbolic program synthesis, perturbation robustness, reasoning correction, and ambiguity resolution. Through comprehensive evaluation of existing VLMs, we highlight their limitations and showcase how PRiSM enables deeper insights into their scientific reasoning capabilities. | [üîó Paper](https://arxiv.org/abs/2512.05930v1) |
| [LLM Harms: A Taxonomy and Discussion](https://arxiv.org/abs/2512.05929v1) | Kevin Chen, Saleh Afroogh, Abhejay Murali, David Atkinson, Amit Dhurandhar, Junfeng Jiao | 2025-12-05 | Responsible AI, Model Evaluation | This study addresses categories of harm surrounding Large Language Models (LLMs) in the field of artificial intelligence. It addresses five categories of harms addressed before, during, and after development of AI applications: pre-development, direct output, Misuse and Malicious Application, and downstream application. By underscoring the need to define risks of the current landscape to ensure accountability, transparency and navigating bias when adapting LLMs for practical applications. It proposes mitigation strategies and future directions for specific domains and a dynamic auditing system guiding responsible development and integration of LLMs in a standardized proposal. | [üîó Paper](https://arxiv.org/abs/2512.05929v1) |
| [A Comparative Study on Synthetic Facial Data Generation Techniques for Face Recognition](https://arxiv.org/abs/2512.05928v1) | Pedro Vidal, Bernardo Biesseck, Luiz E. L. Coelho, Roger Granada, David Menotti | 2025-12-05 | Responsible AI, Model Evaluation, Diffusion Models | Facial recognition has become a widely used method for authentication and identification, with applications for secure access and locating missing persons. Its success is largely attributed to deep learning, which leverages large datasets and effective loss functions to learn discriminative features. Despite these advances, facial recognition still faces challenges in explainability, demographic bias, privacy, and robustness to aging, pose variations, lighting changes, occlusions, and facial expressions. Privacy regulations have also led to the degradation of several datasets, raising legal, ethical, and privacy concerns. Synthetic facial data generation has been proposed as a promising solution. It mitigates privacy issues, enables experimentation with controlled facial attributes, alleviates demographic bias, and provides supplementary data to improve models trained on real data. This study compares the effectiveness of synthetic facial datasets generated using different techniques in facial recognition tasks. We evaluate accuracy, rank-1, rank-5, and the true positive rate at a false positive rate of 0.01% on eight leading datasets, offering a comparative analysis not extensively explored in the literature. Results demonstrate the ability of synthetic data to capture realistic variations while emphasizing the need for further research to close the performance gap with real data. Techniques such as diffusion models, GANs, and 3D models show substantial progress; however, challenges remain. | [üîó Paper](https://arxiv.org/abs/2512.05928v1) |
| [LDLT $\mathcal{L}$-Lipschitz Network: Generalized Deep End-To-End Lipschitz Network Construction](https://arxiv.org/abs/2512.05915v1) | Marius F. R. Juston, Ramavarapu S. Sreenivas, Dustin Nottage, Ahmet Soylemezoglu | 2025-12-05 | Responsible AI, Model Evaluation | Deep residual networks (ResNets) have demonstrated outstanding success in computer vision tasks, attributed to their ability to maintain gradient flow through deep architectures. Simultaneously, controlling the Lipschitz constant in neural networks has emerged as an essential area of research to enhance adversarial robustness and network certifiability. This paper presents a rigorous approach to the general design of $\mathcal{L}$-Lipschitz deep residual networks using a Linear Matrix Inequality (LMI) framework. Initially, the ResNet architecture was reformulated as a cyclic tridiagonal LMI, and closed-form constraints on network parameters were derived to ensure $\mathcal{L}$-Lipschitz continuity; however, using a new $LDL^\top$ decomposition approach for certifying LMI feasibility, we extend the construction of $\mathcal{L}$-Lipchitz networks to any other nonlinear architecture. Our contributions include a provable parameterization methodology for constructing Lipschitz-constrained residual networks and other hierarchical architectures. Cholesky decomposition is also used for efficient parameterization. These findings enable robust network designs applicable to adversarial robustness, certified training, and control systems. The $LDL^\top$ formulation is shown to be a tight relaxation of the SDP-based network, maintaining full expressiveness and achieving 3\%-13\% accuracy gains over SLL Layers on 121 UCI data sets. | [üîó Paper](https://arxiv.org/abs/2512.05915v1) |
## üîπ Autonomous Agents

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Trusted AI Agents in the Cloud](https://arxiv.org/abs/2512.05951v1) | Teofil Bodea, Masanori Misono, Julian Pritzi, Patrick Sabanic, Thore Sommer, Harshavardhan Unnibhavi, David Schall, Nuno Santos, Dimitrios Stavrakakis, Pramod Bhatotia | 2025-12-05 | Autonomous Agents | AI agents powered by large language models are increasingly deployed as cloud services that autonomously access sensitive data, invoke external tools, and interact with other agents. However, these agents run within a complex multi-party ecosystem, where untrusted components can lead to data leakage, tampering, or unintended behavior. Existing Confidential Virtual Machines (CVMs) provide only per binary protection and offer no guarantees for cross-principal trust, accelerator-level isolation, or supervised agent behavior. We present Omega, a system that enables trusted AI agents by enforcing end-to-end isolation, establishing verifiable trust across all contributing principals, and supervising every external interaction with accountable provenance. Omega builds on Confidential VMs and Confidential GPUs to create a Trusted Agent Platform that hosts many agents within a single CVM using nested isolation. It also provides efficient multi-agent orchestration with cross-principal trust establishment via differential attestation, and a policy specification and enforcement framework that governs data access, tool usage, and inter-agent communication for data protection and regulatory compliance. Implemented on AMD SEV-SNP and NVIDIA H100, Omega fully secures agent state across CVM-GPU, and achieves high performance while enabling high-density, policy-compliant multi-agent deployments at cloud scale. | [üîó Paper](https://arxiv.org/abs/2512.05951v1) |
## üîπ RAG

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Enhancing Retrieval-Augmented Generation with Entity Linking for Educational Platforms](https://arxiv.org/abs/2512.05967v1) | Francesco Granata, Francesco Poggi, Misael Mongiov√¨ | 2025-12-05 | RAG | In the era of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) architectures are gaining significant attention for their ability to ground language generation in reliable knowledge sources. Despite their impressive effectiveness in many areas, RAG systems based solely on semantic similarity often fail to ensure factual accuracy in specialized domains, where terminological ambiguity can affect retrieval relevance. This study proposes an enhanced RAG architecture that integrates a factual signal derived from Entity Linking to improve the accuracy of educational question-answering systems in Italian. The system includes a Wikidata-based Entity Linking module and implements three re-ranking strategies to combine semantic and entity-based information: a hybrid score weighting model, reciprocal rank fusion, and a cross-encoder re-ranker. Experiments were conducted on two benchmarks: a custom academic dataset and the standard SQuAD-it dataset. Results show that, in domain-specific contexts, the hybrid schema based on reciprocal rank fusion significantly outperforms both the baseline and the cross-encoder approach, while the cross-encoder achieves the best results on the general-domain dataset. These findings confirm the presence of an effect of domain mismatch and highlight the importance of domain adaptation and hybrid ranking strategies to enhance factual precision and reliability in retrieval-augmented generation. They also demonstrate the potential of entity-aware RAG systems in educational environments, fostering adaptive and reliable AI-based tutoring tools. | [üîó Paper](https://arxiv.org/abs/2512.05967v1) |
| [MaxShapley: Towards Incentive-compatible Generative Search with Fair Context Attribution](https://arxiv.org/abs/2512.05958v1) | Sara Patel, Mingxun Zhou, Giulia Fanti | 2025-12-05 | RAG | Generative search engines based on large language models (LLMs) are replacing traditional search, fundamentally changing how information providers are compensated. To sustain this ecosystem, we need fair mechanisms to attribute and compensate content providers based on their contributions to generated answers. We introduce MaxShapley, an efficient algorithm for fair attribution in generative search pipelines that use retrieval-augmented generation (RAG). MaxShapley is a special case of the celebrated Shapley value; it leverages a decomposable max-sum utility function to compute attributions with linear computation in the number of documents, as opposed to the exponential cost of Shapley values. We evaluate MaxShapley on three multi-hop QA datasets (HotPotQA, MuSiQUE, MS MARCO); MaxShapley achieves comparable attribution quality to exact Shapley computation, while consuming a fraction of its tokens--for instance, it gives up to an 8x reduction in resource consumption over prior state-of-the-art methods at the same attribution accuracy. | [üîó Paper](https://arxiv.org/abs/2512.05958v1) |
## üîπ General AI

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Group Classification (1+2)-dimensional Linear Equation of Asian Options Pricing](https://arxiv.org/abs/2512.05963v1) | Stanislav V. Spichak, Valeriy I. Stogniy, Inna M. Kopas | 2025-12-05 | General AI | We consider a class of (1+2)-dimensional linear partial differential of Asian options pricing. Special cases have been used to models of financial mathematics. We carry out group classification of a class equations. In particular, the maximum dimension Lie invariance algebra within the above class is eight-dimensional. It is shown that an equation with such an algebra can be transformed into the linear Kolmogorov equation with the help of the point transformations of variables. Using the operators of invariance algebra symmetry reduction is carried out and invariant exact solutions are constructed for some equations. | [üîó Paper](https://arxiv.org/abs/2512.05963v1) |
| [Entanglement-Enhanced Quantum Nano-Vibrometry](https://arxiv.org/abs/2512.05961v1) | Colin P. Lualdi, Joshua Rapp, Spencer J. Johnson, Michael Vayninger, Paul G. Kwiat | 2025-12-05 | General AI | The study of dynamic systems at the nanometer scale can benefit from the loss and background resilience offered by quantum two-photon interference. However, fast measurements with the required resolution are difficult to realize. As a solution, we introduce extreme energy entanglement between the photons undergoing interference. Using a flux probing analysis technique, we recover vibrational signals with frequencies as high as 21 kHz. Along with validating nanometer-scale precision and accuracy, we observe a significant quantum advantage when measuring in the presence of loss and background. | [üîó Paper](https://arxiv.org/abs/2512.05961v1) |
| [Minimal two band model and experimental proposals to distinguish pairing mechanisms of the high-T$_c$ superconductor La$_3$Ni$_2$O$_7$](https://arxiv.org/abs/2512.05956v1) | Zheng-Duo Fan, Ashvin Vishwanath | 2025-12-05 | General AI | The discovery of high-T$_c$ superconductivity in La$_3$Ni$_2$O$_7$ has opened the door to a new route to high temperature superconductivity, distinct from that in cuprates and iron-based materials. Yet, despite intense recent activity, we lack experimentally testable protocols for distinguishing between different pairing scenarios. In this Letter, we construct a minimal two-band model that reproduces the Fermi-surface topology observed in recent ARPES measurements and DFT calculations, and we analyze superconductivity arising from two distinct pairing mechanisms. We show that these mechanisms yield sharply different responses to an applied perpendicular electric field. Thus, La$_3$Ni$_2$O$_7$ offers the unique opportunity to cleanly distinguish between different pairing scenarios. Finally, we propose three concrete experimental proposals designed to distinguish these scenarios and thereby identify the pairing mechanism most relevant to the real material. | [üîó Paper](https://arxiv.org/abs/2512.05956v1) |
| [Correspondence-Oriented Imitation Learning: Flexible Visuomotor Control with 3D Conditioning](https://arxiv.org/abs/2512.05953v1) | Yunhao Cao, Zubin Bhaumik, Jessie Jia, Xingyi He, Kuan Fang | 2025-12-05 | General AI | We introduce Correspondence-Oriented Imitation Learning (COIL), a conditional policy learning framework for visuomotor control with a flexible task representation in 3D. At the core of our approach, each task is defined by the intended motion of keypoints selected on objects in the scene. Instead of assuming a fixed number of keypoints or uniformly spaced time intervals, COIL supports task specifications with variable spatial and temporal granularity, adapting to different user intents and task requirements. To robustly ground this correspondence-oriented task representation into actions, we design a conditional policy with a spatio-temporal attention mechanism that effectively fuses information across multiple input modalities. The policy is trained via a scalable self-supervised pipeline using demonstrations collected in simulation, with correspondence labels automatically generated in hindsight. COIL generalizes across tasks, objects, and motion patterns, achieving superior performance compared to prior methods on real-world manipulation tasks under both sparse and dense specifications. | [üîó Paper](https://arxiv.org/abs/2512.05953v1) |
| [Removing correlated noise stripes from the Nancy Grace Roman Space Telescope survey images](https://arxiv.org/abs/2512.05949v1) | Katherine Laliotis, Christopher M. Hirata, Emily Macbeth, Kaili Cao | 2025-12-05 | General AI | Weak gravitational lensing has emerged as a powerful tool for investigating the matter distribution in the Universe and how it has evolved over cosmic time. The Wide Field Instrument (WFI) on the Nancy Grace Roman Space Telescope (Roman) will deliver some of the highest precision measurements of weak lensing ever made. Since weak lensing is based on statistics of faint sources, it can be biased by even tiny instrument systematics, including correlated read noise. Previous works have shown the infrared detectors used in the Roman WFI show correlations in their noise fields at a level significant for weak lensing measurements, even after application of standard reference pixel corrections; of particular concern is 1/f noise, which appears as horizontal banding in the detector frame. In this paper, we present imDestripe: a new Python module utilizing the multiple roll angles in Roman's observing strategy and linear algebra techniques to remove correlated noise stripes from observed images. We test imDestripe in a hybrid simulation by combining real noise realizations (from darks taken during ground testing) with simulated images of the astronomical scene, and find that the power spectrum of the banding can be suppressed by factors of 10--30 on large scales. We briefly discuss plans for further development of imDestripe in the context of the WFI pipeline. | [üîó Paper](https://arxiv.org/abs/2512.05949v1) |
| [Developing synthetic microdata through machine learning for firm-level business surveys](https://arxiv.org/abs/2512.05948v1) | Jorge Cisneros Paz, Timothy Wojan, Matthew Williams, Jennifer Ozawa, Robert Chew, Kimberly Janda, Timothy Navarro, Michael Floyd, Christine Task, Damon Streat | 2025-12-05 | General AI | Public-use microdata samples (PUMS) from the United States (US) Census Bureau on individuals have been available for decades. However, large increases in computing power and the greater availability of Big Data have dramatically increased the probability of re-identifying anonymized data, potentially violating the pledge of confidentiality given to survey respondents. Data science tools can be used to produce synthetic data that preserve critical moments of the empirical data but do not contain the records of any existing individual respondent or business. Developing public-use firm data from surveys presents unique challenges different from demographic data, because there is a lack of anonymity and certain industries can be easily identified in each geographic area. This paper briefly describes a machine learning model used to construct a synthetic PUMS based on the Annual Business Survey (ABS) and discusses various quality metrics. Although the ABS PUMS is currently being refined and results are confidential, we present two synthetic PUMS developed for the 2007 Survey of Business Owners, similar to the ABS business data. Econometric replication of a high impact analysis published in Small Business Economics demonstrates the verisimilitude of the synthetic data to the true data and motivates discussion of possible ABS use cases. | [üîó Paper](https://arxiv.org/abs/2512.05948v1) |
| [On cable-graph percolation between dimensions 2 and 3](https://arxiv.org/abs/2512.05947v1) | Pierre-Fran√ßois Rodriguez, Wen Zhang | 2025-12-05 | General AI | We consider the Gaussian free field on two-dimensional slabs with a thickness described by a height $h$ at spatial scale $N$. We investigate the radius of critical clusters for the associated cable-graph percolation problem, which depends sensitively on the parameter $h$. Our results unveil a whole family of new "fixed points", which interpolate between recent results from arXiv:2303.03782 in two dimensions and from arXiv:2405.17417 and arXiv:2406.02397 in three dimensions, and describe critical behaviour beyond those regimes. In the delocalised phase, the one-arm decay exhibits a "plateau", i.e. it doesn't depend on the speed at which the variance of the field diverges in the large-$N$ limit. Our methods rely on a careful analysis of the interplay between two- and three-dimensional effects for the underlying random walk, which manifest themselves in a corresponding decomposition of the field. | [üîó Paper](https://arxiv.org/abs/2512.05947v1) |
| [Constraining r-process nucleosynthesis via enhanced accuracy neutron-capture experiments](https://arxiv.org/abs/2512.05944v1) | C. Domingo-Pardo, C. Lederer-Woods, A. Mengoni | 2025-12-05 | General AI | The isotopic abundances of r-process elements in the solar system are traditionally derived as residuals from the subtraction of s-process contributions from total solar abundances. However, the uncertainties in s-process nucleosynthesis -particularly those arising from Maxwellian Averaged Cross Sections (MACS)- propagate directly into the r-process residuals, affecting their reliability. Building upon the seminal work of Goriely (1999), who introduced a multi-event s-process model to quantify these uncertainties, we revisit the problem using a simplified yet effective approach. By assuming that the relative uncertainty in s-process isotopic abundances scales linearly with the MACS uncertainties from data libraries (KADoNiS), we identify a subset of isotopes for which the r-process residuals remain significantly uncertain. Using updated solar abundances (Lodders 2025) and s-process contributions from Bisterzo et al. (2014), we present a short list of isotopes that are prime candidates for improved (n,g) measurements at CERN n_TOF in the near future. Our analysis provides a practical framework for prioritizing future experimental efforts that will profit from upgrades and enhancements of the n_TOF facility. It also highlights the need to revisit key neutron-capture cross sections to refine our understanding of the r-process isotopic abundance pattern, commonly used as a benchmark in stellar models of explosive nucleosynthesis. | [üîó Paper](https://arxiv.org/abs/2512.05944v1) |
| [A poset representation for stable contracts in a two-sided market generated by integer choice functions](https://arxiv.org/abs/2512.05942v1) | Alexander V. Karzanov | 2025-12-05 | General AI | Generalizing a variety of earlier problems on stable contracts in two-sided markets, Alkan and Gale introduced in 2003 a general stability model on a bipartite graph $G=(V,E)$ in which the vertices are interpreted as ``agents'', and the edges as possible ``contract'' between pairs of ``agents''. The edges are endowed with nonnegative capacities $b$ giving upper bounds on ``contract intensities'', and the preferencies of each ``agent'' $v\in V$ depend on a \emph{choice function} (CFs) that acts on the set of ``contracts'' involving $v$, obeying three well motivated axioms of \it{consistence}, \it{substitutability} and \it{cardinal monotonicity}. In their model, the capacities and choice functions can take reals or discrete values and, extending well-known earlier results on particular cases, they proved that systems of \it{stable} contracts always exist and, moreover, their set $\cal S$ constitutes a distributive lattice under a natural comparison relation $\prec$.   In this paper, we study Alkan--Gale's model when all capacities and choice functions take integer values. We characterize the set of rotations -- augmenting cycles linking neighboring stable assignments in the lattice $(\cal S,\prec)$, and construct a weighted poset in which the lattice the closed functions is isomorphic to $(\cal S,\prec)$, thus obtaining an explicit representation for the latter. We show that in general the size of the poset is at most $b^{\rm max} E $, where $b^{\rm max}$ is the maximal capacity, and the poset can be constructed in pseudo polynomial time. Then we explain that by imposing an additional condition on CFs, the size of the poset becomes polynomial in $ V $, and the total time reduces to a polynomial in $ V ,\log b^{\rm max}$. | [üîó Paper](https://arxiv.org/abs/2512.05942v1) |
| [Measuring the Effect of Background on Classification and Feature Importance in Deep Learning for AV Perception](https://arxiv.org/abs/2512.05937v1) | Anne Sielemann, Valentin Barner, Stefan Wolf, Masoud Roschani, Jens Ziehn, Juergen Beyerer | 2025-12-05 | General AI | Common approaches to explainable AI (XAI) for deep learning focus on analyzing the importance of input features on the classification task in a given model: saliency methods like SHAP and GradCAM are used to measure the impact of spatial regions of the input image on the classification result. Combined with ground truth information about the location of the object in the input image (e.g., a binary mask), it is determined whether object pixels had a high impact on the classification result, or whether the classification focused on background pixels. The former is considered to be a sign of a healthy classifier, whereas the latter is assumed to suggest overfitting on spurious correlations. A major challenge, however, is that these intuitive interpretations are difficult to test quantitatively, and hence the output of such explanations lacks an explanation itself. One particular reason is that correlations in real-world data are difficult to avoid, and whether they are spurious or legitimate is debatable. Synthetic data in turn can facilitate to actively enable or disable correlations where desired but often lack a sufficient quantification of realism and stochastic properties. [...] Therefore, we systematically generate six synthetic datasets for the task of traffic sign recognition, which differ only in their degree of camera variation and background correlation [...] to quantify the isolated influence of background correlation, different levels of camera variation, and considered traffic sign shapes on the classification performance, as well as background feature importance. [...] Results include a quantification of when and how much background features gain importance to support the classification task based on changes in the training domain [...].   Download: synset.de/datasets/synset-signset-ger/background-effect | [üîó Paper](https://arxiv.org/abs/2512.05937v1) |
| [Transformation of orientation and rotation angles of synchronous satellites: Application to the Galilean moons](https://arxiv.org/abs/2512.05935v1) | Marie Yseboodt, Rose-Marie Baland | 2025-12-05 | General AI | The orientation and rotation of a synchronous satellite can be referred to both its Laplace plane and the ICRF equatorial plane, in terms of Euler angles or spin axis Cartesian coordinates and Earth equatorial coordinates, respectively. We computed second-order analytical expressions to make the transformation between the two systems and applied them to the Galilean satellites (Io, Europa, Ganymede, and Callisto). If one term of the spin axis Cartesian coordinates series is dominant, trigonometric series can be generated for the inertial and orbital obliquities, node longitude and offset with respect to the Cassini plane. Since the transformation does not require any fit of amplitudes and frequencies on numerical series, the physical meaning of the frequencies is preserved from the input series and the amplitudes can be directly related to the geophysical parameters of interest. We provide tables for the coordinates and angles' series assuming that the satellites are entirely solid, and considering two different orbital theories. The possible amplitude ranges for the main terms are also examined in the case where a liquid layer is assumed in the interior model. We use our transformation method to propose an updated IAU WG solution which would result in an improvement with respect to zero obliquity models used so far. This method will also be useful for the interpretation of future Earth-based radar observations or JUICE data. | [üîó Paper](https://arxiv.org/abs/2512.05935v1) |
| [Spectroscopy and Coherent Control of Two-Level System Defect Ensembles Using a Broadband 3D Waveguide](https://arxiv.org/abs/2512.05934v1) | Qianxu Wang, Juan S. Salcedo-Gallo, Salil Bedkihal, Tian Xia, Maciej W. Olszewski, Valla Fatemi, Mattias Fitzpatrick | 2025-12-05 | General AI | Defects in solid-state materials play a central role in determining coherence, stability, and performance in quantum technologies. Although narrowband techniques can probe specific resonances with high precision, a broadband spectroscopic approach captures the full spectrum of defect properties and dynamics. Two-level system (TLS) defects in amorphous dielectrics are a particularly important example because they are major sources of decoherence and energy loss in superconducting quantum devices. However, accessing and characterizing their collective dynamics remains far more challenging than probing individual TLS defects. Building on our previously developed Broadband Cryogenic Transient Dielectric Spectroscopy (BCTDS) technique, we study the coherent control and time-resolved dynamics of TLS defect ensembles over a wide frequency range of 3-5 GHz without requiring full device fabrication, revealing quantum interference effects, memory-dependent dynamics, and dressed-state evolution within the TLS defect bath. The spectral response reveals distinct V-shaped structures corresponding to the bare eigenmode frequencies. Using these features, we extract a TLS defect spectral density of 84 GHz^-1 for a silicon sample, across a 4.1-4.6 GHz span. Furthermore, we systematically investigate amplitude- and phase-controlled interference fringes for multiple temperatures and inter-pulse delays, providing direct evidence of coherent dynamics and control. A driven minimal spin model with dipole-dipole interactions that qualitatively capture the observed behavior is presented. Our results establish BCTDS as a versatile platform for broadband defect spectroscopy, offering new capabilities for diagnosing and mitigating sources of decoherence, engineering many-body dynamics, and exploring non-equilibrium phenomena in disordered quantum systems. | [üîó Paper](https://arxiv.org/abs/2512.05934v1) |
| [Physically-Based Simulation of Automotive LiDAR](https://arxiv.org/abs/2512.05932v1) | L. Dudzik, M. Roschani, A. Sielemann, K. Trampert, J. Ziehn, J. Beyerer, C. Neumann | 2025-12-05 | General AI | We present an analytic model for simulating automotive time-of-flight (ToF) LiDAR that includes blooming, echo pulse width, and ambient light, along with steps to determine model parameters systematically through optical laboratory measurements. The model uses physically based rendering (PBR) in the near-infrared domain. It assumes single-bounce reflections and retroreflections over rasterized rendered images from shading or ray tracing, including light emitted from the sensor as well as stray light from other, non-correlated sources such as sunlight. Beams from the sensor and sensitivity of the receiving diodes are modeled with flexible beam steering patterns and with non-vanishing diameter.   Different (all non-real time) computational approaches can be chosen based on system properties, computing capabilities, and desired output properties.   Model parameters include system-specific properties, namely the physical spread of the LiDAR beam, combined with the sensitivity of the receiving diode; the intensity of the emitted light; the conversion between the intensity of reflected light and the echo pulse width; and scenario parameters such as environment lighting, positioning, and surface properties of the target(s) in the relevant infrared domain. System-specific properties of the model are determined from laboratory measurements of the photometric luminance on different target surfaces aligned with a goniometer at 0.01¬∞ resolution, which marks the best available resolution for measuring the beam pattern.   The approach is calibrated for and tested on two automotive LiDAR systems, the Valeo Scala Gen. 2 and the Blickfeld Cube 1. Both systems differ notably in their properties and available interfaces, but the relevant model parameters could be extracted successfully. | [üîó Paper](https://arxiv.org/abs/2512.05932v1) |
| [BalLOT: Balanced $k$-means clustering with optimal transport](https://arxiv.org/abs/2512.05926v1) | Wenyan Luo, Dustin G. Mixon | 2025-12-05 | General AI | We consider the fundamental problem of balanced $k$-means clustering. In particular, we introduce an optimal transport approach to alternating minimization called BalLOT, and we show that it delivers a fast and effective solution to this problem. We establish this with a variety of numerical experiments before proving several theoretical guarantees. First, we prove that for generic data, BalLOT produces integral couplings at each step. Next, we perform a landscape analysis to provide theoretical guarantees for both exact and partial recoveries of planted clusters under the stochastic ball model. Finally, we propose initialization schemes that achieve one-step recovery of planted clusters. | [üîó Paper](https://arxiv.org/abs/2512.05926v1) |
| [Untangling the IBP Equations](https://arxiv.org/abs/2512.05923v1) | Junhan W. Liu, Alexander Mitov | 2025-12-05 | General AI | In this work, we present an algorithm for the diagonalization of the Integration-by-Parts (IBP) equations. Diagonalized IBP equations are indispensable for reducing loop integrals with high numerator powers to master integrals and for solving IBP identities in closed analytic form. A prime example is provided by multivariate Mellin representations of loop amplitudes and cross sections. The extension of these methods to other multivariate recurrence relations is also discussed. As a by-product of our diagonalization procedure, we show how the IBP equations can be cast into an efficient, fully triangular form that is well suited for computer implementation. Several complicated topologies have been computed. | [üîó Paper](https://arxiv.org/abs/2512.05923v1) |
| [LPD: Learnable Prototypes with Diversity Regularization for Weakly Supervised Histopathology Segmentation](https://arxiv.org/abs/2512.05922v1) | Khang Le, Anh Mai Vu, Thi Kim Trang Vo, Ha Thach, Ngoc Bui Lam Quang, Thanh-Huy Nguyen, Minh H. N. Le, Zhu Han, Chandra Mohan, Hien Van Nguyen | 2025-12-05 | General AI | Weakly supervised semantic segmentation (WSSS) in histopathology reduces pixel-level labeling by learning from image-level labels, but it is hindered by inter-class homogeneity, intra-class heterogeneity, and CAM-induced region shrinkage (global pooling-based class activation maps whose activations highlight only the most distinctive areas and miss nearby class regions). Recent works address these challenges by constructing a clustering prototype bank and then refining masks in a separate stage; however, such two-stage pipelines are costly, sensitive to hyperparameters, and decouple prototype discovery from segmentation learning, limiting their effectiveness and efficiency. We propose a cluster-free, one-stage learnable-prototype framework with diversity regularization to enhance morphological intra-class heterogeneity coverage. Our approach achieves state-of-the-art (SOTA) performance on BCSS-WSSS, outperforming prior methods in mIoU and mDice. Qualitative segmentation maps show sharper boundaries and fewer mislabels, and activation heatmaps further reveal that, compared with clustering-based prototypes, our learnable prototypes cover more diverse and complementary regions within each class, providing consistent qualitative evidence for their effectiveness. | [üîó Paper](https://arxiv.org/abs/2512.05922v1) |
| [Thermodynamics of Shear Equilibration During Magnetic Reconnection Onset in Mixed-Equilibrium Current Sheets](https://arxiv.org/abs/2512.05921v1) | Dominic Payne, Marc Swisdak, James Drake, Tak Chu Li | 2025-12-05 | General AI | Magnetic shear across the polarity inversion line (PIL) plays an important role in the explosive nature of reconnection onset and in the equilibration of current sheets, acting as a source of free energy that can enhance or inhibit the onset process under certain conditions. In this study, we use a 2D PIC simulation to examine the local interaction between the reconnection guide field and thermodynamic variables during reconnection onset in a region of initially depleted thermal energy and enhanced magnetic energy in a large guide field background. We identify critical stages of the equilibration process, characterize intervals based on whether the pressure evolution is driven by changes in density or temperature, and discuss what these intervals imply about the evolution of local heat and work density. Finally, we examine power densities associated with electromagnetic field time evolution and electromagnetic energy transfer and compare to those related to thermodynamic changes. | [üîó Paper](https://arxiv.org/abs/2512.05921v1) |
| [NICE: Neural Implicit Craniofacial Model for Orthognathic Surgery Prediction](https://arxiv.org/abs/2512.05920v1) | Jiawen Yang, Yihui Cao, Xuanyu Tian, Yuyao Zhang, Hongjiang Wei | 2025-12-05 | General AI | Orthognathic surgery is a crucial intervention for correcting dentofacial skeletal deformities to enhance occlusal functionality and facial aesthetics. Accurate postoperative facial appearance prediction remains challenging due to the complex nonlinear interactions between skeletal movements and facial soft tissue. Existing biomechanical, parametric models and deep-learning approaches either lack computational efficiency or fail to fully capture these intricate interactions. To address these limitations, we propose Neural Implicit Craniofacial Model (NICE) which employs implicit neural representations for accurate anatomical reconstruction and surgical outcome prediction. NICE comprises a shape module, which employs region-specific implicit Signed Distance Function (SDF) decoders to reconstruct the facial surface, maxilla, and mandible, and a surgery module, which employs region-specific deformation decoders. These deformation decoders are driven by a shared surgical latent code to effectively model the complex, nonlinear biomechanical response of the facial surface to skeletal movements, incorporating anatomical prior knowledge. The deformation decoders output point-wise displacement fields, enabling precise modeling of surgical outcomes. Extensive experiments demonstrate that NICE outperforms current state-of-the-art methods, notably improving prediction accuracy in critical facial regions such as lips and chin, while robustly preserving anatomical integrity. This work provides a clinically viable tool for enhanced surgical planning and patient consultation in orthognathic procedures. | [üîó Paper](https://arxiv.org/abs/2512.05920v1) |
| [Magnetic and moir√© Proximity Effects in WSe2/WSe2/CrI3 Trilayers](https://arxiv.org/abs/2512.05917v1) | Junyi Liu, Xu Zhang, Gang Lu | 2025-12-05 | General AI | Integrating magnetic order to moir√© superlattices is of significant scientific and technological interest. Based on first-principles calculations, we study the interplay of magnetic proximity and moir√© proximity in WSe2/WSe2/CrI3 trilayers with different stackings and twist angles. Large valley splitting is observed due to redistribution of the exciton charge density across layers via a super-exchange-like mechanism, and its electric-field dependence bears similarity to electrically tunable and valley-selective Feshbach resonances. The valley splitting can be magnified in moir√© superlattices owing to the superposition of Umklapp excitons folded from moir√© minibands, yielding spatially modulated and enhanced magnetic proximity. The moir√© proximity effect is demonstrated via an imprinted moir√© potential on CrI3 layer and its feedback to the direct moir√© potential on WSe2 bilayers is observed. The cooperation between the direct and imprinted moir√© potentials is shown to yield novel topological and correlated states. | [üîó Paper](https://arxiv.org/abs/2512.05917v1) |
| [ALMAGAL VI. The spatial distribution of dense cores during the evolution of cluster-forming massive clump](https://arxiv.org/abs/2512.05914v1) | E. Schisano, S. Molinari, A. Coletta, D. Elia, P. Schilke, A. Traficante, √Å. Sanchez-Monge, H. Beuther, M. Benedettini, C. Mininni, R. S. Klessen, J. D. Soler, A. Nucara, S. Pezzuto, F. van der Tak, P. Hennebelle, M. T. Beltr√°n, L. Moscadelli, K. L. J. Rygl, P. Sanhueza, P. M. Koch, D. C. Lis, R. Kuiper, G. A. Fuller, A. Avison, L. Bronfman, U. Lebreuilly, T. M√∂ller, T. Liu, V. -M. Pelkonen, L. Testi, Q. Zhang, T. Zhang, A. Ahmadi, J. Allande, C. Battersby, J. Wallace, C. L. Brogan, S. Clarke, F. De Angelis, F. Fontani, P. T. P. Ho, T. R. Hunter, B. Jones, K. G. Johnston, P. D. Klaassen, S. J. Liu, S. -Y. Liu, Y. Maruccia, A. J. Rigby, Y. -N. Su, Y. Tang, S. Walch, H. Zinnecker | 2025-12-05 | General AI | High-mass stars and star clusters form from the fragmentation of massive dense clumps driven by gravity, turbulence, and magnetic fields. The ALMAGAL project observed $\sim1000$ clumps at $\sim$1000\,au resolution, enabling a statistically significant characterization of this process across a large range of clump physical parameters and evolutionary stages. In this work, we investigated the spatial distribution of dense cores in the 514 massive, potentially cluster-forming, clumps hosting at least 4 cores, to trace fragmentation's initial conditions and early evolution. We used quantitative descriptors, evaluated against the clump bolometric luminosity-to-mass ratio as an indicator of evolution. Core separations were measured with the minimum spanning tree method (MST) and compared with the Jeans gravitational fragmentation theory. We used the $Q$ parameter and the mass segregation ratio, $Œõ_{MSR}$, to evaluate whether cores have specific arrangements or differences depending on their masses. ALMAGAL cores are usually arranged in elliptical groups with an axis ratio $e\sim2.2$, but $e\geq$5 is also observed. A single characteristic core separation per clump is found in $\sim76$% of cases, but signatures of multiple fragmentation lengths not rare. Typical core separations are compatible with the clump-averaged thermal Jeans length, $Œª^{th}_{J}$, though a population, typical of low-fragmented/young clumps, has wider separations with $l\approx3\timesŒª^{th}_{J}$. The core separation decreases on average from $l\sim22000$ au in younger systems to $l\sim7000$ au in more evolved ones. Cores are typically distributed in fractal-type subclusters, with centrally concentrated patterns appearing only at later stages, but without a progressive evolutionary transition. Finally, mass segregation is found in 110 systems, with its occurrence increasing with evolution. | [üîó Paper](https://arxiv.org/abs/2512.05914v1) |
