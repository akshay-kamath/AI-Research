# üìå AI Research Papers (December08 to December14)

## üîπ LLM

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Particulate: Feed-Forward 3D Object Articulation](https://arxiv.org/abs/2512.11798v1) | Ruining Li, Yuxin Yao, Chuanxia Zheng, Christian Rupprecht, Joan Lasenby, Shangzhe Wu, Andrea Vedaldi | 2025-12-12 | LLM, Training & Evaluation, Optimization | We present Particulate, a feed-forward approach that, given a single static 3D mesh of an everyday object, directly infers all attributes of the underlying articulated structure, including its 3D parts, kinematic structure, and motion constraints. At its core is a transformer network, Part Articulation Transformer, which processes a point cloud of the input mesh using a flexible and scalable architecture to predict all the aforementioned attributes with native multi-joint support. We train the network end-to-end on a diverse collection of articulated 3D assets from public datasets. During inference, Particulate lifts the network's feed-forward prediction to the input mesh, yielding a fully articulated 3D model in seconds, much faster than prior approaches that require per-object optimization. Particulate can also accurately infer the articulated structure of AI-generated 3D assets, enabling full-fledged extraction of articulated 3D objects from a single (real or synthetic) image when combined with an off-the-shelf image-to-3D generator. We further introduce a new challenging benchmark for 3D articulation estimation curated from high-quality public 3D assets, and redesign the evaluation protocol to be more consistent with human preferences. Quantitative and qualitative results show that Particulate significantly outperforms state-of-the-art approaches. | [üîó Paper](https://arxiv.org/abs/2512.11798v1) |
| [Uncertainty-Aware Domain Adaptation for Vitiligo Segmentation in Clinical Photographs](https://arxiv.org/abs/2512.11791v1) | Wentao Jiang, Vamsi Varra, Caitlin Perez-Stable, Harrison Zhu, Meredith Apicella, Nicole Nyamongo | 2025-12-12 | LLM | Accurately quantifying vitiligo extent in routine clinical photographs is crucial for longitudinal monitoring of treatment response. We propose a trustworthy, frequency-aware segmentation framework built on three synergistic pillars: (1) a data-efficient training strategy combining domain-adaptive pre-training on the ISIC 2019 dataset with an ROI-constrained dual-task loss to suppress background noise; (2) an architectural refinement via a ConvNeXt V2-based encoder enhanced with a novel High-Frequency Spectral Gating (HFSG) module and stem-skip connections to capture subtle textures; and (3) a clinical trust mechanism employing K-fold ensemble and Test-Time Augmentation (TTA) to generate pixel-wise uncertainty maps. Extensive validation on an expert-annotated clinical cohort demonstrates superior performance, achieving a Dice score of 85.05% and significantly reducing boundary error (95% Hausdorff Distance improved from 44.79 px to 29.95 px), consistently outperforming strong CNN (ResNet-50 and UNet++) and Transformer (MiT-B5) baselines. Notably, our framework demonstrates high reliability with zero catastrophic failures and provides interpretable entropy maps to identify ambiguous regions for clinician review. Our approach suggests that the proposed framework establishes a robust and reliable standard for automated vitiligo assessment. | [üîó Paper](https://arxiv.org/abs/2512.11791v1) |
| [Softmax as Linear Attention in the Large-Prompt Regime: a Measure-based Perspective](https://arxiv.org/abs/2512.11784v1) | Etienne Boursier, Claire Boyer | 2025-12-12 | LLM, Training & Evaluation, Optimization | Softmax attention is a central component of transformer architectures, yet its nonlinear structure poses significant challenges for theoretical analysis. We develop a unified, measure-based framework for studying single-layer softmax attention under both finite and infinite prompts. For i.i.d. Gaussian inputs, we lean on the fact that the softmax operator converges in the infinite-prompt limit to a linear operator acting on the underlying input-token measure. Building on this insight, we establish non-asymptotic concentration bounds for the output and gradient of softmax attention, quantifying how rapidly the finite-prompt model approaches its infinite-prompt counterpart, and prove that this concentration remains stable along the entire training trajectory in general in-context learning settings with sub-Gaussian tokens. In the case of in-context linear regression, we use the tractable infinite-prompt dynamics to analyze training at finite prompt length. Our results allow optimization analyses developed for linear attention to transfer directly to softmax attention when prompts are sufficiently long, showing that large-prompt softmax attention inherits the analytical structure of its linear counterpart. This, in turn, provides a principled and broadly applicable toolkit for studying the training dynamics and statistical behavior of softmax attention layers in large prompt regimes. | [üîó Paper](https://arxiv.org/abs/2512.11784v1) |
| [SUMFORU: An LLM-Based Review Summarization Framework for Personalized Purchase Decision Support](https://arxiv.org/abs/2512.11755v1) | Yuming Feng, Xinrui Jiang | 2025-12-12 | LLM, Training & Evaluation, Fine-Tuning, RLHF, Optimization | Online product reviews contain rich but noisy signals that overwhelm users and hinder effective decision-making. Existing LLM-based summarizers remain generic and fail to account for individual preferences, limiting their practical utility. We propose SUMFORU, a steerable review summarization framework that aligns outputs with explicit user personas to support personalized purchase decisions. Our approach integrates a high-quality data pipeline built from the Amazon 2023 Review Dataset with a two-stage alignment procedure: (1) persona-aware Supervised Fine-Tuning (SFT) via asymmetric knowledge distillation, and (2) Reinforcement Learning with AI Feedback (RLAIF) using a preference estimator to capture fine-grained, persona-relevant signals. We evaluate the model across rule-based, LLM-based, and human-centered metrics, demonstrating consistent improvements in consistency, grounding, and preference alignment. Our framework achieves the highest performance across all evaluation settings and generalizes effectively to unseen product categories. Our results highlight the promise of steerable pluralistic alignment for building next-generation personalized decision-support systems. | [üîó Paper](https://arxiv.org/abs/2512.11755v1) |
## üîπ Diffusion Models

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Reducing Domain Gap with Diffusion-Based Domain Adaptation for Cell Counting](https://arxiv.org/abs/2512.11763v1) | Mohammad Dehghanmanshadi, Wallapak Tavanapong | 2025-12-12 | Diffusion Models, Optimization | Generating realistic synthetic microscopy images is critical for training deep learning models in label-scarce environments, such as cell counting with many cells per image. However, traditional domain adaptation methods often struggle to bridge the domain gap when synthetic images lack the complex textures and visual patterns of real samples. In this work, we adapt the Inversion-Based Style Transfer (InST) framework originally designed for artistic style transfer to biomedical microscopy images. Our method combines latent-space Adaptive Instance Normalization with stochastic inversion in a diffusion model to transfer the style from real fluorescence microscopy images to synthetic ones, while weakly preserving content structure.   We evaluate the effectiveness of our InST-based synthetic dataset for downstream cell counting by pre-training and fine-tuning EfficientNet-B0 models on various data sources, including real data, hard-coded synthetic data, and the public Cell200-s dataset. Models trained with our InST-synthesized images achieve up to 37\% lower Mean Absolute Error (MAE) compared to models trained on hard-coded synthetic data, and a 52\% reduction in MAE compared to models trained on Cell200-s (from 53.70 to 25.95 MAE). Notably, our approach also outperforms models trained on real data alone (25.95 vs. 27.74 MAE). Further improvements are achieved when combining InST-synthesized data with lightweight domain adaptation techniques such as DACS with CutMix. These findings demonstrate that InST-based style transfer most effectively reduces the domain gap between synthetic and real microscopy data. Our approach offers a scalable path for enhancing cell counting performance while minimizing manual labeling effort. The source code and resources are publicly available at: https://github.com/MohammadDehghan/InST-Microscopy. | [üîó Paper](https://arxiv.org/abs/2512.11763v1) |
## üîπ RLHF

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Agile Flight Emerges from Multi-Agent Competitive Racing](https://arxiv.org/abs/2512.11781v1) | Vineet Pasumarti, Lorenzo Bianchi, Antonio Loquercio | 2025-12-12 | RLHF | Through multi-agent competition and the sparse high-level objective of winning a race, we find that both agile flight (e.g., high-speed motion pushing the platform to its physical limits) and strategy (e.g., overtaking or blocking) emerge from agents trained with reinforcement learning. We provide evidence in both simulation and the real world that this approach outperforms the common paradigm of training agents in isolation with rewards that prescribe behavior, e.g., progress on the raceline, in particular when the complexity of the environment increases, e.g., in the presence of obstacles. Moreover, we find that multi-agent competition yields policies that transfer more reliably to the real world than policies trained with a single-agent progress-based reward, despite the two methods using the same simulation environment, randomization strategy, and hardware. In addition to improved sim-to-real transfer, the multi-agent policies also exhibit some degree of generalization to opponents unseen at training time. Overall, our work, following in the tradition of multi-agent competitive game-play in digital domains, shows that sparse task-level rewards are sufficient for training agents capable of advanced low-level control in the physical world.   Code: https://github.com/Jirl-upenn/AgileFlight_MultiAgent | [üîó Paper](https://arxiv.org/abs/2512.11781v1) |
| [Covariate-assisted graph matching](https://arxiv.org/abs/2512.11761v1) | Trisha Dawn, Jes√∫s Arroyo | 2025-12-12 | RLHF | Data integration is essential across diverse domains, from historical records to biomedical research, facilitating joint statistical inference. A crucial initial step in this process involves merging multiple data sources based on matching individual records, often in the absence of unique identifiers. When the datasets are networks, this problem is typically addressed through graph matching methodologies. For such cases, auxiliary features or covariates associated with nodes or edges can be instrumental in achieving improved accuracy. However, most existing graph matching techniques do not incorporate this information, limiting their performance against non-identifiable and erroneous matches. To overcome these limitations, we propose two novel covariate-assisted seeded graph matching methods, where a partial alignment for a set of nodes, called seeds, is known. The first one solves a quadratic assignment problem (QAP) over the whole graph, while the second one only leverages the local neighborhood structure of seed nodes for computational scalability. Both methods are grounded in a conditional modeling framework, where elements of one graph's adjacency matrix are modeled using a generalized linear model (GLM), given the other graph and the available covariates. We establish theoretical guarantees for model estimation error and exact recovery of the solution of the QAP. The effectiveness of our methods is demonstrated through numerical experiments and in an application to matching the statistics academic genealogy and the collaboration networks. By leveraging additional covariates, we achieve improved alignment accuracy. Our work highlights the power of integrating covariate information in the classical graph matching setup, offering a practical and improved framework for combining network data with wide-ranging applications. | [üîó Paper](https://arxiv.org/abs/2512.11761v1) |
## üîπ Multimodal AI

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties](https://arxiv.org/abs/2512.11799v1) | Ye Fang, Tong Wu, Valentin Deschaintre, Duygu Ceylan, Iliyan Georgiev, Chun-Hao Paul Huang, Yiwei Hu, Xuelin Chen, Tuanfeng Yang Wang | 2025-12-12 | Multimodal AI | Large-scale video generation models have shown remarkable potential in modeling photorealistic appearance and lighting interactions in real-world scenes. However, a closed-loop framework that jointly understands intrinsic scene properties (e.g., albedo, normal, material, and irradiance), leverages them for video synthesis, and supports editable intrinsic representations remains unexplored. We present V-RGBX, the first end-to-end framework for intrinsic-aware video editing. V-RGBX unifies three key capabilities: (1) video inverse rendering into intrinsic channels, (2) photorealistic video synthesis from these intrinsic representations, and (3) keyframe-based video editing conditioned on intrinsic channels. At the core of V-RGBX is an interleaved conditioning mechanism that enables intuitive, physically grounded video editing through user-selected keyframes, supporting flexible manipulation of any intrinsic modality. Extensive qualitative and quantitative results show that V-RGBX produces temporally consistent, photorealistic videos while propagating keyframe edits across sequences in a physically plausible manner. We demonstrate its effectiveness in diverse applications, including object appearance editing and scene-level relighting, surpassing the performance of prior methods. | [üîó Paper](https://arxiv.org/abs/2512.11799v1) |
| [AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis](https://arxiv.org/abs/2512.11797v1) | Junjie Ye, Rong Xue, Basile Van Hoorick, Pavel Tokmakov, Muhammad Zubair Irshad, Yue Wang, Vitor Guizilini | 2025-12-12 | Multimodal AI, Model Evaluation, Diffusion Models, Scaling Laws | The collection of large-scale and diverse robot demonstrations remains a major bottleneck for imitation learning, as real-world data acquisition is costly and simulators offer limited diversity and fidelity with pronounced sim-to-real gaps. While generative models present an attractive solution, existing methods often alter only visual appearances without creating new behaviors, or suffer from embodiment inconsistencies that yield implausible motions. To address these limitations, we introduce AnchorDream, an embodiment-aware world model that repurposes pretrained video diffusion models for robot data synthesis. AnchorDream conditions the diffusion process on robot motion renderings, anchoring the embodiment to prevent hallucination while synthesizing objects and environments consistent with the robot's kinematics. Starting from only a handful of human teleoperation demonstrations, our method scales them into large, diverse, high-quality datasets without requiring explicit environment modeling. Experiments show that the generated data leads to consistent improvements in downstream policy learning, with relative gains of 36.4% in simulator benchmarks and nearly double performance in real-world studies. These results suggest that grounding generative world models in robot motion provides a practical path toward scaling imitation learning. | [üîó Paper](https://arxiv.org/abs/2512.11797v1) |
| [Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation](https://arxiv.org/abs/2512.11792v1) | Yang Fei, George Stoica, Jingyuan Liu, Qifeng Chen, Ranjay Krishna, Xiaojuan Wang, Benlin Liu | 2025-12-12 | Multimodal AI, Scaling Laws, Diffusion Models, Optimization | Reality is a dance between rigid constraints and deformable structures. For video models, that means generating motion that preserves fidelity as well as structure. Despite progress in diffusion models, producing realistic structure-preserving motion remains challenging, especially for articulated and deformable objects such as humans and animals. Scaling training data alone, so far, has failed to resolve physically implausible transitions. Existing approaches rely on conditioning with noisy motion representations, such as optical flow or skeletons extracted using an external imperfect model. To address these challenges, we introduce an algorithm to distill structure-preserving motion priors from an autoregressive video tracking model (SAM2) into a bidirectional video diffusion model (CogVideoX). With our method, we train SAM2VideoX, which contains two innovations: (1) a bidirectional feature fusion module that extracts global structure-preserving motion priors from a recurrent model like SAM2; (2) a Local Gram Flow loss that aligns how local features move together. Experiments on VBench and in human studies show that SAM2VideoX delivers consistent gains (+2.60\% on VBench, 21-22\% lower FVD, and 71.4\% human preference) over prior baselines. Specifically, on VBench, we achieve 95.51\%, surpassing REPA (92.91\%) by 2.60\%, and reduce FVD to 360.57, a 21.20\% and 22.46\% improvement over REPA- and LoRA-finetuning, respectively. The project website can be found at https://sam2videox.github.io/ . | [üîó Paper](https://arxiv.org/abs/2512.11792v1) |
| [Toward a Decision Support System for Energy-Efficient Ferry Operation on Lake Constance based on Optimal Control](https://arxiv.org/abs/2512.11786v1) | Hannes Homburger, Bastian J√§ckl, Stefan Wirtensohn, Christian Stopp, Maximilian T. Fischer, Moritz Diehl, Daniel A. Keim, Johannes Reuter | 2025-12-12 | Multimodal AI | The maritime sector is undergoing a disruptive technological change driven by three main factors: autonomy, decarbonization, and digital transformation. Addressing these factors necessitates a reassessment of inland vessel operations. This paper presents the design and development of a decision support system for ferry operations based on a shrinking-horizon optimal control framework. The problem formulation incorporates a mathematical model of the ferry's dynamics and environmental disturbances, specifically water currents and wind, which can significantly influence the dynamics. Real-world data and illustrative scenarios demonstrate the potential of the proposed system to effectively support ferry crews by providing real-time guidance. This enables enhanced operational efficiency while maintaining predefined maneuver durations. The findings suggest that optimal control applications hold substantial promise for advancing future ferry operations on inland waters. A video of the real-world ferry MS Insel Mainau operating on Lake Constance is available at: https://youtu.be/i1MjCdbEQyE | [üîó Paper](https://arxiv.org/abs/2512.11786v1) |
| [MatAnyone 2: Scaling Video Matting via a Learned Quality Evaluator](https://arxiv.org/abs/2512.11782v1) | Peiqing Yang, Shangchen Zhou, Kai Hao, Qingyi Tao | 2025-12-12 | Multimodal AI, Training & Evaluation, Scaling Laws | Video matting remains limited by the scale and realism of existing datasets. While leveraging segmentation data can enhance semantic stability, the lack of effective boundary supervision often leads to segmentation-like mattes lacking fine details. To this end, we introduce a learned Matting Quality Evaluator (MQE) that assesses semantic and boundary quality of alpha mattes without ground truth. It produces a pixel-wise evaluation map that identifies reliable and erroneous regions, enabling fine-grained quality assessment. The MQE scales up video matting in two ways: (1) as an online matting-quality feedback during training to suppress erroneous regions, providing comprehensive supervision, and (2) as an offline selection module for data curation, improving annotation quality by combining the strengths of leading video and image matting models. This process allows us to build a large-scale real-world video matting dataset, VMReal, containing 28K clips and 2.4M frames. To handle large appearance variations in long videos, we introduce a reference-frame training strategy that incorporates long-range frames beyond the local window for effective training. Our MatAnyone 2 achieves state-of-the-art performance on both synthetic and real-world benchmarks, surpassing prior methods across all metrics. | [üîó Paper](https://arxiv.org/abs/2512.11782v1) |
| [BLURR: A Boosted Low-Resource Inference for Vision-Language-Action Models](https://arxiv.org/abs/2512.11769v1) | Xiaoyu Ma, Zhengqing Yuan, Zheyuan Zhang, Kaiwen Shi, Lichao Sun, Yanfang Ye | 2025-12-12 | Multimodal AI, Training & Evaluation, Production and Deployment | Vision-language-action (VLA) models enable impressive zero shot manipulation, but their inference stacks are often too heavy for responsive web demos or high frequency robot control on commodity GPUs. We present BLURR, a lightweight inference wrapper that can be plugged into existing VLA controllers without retraining or changing model checkpoints. Instantiated on the pi-zero VLA controller, BLURR keeps the original observation interfaces and accelerates control by combining an instruction prefix key value cache, mixed precision execution, and a single step rollout schedule that reduces per step computation. In our SimplerEnv based evaluation, BLURR maintains task success rates comparable to the original controller while significantly lowering effective FLOPs and wall clock latency. We also build an interactive web demo that allows users to switch between controllers and toggle inference options in real time while watching manipulation episodes. This highlights BLURR as a practical approach for deploying modern VLA policies under tight compute budgets. | [üîó Paper](https://arxiv.org/abs/2512.11769v1) |
## üîπ Optimization

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Moment-Based 3D Gaussian Splatting: Resolving Volumetric Occlusion with Order-Independent Transmittance](https://arxiv.org/abs/2512.11800v1) | Jan U. M√ºller, Robin Tim Landsgesell, Leif Van Holland, Patrick Stotko, Reinhard Klein | 2025-12-12 | Optimization | The recent success of 3D Gaussian Splatting (3DGS) has reshaped novel view synthesis by enabling fast optimization and real-time rendering of high-quality radiance fields. However, it relies on simplified, order-dependent alpha blending and coarse approximations of the density integral within the rasterizer, thereby limiting its ability to render complex, overlapping semi-transparent objects. In this paper, we extend rasterization-based rendering of 3D Gaussian representations with a novel method for high-fidelity transmittance computation, entirely avoiding the need for ray tracing or per-pixel sample sorting. Building on prior work in moment-based order-independent transparency, our key idea is to characterize the density distribution along each camera ray with a compact and continuous representation based on statistical moments. To this end, we analytically derive and compute a set of per-pixel moments from all contributing 3D Gaussians. From these moments, a continuous transmittance function is reconstructed for each ray, which is then independently sampled within each Gaussian. As a result, our method bridges the gap between rasterization and physical accuracy by modeling light attenuation in complex translucent media, significantly improving overall reconstruction and rendering quality. | [üîó Paper](https://arxiv.org/abs/2512.11800v1) |
| [Learning Minimal Representations of Fermionic Ground States](https://arxiv.org/abs/2512.11767v1) | Felix Frohnert, Emiel Koridon, Stefano Polla | 2025-12-12 | Optimization | We introduce an unsupervised machine-learning framework that discovers optimally compressed representations of quantum many-body ground states. Using an autoencoder neural network architecture on data from $L$-site Fermi-Hubbard models, we identify minimal latent spaces with a sharp reconstruction quality threshold at $L-1$ latent dimensions, matching the system's intrinsic degrees of freedom. We demonstrate the use of the trained decoder as a differentiable variational ansatz to minimize energy directly within the latent space. Crucially, this approach circumvents the $N$-representability problem, as the learned manifold implicitly restricts the optimization to physically valid quantum states. | [üîó Paper](https://arxiv.org/abs/2512.11767v1) |
| [SpectralKrum: A Spectral-Geometric Defense Against Byzantine Attacks in Federated Learning](https://arxiv.org/abs/2512.11760v1) | Aditya Tripathi, Karan Sharma, Rahul Mishra, Tapas Kumar Maiti | 2025-12-12 | Optimization | Federated Learning (FL) distributes model training across clients who retain their data locally, but this architecture exposes a fundamental vulnerability: Byzantine clients can inject arbitrarily corrupted updates that degrade or subvert the global model. While robust aggregation methods (including Krum, Bulyan, and coordinate-wise defenses) offer theoretical guarantees under idealized assumptions, their effectiveness erodes substantially when client data distributions are heterogeneous (non-IID) and adversaries can observe or approximate the defense mechanism.   This paper introduces SpectralKrum, a defense that fuses spectral subspace estimation with geometric neighbor-based selection. The core insight is that benign optimization trajectories, despite per-client heterogeneity, concentrate near a low-dimensional manifold that can be estimated from historical aggregates. SpectralKrum projects incoming updates into this learned subspace, applies Krum selection in compressed coordinates, and filters candidates whose orthogonal residual energy exceeds a data-driven threshold. The method requires no auxiliary data, operates entirely on model updates, and preserves FL privacy properties.   We evaluate SpectralKrum against eight robust baselines across seven attack scenarios on CIFAR-10 with Dirichlet-distributed non-IID partitions (alpha = 0.1). Experiments spanning over 56,000 training rounds show that SpectralKrum is competitive against directional and subspace-aware attacks (adaptive-steer, buffer-drift), but offers limited advantage under label-flip and min-max attacks where malicious updates remain spectrally indistinguishable from benign ones. | [üîó Paper](https://arxiv.org/abs/2512.11760v1) |
| [Generative Parametric Design (GPD): A framework for real-time geometry generation and on-the-fly multiparametric approximation](https://arxiv.org/abs/2512.11748v1) | Mohammed El Fallaki Idrissi, Jad Mounayer, Sebastian Rodriguez, Fodil Meraghni, Francisco Chinesta | 2025-12-12 | Optimization | This paper presents a novel paradigm in simulation-based engineering sciences by introducing a new framework called Generative Parametric Design (GPD). The GPD framework enables the generation of new designs along with their corresponding parametric solutions given as a reduced basis. To achieve this, two Rank Reduction Autoencoders (RRAEs) are employed, one for encoding and generating the design or geometry, and the other for encoding the sparse Proper Generalized Decomposition (sPGD) mode solutions. These models are linked in the latent space using regression techniques, allowing efficient transitions between design and their associated sPGD modes. By empowering design exploration and optimization, this framework also advances digital and hybrid twin development, enhancing predictive modeling and real-time decision-making in engineering applications. The developed framework is demonstrated on two-phase microstructures, in which the multiparametric solutions account for variations in two key material parameters. | [üîó Paper](https://arxiv.org/abs/2512.11748v1) |
## üîπ Scaling Laws

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [A General Algorithm for Detecting Higher-Order Interactions via Random Sequential Additions](https://arxiv.org/abs/2512.11793v1) | Ahmad Shamail, Claire McWhite | 2025-12-12 | Scaling Laws | Many systems exhibit complex interactions between their components: some features or actions amplify each other's effects, others provide redundant information, and some contribute independently. We present a simple geometric method for discovering interactions and redundancies: when elements are added in random sequential orders and their contributions plotted over many trials, characteristic L-shaped patterns emerge that directly reflect interaction structure. The approach quantifies how the contribution of each element depends on those added before it, revealing patterns that distinguish interaction, independence, and redundancy on a unified scale. When pairwise contributions are visualized as two--dimensional point clouds, redundant pairs form L--shaped patterns where only the first-added element contributes, while synergistic pairs form L--shaped patterns where only elements contribute together. Independent elements show order--invariant distributions. We formalize this with the L--score, a continuous measure ranging from $-1$ (perfect synergy, e.g. $Y=X_1X_2$) to $0$ (independence) to $+1$ (perfect redundancy, $X_1 \approx X_2$). The relative scaling of the L--shaped arms reveals feature dominance in which element consistently provides more information. Although computed only from pairwise measurements, higher--order interactions among three or more elements emerge naturally through consistent cross--pair relationships (e.g. AB, AC, BC). The method is metric--agnostic and broadly applicable to any domain where performance can be evaluated incrementally over non-repeating element sequences, providing a unified geometric approach to uncovering interaction structure. | [üîó Paper](https://arxiv.org/abs/2512.11793v1) |
| [Quantum Krylov algorithm using unitary decomposition for exact eigenstates of fermionic systems using quantum computers](https://arxiv.org/abs/2512.11788v1) | Ayush Asthana | 2025-12-12 | Scaling Laws | Quantum Krylov algorithms have emerged as a useful framework for quantum simulations in quantum chemistry and many-body physics, offering a favorable trade-off between potential quantum speedups and practical resource demands. However, the current primary approach to building Krylov vectors in these algorithms is to use real or imaginary-time evolution, which is not exact, require an arbitrary time-step parameter ($Œît$), and degrade the Krylov vectors quickly with increasing $Œît$. In this paper, we develop a quantum Krylov algorithm without time evolution and with an exact formulation of the Krylov subspace, named ``Quantum Krylov using Unitary Decomposition'' (QKUD), along with implementation proposals for quantum computers. Not only is this algorithm exact in the limit $Œµ\to 0$ of the error parameter $Œµ$, but it also produces more accurate Krylov vectors at $Œµ\neq 0$ than conventional time evolution due to more favorable error scaling (O($Œµ^2$) vs O($Œît$)). Through simulations, we demonstrate that these theoretical benefits yield numerical advantages: (i) QKUD provides numerically exact results at small $Œµ$, (ii) it remains stable across a broad range of $Œµ$ values, indicating low parameter sensitivity, and (iii) it can solve problems unreachable by conventional time evolution. This development resolves a central limitation of quantum Krylov algorithms, namely their inexactness and sensitivity to the time-step parameter, and paves the way for new and powerful quantum Krylov algorithms for quantum computers with a stronger promise of quantum advantage. | [üîó Paper](https://arxiv.org/abs/2512.11788v1) |
| [Statistics of the vortex pinning potential in superconducting films](https://arxiv.org/abs/2512.11780v1) | Matvei S. Kniazev, Nikolai A. Stepanov, Mikhail A. Skvortsov | 2025-12-12 | Scaling Laws | We investigate the statistical properties of the vortex pinning potential in a thin superconducting film. Modeling intrinsic inhomogeneities by a random-temperature Ginzburg-Landau functional with short-range Gaussian disorder, we derive the pinning landscape $E(\mathbf{R})$ by determining how the vortex core adapts to randomness. Within the hard-core approximation, applicable for weak disorder, the energy landscape exhibits Gaussian statistics. In this regime, the mean areal density of its minima is given by $n_\text{min}\approx(6Œæ)^{-2}$, indicating that the typical spacing between neighboring minima is significantly larger than the vortex core size $Œæ$. Going beyond the hard-core approximation, we allow the vortex order parameter to relax in response to the inhomogeneities. As a result, the pinning potential statistics become non-Gaussian. We calculate the leading correction due to the core deformation, which reduces the density of minima with a relative magnitude scaling as $(T_c-T)^{-1/2}$. | [üîó Paper](https://arxiv.org/abs/2512.11780v1) |
## üîπ Training & Evaluation

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Solving the Cosmological Vlasov-Poisson Equations with Physics-Informed Kolmogorov-Arnold Networks](https://arxiv.org/abs/2512.11795v1) | Nicolas Cerardi, Emma Tolley, Ashutosh Mishra | 2025-12-12 | Training & Evaluation | Cold dark matter (CDM) evolves as a collisionless fluid under the Vlasov-Poisson equations, but N-body simulations approximate this evolution by discretising the distribution function into particles, introducing discreteness effects at small scales. We present a physics-informed neural network approach that evolves CDM fields without any use of N-body data or methods, using a Kolmogorov-Arnold network (KAN) to model the continuous displacement field for one-dimensional halo collapse. Physical constraints derived from the Vlasov-Poisson equations are embedded directly into the loss function, enabling accurate evolution beyond the first shell crossing. The trained model achieves sub-percent errors on the residuals even after seven shell crossings and matches N-body results while providing a resolution-free representation of the displacement field. In addition, displacement errors do not grow over time, a very interesting feature with respect to N-body methods. It can also reconstruct initial conditions through backward evolution when sufficient final-state information is available. Although current runtimes exceed those of N-body methods, this framework offers a new route to high-fidelity CDM evolution without particle discretisation, with prospects for extension to higher dimensions. | [üîó Paper](https://arxiv.org/abs/2512.11795v1) |
| [Conditional Coverage Diagnostics for Conformal Prediction](https://arxiv.org/abs/2512.11779v1) | Sacha Braun, David Holzm√ºller, Michael I. Jordan, Francis Bach | 2025-12-12 | Training & Evaluation | Evaluating conditional coverage remains one of the most persistent challenges in assessing the reliability of predictive systems. Although conformal methods can give guarantees on marginal coverage, no method can guarantee to produce sets with correct conditional coverage, leaving practitioners without a clear way to interpret local deviations. To overcome sample-inefficiency and overfitting issues of existing metrics, we cast conditional coverage estimation as a classification problem. Conditional coverage is violated if and only if any classifier can achieve lower risk than the target coverage. Through the choice of a (proper) loss function, the resulting risk difference gives a conservative estimate of natural miscoverage measures such as L1 and L2 distance, and can even separate the effects of over- and under-coverage, and non-constant target coverages. We call the resulting family of metrics excess risk of the target coverage (ERT). We show experimentally that the use of modern classifiers provides much higher statistical power than simple classifiers underlying established metrics like CovGap. Additionally, we use our metric to benchmark different conformal prediction methods. Finally, we release an open-source package for ERT as well as previous conditional coverage metrics. Together, these contributions provide a new lens for understanding, diagnosing, and improving the conditional reliability of predictive systems. | [üîó Paper](https://arxiv.org/abs/2512.11779v1) |
| [SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder](https://arxiv.org/abs/2512.11749v1) | Minglei Shi, Haolin Wang, Borui Zhang, Wenzhao Zheng, Bohan Zeng, Ziyang Yuan, Xiaoshi Wu, Yuanxing Zhang, Huan Yang, Xintao Wang, Pengfei Wan, Kun Gai, Jie Zhou, Jiwen Lu | 2025-12-12 | Training & Evaluation, Diffusion Models, Scaling Laws | Visual generation grounded in Visual Foundation Model (VFM) representations offers a highly promising unified pathway for integrating visual understanding, perception, and generation. Despite this potential, training large-scale text-to-image diffusion models entirely within the VFM representation space remains largely unexplored. To bridge this gap, we scale the SVG (Self-supervised representations for Visual Generation) framework, proposing SVG-T2I to support high-quality text-to-image synthesis directly in the VFM feature domain. By leveraging a standard text-to-image diffusion pipeline, SVG-T2I achieves competitive performance, reaching 0.75 on GenEval and 85.78 on DPG-Bench. This performance validates the intrinsic representational power of VFMs for generative tasks. We fully open-source the project, including the autoencoder and generation model, together with their training, inference, evaluation pipelines, and pre-trained weights, to facilitate further research in representation-driven visual generation. | [üîó Paper](https://arxiv.org/abs/2512.11749v1) |
## üîπ Model Evaluation

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Super Suffixes: Bypassing Text Generation Alignment and Guard Models Simultaneously](https://arxiv.org/abs/2512.11783v1) | Andrew Adiletta, Kathryn Adiletta, Kemal Derya, Berk Sunar | 2025-12-12 | Model Evaluation, LLM, Responsible AI, RLHF, Optimization, Security & Adversarial ML | The rapid deployment of Large Language Models (LLMs) has created an urgent need for enhanced security and privacy measures in Machine Learning (ML). LLMs are increasingly being used to process untrusted text inputs and even generate executable code, often while having access to sensitive system controls. To address these security concerns, several companies have introduced guard models, which are smaller, specialized models designed to protect text generation models from adversarial or malicious inputs. In this work, we advance the study of adversarial inputs by introducing Super Suffixes, suffixes capable of overriding multiple alignment objectives across various models with different tokenization schemes. We demonstrate their effectiveness, along with our joint optimization technique, by successfully bypassing the protection mechanisms of Llama Prompt Guard 2 on five different text generation models for malicious text and code generation. To the best of our knowledge, this is the first work to reveal that Llama Prompt Guard 2 can be compromised through joint optimization.   Additionally, by analyzing the changing similarity of a model's internal state to specific concept directions during token sequence processing, we propose an effective and lightweight method to detect Super Suffix attacks. We show that the cosine similarity between the residual stream and certain concept directions serves as a distinctive fingerprint of model intent. Our proposed countermeasure, DeltaGuard, significantly improves the detection of malicious prompts generated through Super Suffixes. It increases the non-benign classification rate to nearly 100%, making DeltaGuard a valuable addition to the guard model stack and enhancing robustness against adversarial prompt attacks. | [üîó Paper](https://arxiv.org/abs/2512.11783v1) |
## üîπ Ongoing Learning

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [LUCID: Learning-Enabled Uncertainty-Aware Certification of Stochastic Dynamical Systems](https://arxiv.org/abs/2512.11750v1) | Ernesto Casablanca, Oliver Sch√∂n, Paolo Zuliani, Sadegh Soudjani | 2025-12-12 | Ongoing Learning, Optimization | Ensuring the safety of AI-enabled systems, particularly in high-stakes domains such as autonomous driving and healthcare, has become increasingly critical. Traditional formal verification tools fall short when faced with systems that embed both opaque, black-box AI components and complex stochastic dynamics. To address these challenges, we introduce LUCID (Learning-enabled Uncertainty-aware Certification of stochastIc Dynamical systems), a verification engine for certifying safety of black-box stochastic dynamical systems from a finite dataset of random state transitions. As such, LUCID is the first known tool capable of establishing quantified safety guarantees for such systems. Thanks to its modular architecture and extensive documentation, LUCID is designed for easy extensibility. LUCID employs a data-driven methodology rooted in control barrier certificates, which are learned directly from system transition data, to ensure formal safety guarantees. We use conditional mean embeddings to embed data into a reproducing kernel Hilbert space (RKHS), where an RKHS ambiguity set is constructed that can be inflated to robustify the result to out-of-distribution behavior. A key innovation within LUCID is its use of a finite Fourier kernel expansion to reformulate a semi-infinite non-convex optimization problem into a tractable linear program. The resulting spectral barrier allows us to leverage the fast Fourier transform to generate the relaxed problem efficiently, offering a scalable yet distributionally robust framework for verifying safety. LUCID thus offers a robust and efficient verification framework, able to handle the complexities of modern black-box systems while providing formal guarantees of safety. These unique capabilities are demonstrated on challenging benchmarks. | [üîó Paper](https://arxiv.org/abs/2512.11750v1) |
## üîπ Responsible AI

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [The Adaptive Vekua Cascade: A Differentiable Spectral-Analytic Solver for Physics-Informed Representation](https://arxiv.org/abs/2512.11776v1) | Vladimer Khasia | 2025-12-12 | Responsible AI, Model Evaluation | Coordinate-based neural networks have emerged as a powerful tool for representing continuous physical fields, yet they face two fundamental pathologies: spectral bias, which hinders the learning of high-frequency dynamics, and the curse of dimensionality, which causes parameter explosion in discrete feature grids. We propose the Adaptive Vekua Cascade (AVC), a hybrid architecture that bridges deep learning and classical approximation theory. AVC decouples manifold learning from function approximation by using a deep network to learn a diffeomorphic warping of the physical domain, projecting complex spatiotemporal dynamics onto a latent manifold where the solution is represented by a basis of generalized analytic functions. Crucially, we replace the standard gradient-descent output layer with a differentiable linear solver, allowing the network to optimally resolve spectral coefficients in a closed form during the forward pass. We evaluate AVC on a suite of five rigorous physics benchmarks, including high-frequency Helmholtz wave propagation, sparse medical reconstruction, and unsteady 3D Navier-Stokes turbulence. Our results demonstrate that AVC achieves state-of-the-art accuracy while reducing parameter counts by orders of magnitude (e.g., 840 parameters vs. 4.2 million for 3D grids) and converging 2-3x faster than implicit neural representations. This work establishes a new paradigm for memory-efficient, spectrally accurate scientific machine learning. The code is available at https://github.com/VladimerKhasia/vecua. | [üîó Paper](https://arxiv.org/abs/2512.11776v1) |
| [Hypergraph based Multi-Party Payment Channel](https://arxiv.org/abs/2512.11775v1) | Ayush Nainwal, Atharva Kamble, Nitin Awathare | 2025-12-12 | Responsible AI, Model Evaluation, Production and Deployment | Public blockchains inherently offer low throughput and high latency, motivating off-chain scalability solutions such as Payment Channel Networks (PCNs). However, existing PCNs suffer from liquidity fragmentation-funds locked in one channel cannot be reused elsewhere-and channel depletion, both of which limit routing efficiency and reduce transaction success rates. Multi-party channel (MPC) constructions mitigate these issues, but they typically rely on leaders or coordinators, creating single points of failure and providing only limited flexibility for inter-channel payments.   We introduce Hypergraph-based Multi-Party Payment Channels (H-MPCs), a new off-chain construction that replaces bilateral channels with collectively funded hyperedges. These hyperedges enable fully concurrent, leaderless intra- and inter-hyperedge payments through verifiable, proposer-ordered DAG updates, offering significantly greater flexibility and concurrency than prior designs.   Our implementation on a 150-node network demonstrates a transaction success rate of approximately 94% without HTLC expiry or routing failures, highlighting the robustness of H-MPCs. | [üîó Paper](https://arxiv.org/abs/2512.11775v1) |
| [Smudged Fingerprints: A Systematic Evaluation of the Robustness of AI Image Fingerprints](https://arxiv.org/abs/2512.11771v1) | Kai Yao, Marc Juarez | 2025-12-12 | Responsible AI, Model Evaluation, Training & Evaluation, Security & Adversarial ML | Model fingerprint detection techniques have emerged as a promising approach for attributing AI-generated images to their source models, but their robustness under adversarial conditions remains largely unexplored. We present the first systematic security evaluation of these techniques, formalizing threat models that encompass both white- and black-box access and two attack goals: fingerprint removal, which erases identifying traces to evade attribution, and fingerprint forgery, which seeks to cause misattribution to a target model. We implement five attack strategies and evaluate 14 representative fingerprinting methods across RGB, frequency, and learned-feature domains on 12 state-of-the-art image generators. Our experiments reveal a pronounced gap between clean and adversarial performance. Removal attacks are highly effective, often achieving success rates above 80% in white-box settings and over 50% under constrained black-box access. While forgery is more challenging than removal, its success significantly varies across targeted models. We also identify a utility-robustness trade-off: methods with the highest attribution accuracy are often vulnerable to attacks. Although some techniques exhibit robustness in specific settings, none achieves high robustness and accuracy across all evaluated threat models. These findings highlight the need for techniques balancing robustness and accuracy, and identify the most promising approaches for advancing this goal. | [üîó Paper](https://arxiv.org/abs/2512.11771v1) |
| [Computing the molecular ground state energy in a restricted active space using quantum annealing](https://arxiv.org/abs/2512.11757v1) | Stefano Bruni, Enrico Prati | 2025-12-12 | Responsible AI, Scaling Laws, Model Evaluation, Optimization | Calculating the molecular ground-state energy is a central challenge in computational chemistry. Conventional methods such as the Complete Active Space Configuration Interaction scale exponentially with molecular size, limiting their applicability to large molecules. Quantum computing offers a promising alternative by mapping molecular Hamiltonians by qubits, enabling cheaper computational scaling. Previous studies have shown that it is possible to formulate molecular ground state calculations as discrete optimization problems, addressable by quantum annealing. However, these efforts have been limited by previous generations of hardware and suboptimal annealing techniques. Here, the $H_{2}O$ ground-state problem is mapped to an Ising Hamiltonian using the Xian-Bias-Kas (XBK) method. By taking advantage of enhanced qubit connectivity and shorter embedding chains, it is solved with a more than doubled probability of achieving Hartree-Fock-level solutions with respect to the most advanced predecessor. Advanced annealing strategies extend Hartree-Fock-level accuracy to significantly larger problem instances, enabling solutions that use nearly 2.5 times more physically embedded qubits than the largest cases previously reported and allowing to improve annealing results by two orders of magnitude, reaching an energy difference of 0.120~Hartree relative to Hartree-Fock. These results show tangible progress toward practical quantum annealing applications in NISQ era. | [üîó Paper](https://arxiv.org/abs/2512.11757v1) |
| [Elastocapillary adhesion of soft gel microspheres](https://arxiv.org/abs/2512.11752v1) | Joseph N. Headley, Edgar W. Lyons, Mathew Q. Giso, Emily P. Kuwaye, Caroline D. Tally, Aidan J. Duncan, Chaitanya Joshi, Timothy J. Atherton, Katharine E. Jensen | 2025-12-12 | Responsible AI, Model Evaluation | Softer means stickier for solid adhesives, because material compliance facilitates close contact between non-conformal surfaces. Recent discoveries have revealed that soft materials can exhibit a rich array of new physics arising from competing effects of continuum elasticity, fluid-like surface mechanics, and internal poroelastic flows, all of which can directly impact interfacial interactions. In this work, we investigate this complex interplay across several orders of magnitude of elastic stiffness by measuring the complete adhesive contact geometry between compliant silicone gel microspheres and flat, rigid substrates. We observe a continuous elastocapillary transition in adhesion mechanics, with novel features revealed by both the breadth of data and the detailed contact geometries. Importantly, soft gel spheres exhibit a remarkably broad range of near-equilibrium contact morphologies and their contact line deformation is always mediated by a fluid contact zone that phase separates from the gel. To explain this, we develop a new model incorporating elastocapillary and poroelastic mechanics that predicts the complete range of adhesive behavior and elucidates energetic tradeoffs. The data and model together reveal a shallow energy landscape that may contribute to the robustness of everyday adhesives. | [üîó Paper](https://arxiv.org/abs/2512.11752v1) |
## üîπ General AI

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [A Room-Temperature Extreme High Vacuum System for Trapped-Ion Quantum Information Processing](https://arxiv.org/abs/2512.11794v1) | Lewis Hahn, Nikhil Kotibhaskar, Fabien Lefebvre, Sakshee Patil, Sainath Motlakunta, Mahmood Sabooni, Rajibul Islam | 2025-12-12 | General AI | We present a room-temperature Extreme High Vacuum (XHV) system engineered to support the long-duration operation of a trapped-ion quantum processor. Background-gas collisions impose limitations on trapped-ion performance and scalability by interrupting algorithmic execution and, in some cases, ejecting ions from the trap. Using molecular-flow simulations, we optimize the chamber geometry, conductance pathways, and pumping configuration to maximize the effective pumping speed at the ion location. We perform high-temperature heat treatment of stainless steel vacuum components to achieve the desired outgassing rate, guided by quantitative relations of bulk diffusive processes, allowing us to reduce the \(\mathrm{H_2}\) outgassing load to the \(10^{-15}\,\mathrm{mbar\,l\,s^{-1}\,cm^{-2}}\) level. The final pressure in our chamber, measured by a hot cathode gauge, is \(1.5\times10^{-12}\,\mathrm{mbar}\), corresponding to the gauge's measurement limit. We measure the local pressure at the ion location by observing collision-induced reordering events in a long ion chain of mixed-isotope Yb\(^+\). From the observed reordering frequency, we extract the average interval between collisions to be \((1.9 \pm 0.1)\,\mathrm{hrs/ion}\). This corresponds to a local pressure of \((3.9 \pm 0.3)\times10^{-12}\,\mathrm{mbar}\) at the ion location, assuming that all collisions arise from background H\(_2\) molecules at room temperature. Our demonstration extends the continuous operation time of a quantum processor while maintaining the simplicity of a room-temperature system that does not require cryogenic apparatus. | [üîó Paper](https://arxiv.org/abs/2512.11794v1) |
| [GLIMPSE-D: An Exotic Balmer-Jump Object at z=6.20? Revisiting Photometric Selection and the Cosmic Abundance of Pop III Galaxies](https://arxiv.org/abs/2512.11790v1) | Seiji Fujimoto, Yoshihisa Asada, Rohan P. Naidu, John Chisholm, Hakim Atek, Gabriel Brammer, Danielle A. Berg, Daniel Schaerer, Vasily Kokorev, Lukas J. Furtak, Johan Richard, Alessandra Venditti, Volker Bromm, Angela Adamo, Adelaide Claeyssens, Miroslava Dessauges-Zavadsky, Qinyue Fei, Tiger Yu-Yang Hsiao, Damien Korber, Julian B. Munoz, Richard Pan, Alberto Saldana-Lopez | 2025-12-12 | General AI | We present deep JWST/NIRSpec G395M spectroscopy of GLIMPSE-16043, a promising $z\sim6$ Pop III candidate originally identified through NIRCam photometry as having weak [OIII]$Œª\lambda4959,5007$ emission. Our follow-up reveals clear [OIII] emission, ruling out a genuine zero-metallicity nature. However, the combination of the measured line fluxes and photometry indicates that its spectral energy distribution requires an extraordinarily strong Balmer jump ($-1.66 \pm 0.47$ mag) and H$Œ±$ equivalent width ($3750\pm1800$ √Ö), features that cannot be reproduced by current stellar+nebular or pure nebular photoionization models. The only models approaching the observations to almost within $1œÉ$ involve a hot ($T_{\rm eff}\!\simeq\!10^{4.7}$ K) single blackbody embedded in a low-$T_{\rm e}$ nebular environment, suggestive of scenarios such as a tidal-disruption event or a microquasar with strong disk winds. This cautions that photometric Pop~III selections are vulnerable to contamination when the rest-frame optical continuum is undetected. Motivated by this, we refine the photometric Pop III selection criteria to exclude the locus of extreme Balmer-jump objects. The revised criteria also recover the recently reported spectroscopic candidate AMORE6, demonstrating that the updated selection preserves sensitivity to genuine Pop III-like sources while removing key contaminants. Applying the refined criteria across legacy survey fields and five newly released CANUCS lensing cluster fields, we revisit the Pop III UV luminosity function and estimate the Pop III cosmic star-formation rate density to be $\approx[10^{-6}$--$10^{-4}]$~$M_{\odot}$~yr$^{-1}$~cMpc$^{-3}$ at $z\simeq6$--7, falling in the range of current theoretical predictions. | [üîó Paper](https://arxiv.org/abs/2512.11790v1) |
| [The Gevrey class of the Euler-Bernoulli beam model with singularities](https://arxiv.org/abs/2512.11789v1) | Jaime E. Munoz Rivera, Maria Grazia Naso, Bruna T. Silva Sozzo | 2025-12-12 | General AI | We study the Euler-Bernoulli beam model with singularities at the points $x=Œæ_1$, $x=Œæ_2$ and with localized viscoelastic dissipation of Kelvin-Voigt type. We assume that the beam is composed by two materials; one is an elastic material and the other one is a viscoelastic material of Kelvin-Voigt type.   Our main result is that the corresponding semigroup is immediately differentiable and also of Gevrey class $4$.   In particular, our result implies that the model is exponentially stable, has the linear stability property, and the smoothing effect property over the initial data. | [üîó Paper](https://arxiv.org/abs/2512.11789v1) |
| [Tree-Level Gravity Amplitudes at Infinity](https://arxiv.org/abs/2512.11787v1) | Justin Lemmon, Jaroslav Trnka | 2025-12-12 | General AI | In this note we study on-shell tree-level gravity amplitudes in the infinite momentum limit. In the case of the two-line BCFW shift, we have a famous improved behavior at infinity that allows for the amplitude to be reconstructed from the pole factorization. For other shifts, the poles at infinity are present and need to be considered, however general principles do not fix the residues of the amplitude on these poles. The web of all possible shifts is large, we focus primarily on a case of $(n{-}2)$-line anti-holomorphic shift, which also appears in the context of unitarity cuts of gravity loop integrands. We will find that for one class of shifts the gravity amplitudes at infinity exhibit a peculiar factorization property, quite different from the usual factorization on poles, while for other shifts, they evaluate to the same amplitude on shifted kinematics. We also discuss generalizations of our results to other anti-holomorphic shifts. | [üîó Paper](https://arxiv.org/abs/2512.11787v1) |
| [Universal entrywise eigenvector fluctuations in delocalized spiked matrix models and asymptotics of rounded spectral algorithms](https://arxiv.org/abs/2512.11785v1) | Shujing Chen, Dmitriy Kunisky | 2025-12-12 | General AI | We consider the distribution of the top eigenvector $\widehat{v}$ of a spiked matrix model of the form $H = Œ∏vv^* + W$, in the supercritical regime where $H$ has an outlier eigenvalue of comparable magnitude to $\ W\ $. We show that, if $v$ is sufficiently delocalized, then the distribution of the individual entries of $\widehat{v}$ (not, we emphasize, merely the inner product $\langle \widehat{v}, v\rangle$) is universal over a large class of generalized Wigner matrices $W$ having independent entries, depending only on the first two moments of the distributions of the entries of $W$. This complements the observation of Capitaine and Donati-Martin (2018) that these distributions are not universal when $v$ is instead sufficiently localized. Further, for $W$ having entrywise variances close to constant and thus resembling a Wigner matrix, we show by comparing to the case of $W$ drawn from the Gaussian orthogonal or unitary ensembles that averages of entrywise functions of $\widehat{v}$ behave as they would if $\widehat{v}$ had Gaussian fluctuations around a suitable multiple of $v$. We apply these results to study spectral algorithms followed by rounding procedures in dense stochastic block models and synchronization problems over the cyclic and circle groups, obtaining the first precise asymptotic characterizations of the error rates of such algorithms. | [üîó Paper](https://arxiv.org/abs/2512.11785v1) |
| [A Doubled Adjacency Spectral Embedding Approach to Graph Clustering](https://arxiv.org/abs/2512.11777v1) | Sinyoung Park, Matthew Nunes, Sandipan Roy | 2025-12-12 | General AI | Spectral clustering is a popular tool in network data analysis, with applications in a variety of scientific application areas. However, many studies have shown that spectral clustering does not perform well on certain network structures, particularly core-periphery networks. To improve clustering performance in core-periphery structures, Adjacency Spectral Embedding (ASE) has been introduced, which performs clustering via a network's adjacency matrix instead of the graph Laplacian. Despite its advantages in this setting, the optimal performance of ASE is limited to dense networks, whilst network data observed in practice is often sparse in nature. To address this limitation, we propose a new approach which we term Doubled Adjacency Spectral Embedding (DASE), motivated by the observation that the squared adjacency matrix will leverage the fewer connections in sparse structures more efficiently in clustering applications. Theoretical results establish that DASE enjoys good consistency properties when determining sparse community structure. The performance and general applicability of the proposed method is evaluated using extensive simulations on both directed and undirected networks. Our results highlight the improved clustering performance on both sparse and dense networks in the presence of core-periphery structures. We illustrate our proposed technique on real-world employment and transportation datasets. | [üîó Paper](https://arxiv.org/abs/2512.11777v1) |
| [Towards Breath Based Diagnostics via Water-mediated Capture of Synthetic Breath Biomarkers in SERS-active Plasmonic Nanogaps](https://arxiv.org/abs/2512.11774v1) | Aditya Garg, Marissa Morales, Aashini Shah, Daniel Kim, Ming Lei, Sahil Patel, Jia Dong, Seleem Badawy, Sangeeta Bhatia, Loza F. Tadesse | 2025-12-12 | General AI | Volatile organic compounds (VOCs) are valuable health indicators, with synthetic breath biomarkers offering rapid and disease specific diagnostics. However, their <100 ppb level exhalation requires mass spectrometry, limiting clinical integration. Surface-enhanced Raman spectroscopy (SERS) offers a portable, cost-effective alternative. Yet, detecting synthetic breath biomarkers, with inherently low Raman cross-sections, at <100 ppb remains challenging. We demonstrate SERS detection down to clinically relevant 10 ppb via water-mediated trapping in hydroxylated nanoporous silica-coated plasmonic nanogaps, using pentafluoropropylamine (PFP) as a representative synthetic breath biomarker. Uniform nanogaps, with >1000 times electric field enhancement, were generated between a gold film and gold-silica core-shell nanoparticle assemblies using electric field-driven evaporation. Oxygen plasma treatment hydroxylated the silica, enabling water-mediated hydrogen bonding that strengthened PFP adsorption, confirmed by density functional theory. This mechanism improved SERS sensitivity by 10000 fold, enabling ppb level PFP detection in mouse bronchial fluid and establishing a VOC capturing SERS platform for breath-based diagnostics. | [üîó Paper](https://arxiv.org/abs/2512.11774v1) |
| [ProbeMDE: Uncertainty-Guided Active Proprioception for Monocular Depth Estimation in Surgical Robotics](https://arxiv.org/abs/2512.11773v1) | Britton Jordan, Jordan Thompson, Jesse F. d'Almeida, Hao Li, Nithesh Kumar, Susheela Sharma Stern, Ipek Oguz, Robert J. Webster, Daniel Brown, Alan Kuntz, James Ferguson | 2025-12-12 | General AI | Monocular depth estimation (MDE) provides a useful tool for robotic perception, but its predictions are often uncertain and inaccurate in challenging environments such as surgical scenes where textureless surfaces, specular reflections, and occlusions are common. To address this, we propose ProbeMDE, a cost-aware active sensing framework that combines RGB images with sparse proprioceptive measurements for MDE. Our approach utilizes an ensemble of MDE models to predict dense depth maps conditioned on both RGB images and on a sparse set of known depth measurements obtained via proprioception, where the robot has touched the environment in a known configuration. We quantify predictive uncertainty via the ensemble's variance and measure the gradient of the uncertainty with respect to candidate measurement locations. To prevent mode collapse while selecting maximally informative locations to propriocept (touch), we leverage Stein Variational Gradient Descent (SVGD) over this gradient map. We validate our method in both simulated and physical experiments on central airway obstruction surgical phantoms. Our results demonstrate that our approach outperforms baseline methods across standard depth estimation metrics, achieving higher accuracy while minimizing the number of required proprioceptive measurements. | [üîó Paper](https://arxiv.org/abs/2512.11773v1) |
| [A Vlasov-Bohm approach to Quantum Mechanics for statistical systems](https://arxiv.org/abs/2512.11772v1) | Pedro Luis Grande, Raul Carlos Fadanelli, Maarten Vos | 2025-12-12 | General AI | Quantum mechanics is the most successful theory to describe microscopic phenomena. It was derived in different ways over the past 100 years by Heisenberg, Schr√∂dinger, and Feynman. At the same time, other interpretations have been suggested, including the Bohm-De Broglie interpretation and the so-called Bohmian mechanics. Here, we show that Bohmian mechanics, which utilizes the concept of the Bohm quantum potential, can also serve as a starting point for quantizing classical non-relativistic systems. By incorporating the Bohm quantum potential into the Vlasov framework, we obtain a mean-field theory that captures the corpuscular nature of matter, in agreement with quantum mechanics within the Random Phase Approximation (RPA). | [üîó Paper](https://arxiv.org/abs/2512.11772v1) |
| [Leptophilic Interactions in Nuclear Energy Density Functional Theory](https://arxiv.org/abs/2512.11770v1) | S. O. Kara | 2025-12-12 | General AI | We develop a unified theoretical framework that embeds a light leptophilic vector boson into nuclear energy density functional (EDF) theory. Starting from an underlying leptophilic gauge interaction, the mediator is integrated out in the static limit, yielding an effective current--current interaction that couples proton and lepton densities. This interaction is incorporated self-consistently into relativistic mean-field equations, defining a leptophilic extension of conventional nuclear EDFs.   The resulting leptophilic EDF induces correlated modifications of proton and lepton chemical potentials, directly affecting beta equilibrium in dense matter. In uniform matter, these effects lead to percent-level changes in the proton fraction, symmetry energy, and equation of state within phenomenologically allowed parameter ranges. In finite nuclei, the modified proton mean field generates shifts of $10^{-3}$--$10^{-2}\,\mathrm{fm}$ in neutron-skin thicknesses, comparable to current experimental sensitivities.   Our results demonstrate that light leptophilic interactions leave coherent and experimentally accessible imprints on both nuclear structure and dense-matter observables. The framework introduced here provides a controlled and realistic extension of nuclear EDF theory, enabling nuclear systems to serve as laboratories for probing new physics in the leptonic sector. | [üîó Paper](https://arxiv.org/abs/2512.11770v1) |
| [A low-cost ice melt monitoring system using wind-induced motion of mass-balance stakes](https://arxiv.org/abs/2512.11768v1) | Felix St-Amour, H. Cynthia Chiang, Jamie Cox, Eamon Egan, Ian Hendricksen, Jonathan Sievers, Laura Thomson | 2025-12-12 | General AI | Surface ablation measurements of glaciers are critical for understanding mass change over time. Mass-balance stakes are commonly used for localized measurements, with the exposed length typically measured manually at infrequent intervals. This paper presents the design and validation of new instrumentation that automates mass-balance stake readings, thus enabling continuous measurements with high temporal resolution. The instrumentation comprises readout electronics that are mounted on mass-balance stakes to measure wind-induced vibrations. The stake vibrational frequency depends sensitively on the exposed length, and changes in the measured frequency therefore probe glacier surface melt and accumulation. Initial instrumentation field tests conducted at Color Lake on Umingmat Nunaat (Axel Heiberg Island), Nunavut, demonstrate centimeter-level precision on length measurements. The instrumentation can be attached to existing mass-balance stakes and is low-cost (~ $50 USD) in comparison to many other systems that perform automated surface ablation measurements. The accessibility of this instrumentation opens new possibilities for localized, high temporal resolution measurements of glacier surface activity at any locations where mass balance stakes are deployed. | [üîó Paper](https://arxiv.org/abs/2512.11768v1) |
| [When Is Nanoconfined Water Different From Interfacial Water?](https://arxiv.org/abs/2512.11766v1) | Xavier R. Advincula, Christoph Schran, Angelos Michaelides | 2025-12-12 | General AI | Water behaves very differently at surfaces and under extreme confinement, but the boundary between these two regimes has remained unclear. Despite evidence that interfacial effects persist under sub-nanometre confinement, the molecular-scale behaviour and its evolution with slit width remain unclear. Here, we use machine-learning molecular dynamics with first-principles accuracy to probe water at graphene surfaces across slit widths ranging from the open-interface limit to angstrom-scale confinement. We find that water undergoes a sharp structural transition: when three or more water layers fit between the walls, the structure of the graphene-water interface is effectively indistinguishable from that in an open system, with density layering, hydrogen bonding, and orientational ordering retaining interfacial character. Below this threshold, however, angstrom-scale confinement strongly reorganises the liquid, producing enhanced ordering, a restructured hydrogen-bond network, and modified orientational motifs. These results establish a molecular-level picture that clearly separates interfacial behaviour from genuine nanoconfinement and provide guidance for predicting and controlling the structure of water in nanoscale solid-liquid environments. | [üîó Paper](https://arxiv.org/abs/2512.11766v1) |
| [High-Frequency Analysis of a Trading Game with Transient Price Impact](https://arxiv.org/abs/2512.11765v1) | Marcel Nutz, Alessandro Prosperi | 2025-12-12 | General AI | We study the high-frequency limit of an $n$-trader optimal execution game in discrete time. Traders face transient price impact of Obizhaeva--Wang type in addition to quadratic instantaneous trading costs $Œ∏(ŒîX_t)^2$ on each transaction $ŒîX_t$. There is a unique Nash equilibrium in which traders choose liquidation strategies minimizing expected execution costs. In the high-frequency limit where the grid of trading dates converges to the continuous interval $[0,T]$, the discrete equilibrium inventories converge at rate $1/N$ to the continuous-time equilibrium of an Obizhaeva--Wang model with additional quadratic costs $\vartheta_0(ŒîX_0)^2$ and $\vartheta_T(ŒîX_T)^2$ on initial and terminal block trades, where $\vartheta_0=(n-1)/2$ and $\vartheta_T=1/2$. The latter model was introduced by Campbell and Nutz as the limit of continuous-time equilibria with vanishing instantaneous costs. Our results extend and refine previous results of Schied, Strehle, and Zhang for the particular case $n=2$ where $\vartheta_0=\vartheta_T=1/2$. In particular, we show how the coefficients $\vartheta_0=(n-1)/2$ and $\vartheta_T=1/2$ arise endogenously in the high-frequency limit: the initial and terminal block costs of the continuous-time model are identified as the limits of the cumulative discrete instantaneous costs incurred over small neighborhoods of $0$ and $T$, respectively, and these limits are independent of $Œ∏>0$. By contrast, when $Œ∏=0$ the discrete-time equilibrium strategies and costs exhibit persistent oscillations and admit no high-frequency limit, mirroring the non-existence of continuous-time equilibria without boundary block costs. Our results show that two different types of trading frictions -- a fine time discretization and small instantaneous costs in continuous time -- have similar regularizing effects and select a canonical model in the limit. | [üîó Paper](https://arxiv.org/abs/2512.11765v1) |
| [Characterizing LHC-Resonances in extended HEFT: information on the nature of extended scalar sectors](https://arxiv.org/abs/2512.11764v1) | Giorgio Arcadi, David Cabo-Almeida, Florian Goertz, Maya Hager | 2025-12-12 | General AI | In theories with extended scalar sectors the lightest new scalar degree of freedom might be accessible at colliders. Going beyond simplified models, such a theory can be described in a gauge-invariant and agnostic way via an EFT with a non-linearly realized electroweak symmetry. In this extended HEFT, depending on the $SU(2)$ nature of the new scalar in the UV, operators will be suppressed by different powers of a heavy mass scale. We use dimensional analysis to systematically evaluate expected hierarchies between Wilson coefficients, leading to structural relations between potential LHC observables, such as di-boson resonances, tau pair production or the di-photon channel. Once future collider data reveals a hint of a new scalar field, it can be fitted to this extended HEFT and such a structural analysis will help interpret it with respect to possible UV models, circumventing the need to individually test each possible model on the data or to fix the $SU(2)$ representation of the new scalar beforehand. For illustration, the framework is applied to the tentative 95 GeV resonance. In addition to its usefulness for collider physics, the extended HEFT can also be beneficial for low-energy observables, allowing to describe new scalars in an agnostic way. | [üîó Paper](https://arxiv.org/abs/2512.11764v1) |
| [The Relative Monadic Metalanguage](https://arxiv.org/abs/2512.11762v1) | Jack Liell-Cock, Zev Shirazi, Sam Staton | 2025-12-12 | General AI | Relative monads provide a controlled view of computation. We generalise the monadic metalanguage to a relative setting and give a complete semantics with strong relative monads. Adopting this perspective, we generalise two existing program calculi from the literature. We provide a linear-non-linear language for graded monads, LNL-RMM, along with a semantic proof that it is a conservative extension of the graded monadic metalanguage. Additionally, we provide a complete semantics for the arrow calculus, showing it is a restricted relative monadic metalanguage. This motivates the introduction of ARMM, a computational lambda calculus-style language for arrows that conservatively extends the arrow calculus. | [üîó Paper](https://arxiv.org/abs/2512.11762v1) |
| [dS/CFT from Defect](https://arxiv.org/abs/2512.11759v1) | Xing Huang, Chen-Te Ma | 2025-12-12 | General AI | We perform a Wick rotation and analytic continuation from global AdS$_{d+1}$ to static dS$_{d+1}$, yielding CFT$_d$ generators with a nonstandard adjoint action tied to dS bulk coordinates. To reproduce the real-scalar two-point function, we introduce a global defect operator that twists the inner product. We further show that $PT$ symmetry is spontaneously broken in CFT$_2$ vacua with a central charge having an imaginary part. Finally, we derive integral identities for bulk and defect correlators, providing a unified framework for computing CFT$_d$ observables in the presence of global and local defects. | [üîó Paper](https://arxiv.org/abs/2512.11759v1) |
| [The Effect of a Non-universal Extinction Curve on the Wesenheit Function and Cepheid Distances](https://arxiv.org/abs/2512.11758v1) | D. M. Skowron, M. L. Fouesneau, R. Drimmel, S. Khanna | 2025-12-12 | General AI | The Wesenheit function is widely used to reduce the effects of interstellar reddening in distance measurements. Its construction, however, relies on the assumption of a universal extinction curve and on fixed values of the total-to-selective extinction ratio, Rv. Recent studies have shown that Rv varies significantly across the Milky Way and between different galaxies, raising concerns about systematic biases in Wesenheit magnitudes and period-Wesenheit relations. In this work, we discuss the impact of non-universal extinction on Wesenheit indices by combining the Rv-dependent extinction curve with a grid of stellar atmosphere models. We compute the integrated extinction in optical and near-infrared passbands, derive Rv-dependent R coefficients for multiple Wesenheit indices, and examine how changes in Rv propagate into Wesenheit magnitudes and Cepheid distances in our Galaxy. We find that the R coefficients in the Wesenheit functions vary strongly with Rv. For classical Cepheids in the Milky Way disk, variations of Rv within the typical observed range (2.6-3.6) can lead to substantial differences in the Wesenheit function, reaching +-0.7 mag from the mean for the Gaia-based Wesenheit index W_G and resulting in distance errors of almost 40%. Near-infrared Wesenheit indices are much less sensitive to Rv changes. Our results clearly show that accounting for variable Rv is essential when applying period-Wesenheit relations, particularly in the optical regime, or that near or mid infrared based distances should be used. While we present this effect for classical Cepheids, it applies to all pulsating stars for which period-Wesenheit relations are used to infer distances. | [üîó Paper](https://arxiv.org/abs/2512.11758v1) |
| [Modular Witten Diagrams and Quantum Extremality](https://arxiv.org/abs/2512.11754v1) | Abhirup Bhattacharya, Onkar Parrikar | 2025-12-12 | General AI | We study entanglement entropy for ball-shaped regions in excited states of holographic conformal field theories. The excited states are prepared by the Euclidean path integral in the CFT with a source turned on for some double-trace operator, with a small, $O(1)$ amplitude $Œª$. On the gravity side, the double-trace operator deforms the bulk geometry as well as the entanglement structure of the state of bulk matter fields. By the quantum extremal surface formula, this leads to a deformation of the shape of the entanglement wedge, an effect which becomes manifest in the entanglement entropy at $O(Œª^2 G_N)$. On the CFT side, we explicitly calculate the entanglement entropy perturbatively in the source amplitude to $O(Œª^2)$, in terms of modular-flowed correlation functions of double-trace operators. We then evaluate these modular-flowed correlation functions using Witten diagrams. This calculation involves a Schwinger-Keldysh contour ordering prescription in the bulk, which we motivate using analytic continuation from Euclidean replica correlators. Focusing on a particular graviton-exchange diagram, we rewrite it in a form where it manifestly reproduces the canonical energy term present in the quantum Ryu-Takayanagi formula, including the shape deformation of the entanglement wedge due to backreaction and quantum effects. | [üîó Paper](https://arxiv.org/abs/2512.11754v1) |
| [Targeted cooling of urban cycling networks for heat-resilient mobility](https://arxiv.org/abs/2512.11753v1) | Agustin Cabrera, David Ziegler, Markus Schl√§pfer | 2025-12-12 | General AI | Cities are increasingly challenged by extreme heat events, which pose serious risks to public health and urban livability. Micromobility users, whose numbers have increased rapidly in recent years, are particularly vulnerable to outdoor heat exposure. Yet, their exposure patterns and the effectiveness of mitigation measures remain poorly understood. Here, we couple a high-resolution urban microclimate model (WRF--BEP--SOLWEIG) with 4.76 million Citi Bike trips in New York City to quantify cyclists' thermal exposure during the June 2024 heatwave and to evaluate targeted cooling strategies. Results show that a small fraction of the street network concentrates the majority of rider heat exposure, and that localized interventions along these segments yield the greatest benefits. Targeted tree planting along just 1.5% of the city's street network reduces total heat-exposed kilometers ridden by 19%, equivalent to a thermal stress reduction of about 4¬∞C, with its impact maximized during midday hours. In contrast, randomized citywide tree planting produces diffuse, resource-intensive cooling, highlighting the superior efficiency of spatially prioritized interventions. Baseline results further indicate that daytime heat stress is higher in lower-income neighborhoods, adding an important social dimension of urban heat exposure. Together, these findings provide a quantitative basis for designing heat-resilient and equitable cycling networks in a warming climate. | [üîó Paper](https://arxiv.org/abs/2512.11753v1) |
| [Forest Kernel Balancing Weights: Outcome-Guided Features for Causal Inference](https://arxiv.org/abs/2512.11751v1) | Andy A. Shen, Eli Ben-Michael, Avi Feller, Luke Keele, Jared Murray | 2025-12-12 | General AI | While balancing covariates between groups is central for observational causal inference, selecting which features to balance remains a challenging problem. Kernel balancing is a promising approach that first estimates a kernel that captures similarity across units and then balances a (possibly low-dimensional) summary of that kernel, indirectly learning important features to balance. In this paper, we propose forest kernel balancing, which leverages the underappreciated fact that tree-based machine learning models, namely random forests and Bayesian additive regression trees (BART), implicitly estimate a kernel based on the co-occurrence of observations in the same terminal leaf node. Thus, even though the resulting kernel is solely a function of baseline features, the selected nonlinearities and other interactions are important for predicting the outcome -- and therefore are important for addressing confounding. Through simulations and applied illustrations, we show that forest kernel balancing leads to meaningful computational and statistical improvement relative to standard kernel methods, which do not incorporate outcome information when learning features. | [üîó Paper](https://arxiv.org/abs/2512.11751v1) |
