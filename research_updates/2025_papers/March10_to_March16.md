# üìå AI Research Papers (March10 to March16)

## üîπ LLM

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Siege: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search](http://arxiv.org/abs/2503.10619v1) | Andy Zhou | 2025-03-13 | LLM | We introduce Siege, a multi-turn adversarial framework that models the gradual erosion of Large Language Model (LLM) safety through a tree search perspective. Unlike single-turn jailbreaks that rely on one meticulously engineered prompt, Siege expands the conversation at each turn in a breadth-first fashion, branching out multiple adversarial prompts that exploit partial compliance from previous responses. By tracking these incremental policy leaks and re-injecting them into subsequent queries, Siege reveals how minor concessions can accumulate into fully disallowed outputs. Evaluations on the JailbreakBench dataset show that Siege achieves a 100% success rate on GPT-3.5-turbo and 97% on GPT-4 in a single multi-turn run, using fewer queries than baselines such as Crescendo or GOAT. This tree search methodology offers an in-depth view of how model safeguards degrade over successive dialogue turns, underscoring the urgency of robust multi-turn testing procedures for language models. | [üîó Paper](http://arxiv.org/abs/2503.10619v1) |
| [OVTR: End-to-End Open-Vocabulary Multiple Object Tracking with Transformer](http://arxiv.org/abs/2503.10616v1) | Jinyang Li, En Yu, Sijia Chen, Wenbing Tao | 2025-03-13 | LLM, Multimodal AI | Open-vocabulary multiple object tracking aims to generalize trackers to unseen categories during training, enabling their application across a variety of real-world scenarios. However, the existing open-vocabulary tracker is constrained by its framework structure, isolated frame-level perception, and insufficient modal interactions, which hinder its performance in open-vocabulary classification and tracking. In this paper, we propose OVTR (End-to-End Open-Vocabulary Multiple Object Tracking with TRansformer), the first end-to-end open-vocabulary tracker that models motion, appearance, and category simultaneously. To achieve stable classification and continuous tracking, we design the CIP (Category Information Propagation) strategy, which establishes multiple high-level category information priors for subsequent frames. Additionally, we introduce a dual-branch structure for generalization capability and deep multimodal interaction, and incorporate protective strategies in the decoder to enhance performance. Experimental results show that our method surpasses previous trackers on the open-vocabulary MOT benchmark while also achieving faster inference speeds and significantly reducing preprocessing requirements. Moreover, the experiment transferring the model to another dataset demonstrates its strong adaptability. Models and code are released at https://github.com/jinyanglii/OVTR. | [üîó Paper](http://arxiv.org/abs/2503.10616v1) |
## üîπ Diffusion Models

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Studying Classifier(-Free) Guidance From a Classifier-Centric Perspective](http://arxiv.org/abs/2503.10638v1) | Xiaoming Zhao, Alexander G. Schwing | 2025-03-13 | Diffusion Models | Classifier-free guidance has become a staple for conditional generation with denoising diffusion models. However, a comprehensive understanding of classifier-free guidance is still missing. In this work, we carry out an empirical study to provide a fresh perspective on classifier-free guidance. Concretely, instead of solely focusing on classifier-free guidance, we trace back to the root, i.e., classifier guidance, pinpoint the key assumption for the derivation, and conduct a systematic study to understand the role of the classifier. We find that both classifier guidance and classifier-free guidance achieve conditional generation by pushing the denoising diffusion trajectories away from decision boundaries, i.e., areas where conditional information is usually entangled and is hard to learn. Based on this classifier-centric understanding, we propose a generic postprocessing step built upon flow-matching to shrink the gap between the learned distribution for a pre-trained denoising diffusion model and the real data distribution, majorly around the decision boundaries. Experiments on various datasets verify the effectiveness of the proposed approach. | [üîó Paper](http://arxiv.org/abs/2503.10638v1) |
| [Distilling Diversity and Control in Diffusion Models](http://arxiv.org/abs/2503.10637v2) | Rohit Gandikota, David Bau | 2025-03-13 | Diffusion Models, Optimization | Distilled diffusion models suffer from a critical limitation: reduced sample diversity compared to their base counterparts. In this work, we uncover that despite this diversity loss, distilled models retain the fundamental concept representations of base models. We demonstrate control distillation - where control mechanisms like Concept Sliders and LoRAs trained on base models can be seamlessly transferred to distilled models and vice-versa, effectively distilling control without any retraining. This preservation of representational structure prompted our investigation into the mechanisms of diversity collapse during distillation. To understand how distillation affects diversity, we introduce Diffusion Target (DT) Visualization, an analysis and debugging tool that reveals how models predict final outputs at intermediate steps. Through DT-Visualization, we identify generation artifacts, inconsistencies, and demonstrate that initial diffusion timesteps disproportionately determine output diversity, while later steps primarily refine details. Based on these insights, we introduce diversity distillation - a hybrid inference approach that strategically employs the base model for only the first critical timestep before transitioning to the efficient distilled model. Our experiments demonstrate that this simple modification not only restores the diversity capabilities from base to distilled models but surprisingly exceeds it, while maintaining nearly the computational efficiency of distilled inference, all without requiring additional training or model modifications. Our code and data are available at https://distillation.baulab.info | [üîó Paper](http://arxiv.org/abs/2503.10637v2) |
| [V2Edit: Versatile Video Diffusion Editor for Videos and 3D Scenes](http://arxiv.org/abs/2503.10634v1) | Yanming Zhang, Jun-Kun Chen, Jipeng Lyu, Yu-Xiong Wang | 2025-03-13 | Diffusion Models, Multimodal AI | This paper introduces V$^2$Edit, a novel training-free framework for instruction-guided video and 3D scene editing. Addressing the critical challenge of balancing original content preservation with editing task fulfillment, our approach employs a progressive strategy that decomposes complex editing tasks into a sequence of simpler subtasks. Each subtask is controlled through three key synergistic mechanisms: the initial noise, noise added at each denoising step, and cross-attention maps between text prompts and video content. This ensures robust preservation of original video elements while effectively applying the desired edits. Beyond its native video editing capability, we extend V$^2$Edit to 3D scene editing via a "render-edit-reconstruct" process, enabling high-quality, 3D-consistent edits even for tasks involving substantial geometric changes such as object insertion. Extensive experiments demonstrate that our V$^2$Edit achieves high-quality and successful edits across various challenging video editing tasks and complex 3D scene editing tasks, thereby establishing state-of-the-art performance in both domains. | [üîó Paper](http://arxiv.org/abs/2503.10634v1) |
| [HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model](http://arxiv.org/abs/2503.10631v1) | Jiaming Liu, Hao Chen, Pengju An, Zhuoyang Liu, Renrui Zhang, Chenyang Gu, Xiaoqi Li, Ziyu Guo, Sixiang Chen, Mengzhen Liu, Chengkai Hou, Mengdi Zhao, KC alex Zhou, Pheng-Ann Heng, Shanghang Zhang | 2025-03-13 | Diffusion Models, LLM, Multimodal AI | Recent advancements in vision-language models (VLMs) for common-sense reasoning have led to the development of vision-language-action (VLA) models, enabling robots to perform generalized manipulation. Although existing autoregressive VLA methods leverage large-scale pretrained knowledge, they disrupt the continuity of actions. Meanwhile, some VLA methods incorporate an additional diffusion head to predict continuous actions, relying solely on VLM-extracted features, which limits their reasoning capabilities. In this paper, we introduce HybridVLA, a unified framework that seamlessly integrates the strengths of both autoregressive and diffusion policies within a single large language model, rather than simply connecting them. To bridge the generation gap, a collaborative training recipe is proposed that injects the diffusion modeling directly into the next-token prediction. With this recipe, we find that these two forms of action prediction not only reinforce each other but also exhibit varying performance across different tasks. Therefore, we design a collaborative action ensemble mechanism that adaptively fuses these two predictions, leading to more robust control. In experiments, HybridVLA outperforms previous state-of-the-art VLA methods across various simulation and real-world tasks, including both single-arm and dual-arm robots, while demonstrating stable manipulation in previously unseen configurations. | [üîó Paper](http://arxiv.org/abs/2503.10631v1) |
| [DiT-Air: Revisiting the Efficiency of Diffusion Model Architecture Design in Text to Image Generation](http://arxiv.org/abs/2503.10618v1) | Chen Chen, Rui Qian, Wenze Hu, Tsu-Jui Fu, Lezhi Li, Bowen Zhang, Alex Schwing, Wei Liu, Yinfei Yang | 2025-03-13 | Diffusion Models, Optimization | In this work, we empirically study Diffusion Transformers (DiTs) for text-to-image generation, focusing on architectural choices, text-conditioning strategies, and training protocols. We evaluate a range of DiT-based architectures--including PixArt-style and MMDiT variants--and compare them with a standard DiT variant which directly processes concatenated text and noise inputs. Surprisingly, our findings reveal that the performance of standard DiT is comparable with those specialized models, while demonstrating superior parameter-efficiency, especially when scaled up. Leveraging the layer-wise parameter sharing strategy, we achieve a further reduction of 66% in model size compared to an MMDiT architecture, with minimal performance impact. Building on an in-depth analysis of critical components such as text encoders and Variational Auto-Encoders (VAEs), we introduce DiT-Air and DiT-Air-Lite. With supervised and reward fine-tuning, DiT-Air achieves state-of-the-art performance on GenEval and T2I CompBench, while DiT-Air-Lite remains highly competitive, surpassing most existing models despite its compact size. | [üîó Paper](http://arxiv.org/abs/2503.10618v1) |
| [ConsisLoRA: Enhancing Content and Style Consistency for LoRA-based Style Transfer](http://arxiv.org/abs/2503.10614v1) | Bolin Chen, Baoquan Zhao, Haoran Xie, Yi Cai, Qing Li, Xudong Mao | 2025-03-13 | Diffusion Models, Optimization | Style transfer involves transferring the style from a reference image to the content of a target image. Recent advancements in LoRA-based (Low-Rank Adaptation) methods have shown promise in effectively capturing the style of a single image. However, these approaches still face significant challenges such as content inconsistency, style misalignment, and content leakage. In this paper, we comprehensively analyze the limitations of the standard diffusion parameterization, which learns to predict noise, in the context of style transfer. To address these issues, we introduce ConsisLoRA, a LoRA-based method that enhances both content and style consistency by optimizing the LoRA weights to predict the original image rather than noise. We also propose a two-step training strategy that decouples the learning of content and style from the reference image. To effectively capture both the global structure and local details of the content image, we introduce a stepwise loss transition strategy. Additionally, we present an inference guidance method that enables continuous control over content and style strengths during inference. Through both qualitative and quantitative evaluations, our method demonstrates significant improvements in content and style consistency while effectively reducing content leakage. | [üîó Paper](http://arxiv.org/abs/2503.10614v1) |
| [CoSTA$\ast$: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing](http://arxiv.org/abs/2503.10613v1) | Advait Gupta, NandaKiran Velaga, Dang Nguyen, Tianyi Zhou | 2025-03-13 | Diffusion Models, Multimodal AI | Text-to-image models like stable diffusion and DALLE-3 still struggle with multi-turn image editing. We decompose such a task as an agentic workflow (path) of tool use that addresses a sequence of subtasks by AI tools of varying costs. Conventional search algorithms require expensive exploration to find tool paths. While large language models (LLMs) possess prior knowledge of subtask planning, they may lack accurate estimations of capabilities and costs of tools to determine which to apply in each subtask. Can we combine the strengths of both LLMs and graph search to find cost-efficient tool paths? We propose a three-stage approach "CoSTA*" that leverages LLMs to create a subtask tree, which helps prune a graph of AI tools for the given task, and then conducts A* search on the small subgraph to find a tool path. To better balance the total cost and quality, CoSTA* combines both metrics of each tool on every subtask to guide the A* search. Each subtask's output is then evaluated by a vision-language model (VLM), where a failure will trigger an update of the tool's cost and quality on the subtask. Hence, the A* search can recover from failures quickly to explore other paths. Moreover, CoSTA* can automatically switch between modalities across subtasks for a better cost-quality trade-off. We build a novel benchmark of challenging multi-turn image editing, on which CoSTA* outperforms state-of-the-art image-editing models or agents in terms of both cost and quality, and performs versatile trade-offs upon user preference. | [üîó Paper](http://arxiv.org/abs/2503.10613v1) |
| [MuDG: Taming Multi-modal Diffusion with Gaussian Splatting for Urban Scene Reconstruction](http://arxiv.org/abs/2503.10604v1) | Yingshuang Zou, Yikang Ding, Chuanrui Zhang, Jiazhe Guo, Bohan Li, Xiaoyang Lyu, Feiyang Tan, Xiaojuan Qi, Haoqian Wang | 2025-03-13 | Diffusion Models, Multimodal AI, Model Evaluation, Responsible AI, Optimization | Recent breakthroughs in radiance fields have significantly advanced 3D scene reconstruction and novel view synthesis (NVS) in autonomous driving. Nevertheless, critical limitations persist: reconstruction-based methods exhibit substantial performance deterioration under significant viewpoint deviations from training trajectories, while generation-based techniques struggle with temporal coherence and precise scene controllability. To overcome these challenges, we present MuDG, an innovative framework that integrates Multi-modal Diffusion model with Gaussian Splatting (GS) for Urban Scene Reconstruction. MuDG leverages aggregated LiDAR point clouds with RGB and geometric priors to condition a multi-modal video diffusion model, synthesizing photorealistic RGB, depth, and semantic outputs for novel viewpoints. This synthesis pipeline enables feed-forward NVS without computationally intensive per-scene optimization, providing comprehensive supervision signals to refine 3DGS representations for rendering robustness enhancement under extreme viewpoint changes. Experiments on the Open Waymo Dataset demonstrate that MuDG outperforms existing methods in both reconstruction and synthesis quality. | [üîó Paper](http://arxiv.org/abs/2503.10604v1) |
| [CameraCtrl II: Dynamic Scene Exploration via Camera-controlled Video Diffusion Models](http://arxiv.org/abs/2503.10592v1) | Hao He, Ceyuan Yang, Shanchuan Lin, Yinghao Xu, Meng Wei, Liangke Gui, Qi Zhao, Gordon Wetzstein, Lu Jiang, Hongsheng Li | 2025-03-13 | Diffusion Models, Multimodal AI | This paper introduces CameraCtrl II, a framework that enables large-scale dynamic scene exploration through a camera-controlled video diffusion model. Previous camera-conditioned video generative models suffer from diminished video dynamics and limited range of viewpoints when generating videos with large camera movement. We take an approach that progressively expands the generation of dynamic scenes -- first enhancing dynamic content within individual video clip, then extending this capability to create seamless explorations across broad viewpoint ranges. Specifically, we construct a dataset featuring a large degree of dynamics with camera parameter annotations for training while designing a lightweight camera injection module and training scheme to preserve dynamics of the pretrained models. Building on these improved single-clip techniques, we enable extended scene exploration by allowing users to iteratively specify camera trajectories for generating coherent video sequences. Experiments across diverse scenarios demonstrate that CameraCtrl Ii enables camera-controlled dynamic scene synthesis with substantially wider spatial exploration than previous approaches. | [üîó Paper](http://arxiv.org/abs/2503.10592v1) |
## üîπ RLHF

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [UniGoal: Towards Universal Zero-shot Goal-oriented Navigation](http://arxiv.org/abs/2503.10630v1) | Hang Yin, Xiuwei Xu, Lingqing Zhao, Ziwei Wang, Jie Zhou, Jiwen Lu | 2025-03-13 | RLHF, Prompt Engineering | In this paper, we propose a general framework for universal zero-shot goal-oriented navigation. Existing zero-shot methods build inference framework upon large language models (LLM) for specific tasks, which differs a lot in overall pipeline and fails to generalize across different types of goal. Towards the aim of universal zero-shot navigation, we propose a uniform graph representation to unify different goals, including object category, instance image and text description. We also convert the observation of agent into an online maintained scene graph. With this consistent scene and goal representation, we preserve most structural information compared with pure text and are able to leverage LLM for explicit graph-based reasoning. Specifically, we conduct graph matching between the scene graph and goal graph at each time instant and propose different strategies to generate long-term goal of exploration according to different matching states. The agent first iteratively searches subgraph of goal when zero-matched. With partial matching, the agent then utilizes coordinate projection and anchor pair alignment to infer the goal location. Finally scene graph correction and goal verification are applied for perfect matching. We also present a blacklist mechanism to enable robust switch between stages. Extensive experiments on several benchmarks show that our UniGoal achieves state-of-the-art zero-shot performance on three studied navigation tasks with a single model, even outperforming task-specific zero-shot methods and supervised universal methods. | [üîó Paper](http://arxiv.org/abs/2503.10630v1) |
| [NIL: No-data Imitation Learning by Leveraging Pre-trained Video Diffusion Models](http://arxiv.org/abs/2503.10626v1) | Mert Albaba, Chenhao Li, Markos Diomataris, Omid Taheri, Andreas Krause, Michael Black | 2025-03-13 | RLHF, Diffusion Models, Multimodal AI | Acquiring physically plausible motor skills across diverse and unconventional morphologies-including humanoid robots, quadrupeds, and animals-is essential for advancing character simulation and robotics. Traditional methods, such as reinforcement learning (RL) are task- and body-specific, require extensive reward function engineering, and do not generalize well. Imitation learning offers an alternative but relies heavily on high-quality expert demonstrations, which are difficult to obtain for non-human morphologies. Video diffusion models, on the other hand, are capable of generating realistic videos of various morphologies, from humans to ants. Leveraging this capability, we propose a data-independent approach for skill acquisition that learns 3D motor skills from 2D-generated videos, with generalization capability to unconventional and non-human forms. Specifically, we guide the imitation learning process by leveraging vision transformers for video-based comparisons by calculating pair-wise distance between video embeddings. Along with video-encoding distance, we also use a computed similarity between segmented video frames as a guidance reward. We validate our method on locomotion tasks involving unique body configurations. In humanoid robot locomotion tasks, we demonstrate that 'No-data Imitation Learning' (NIL) outperforms baselines trained on 3D motion-capture data. Our results highlight the potential of leveraging generative video models for physically plausible skill learning with diverse morphologies, effectively replacing data collection with data generation for imitation learning. | [üîó Paper](http://arxiv.org/abs/2503.10626v1) |
| [Dual-Stage Cross-Modal Network with Dynamic Feature Fusion for Emotional Mimicry Intensity Estimation](http://arxiv.org/abs/2503.10603v2) | Jun Yu, Lingsi Zhu, Yanjun Chi, Yunxiang Zhang, Yang Zheng, Yongqi Wang, Xilong Lu | 2025-03-13 | RLHF, Fine-Tuning, Multimodal AI | Emotional Mimicry Intensity (EMI) estimation serves as a critical technology for understanding human social behavior and enhancing human-computer interaction experiences, where the core challenge lies in dynamic correlation modeling and robust fusion of multimodal temporal signals. To address the limitations of existing methods in insufficient exploitation of modal synergistic effects, noise sensitivity, and limited fine-grained alignment capabilities, this paper proposes a dual-stage cross-modal alignment framework. First, we construct vision-text and audio-text contrastive learning networks based on an improved CLIP architecture, achieving preliminary alignment in the feature space through modality-decoupled pre-training. Subsequently, we design a temporal-aware dynamic fusion module that combines Temporal Convolutional Networks (TCN) and gated bidirectional LSTM to respectively capture the macro-evolution patterns of facial expressions and local dynamics of acoustic features. Innovatively, we introduce a quality-guided modality fusion strategy that enables modality compensation under occlusion and noisy scenarios through differentiable weight allocation. Experimental results on the Hume-Vidmimic2 dataset demonstrate that our method achieves an average Pearson correlation coefficient of 0.35 across six emotion dimensions, outperforming the best baseline by 40\%. Ablation studies further validate the effectiveness of the dual-stage training strategy and dynamic fusion mechanism, providing a novel technical pathway for fine-grained emotion analysis in open environments. | [üîó Paper](http://arxiv.org/abs/2503.10603v2) |
## üîπ Multimodal AI

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Uncertainty in Action: Confidence Elicitation in Embodied Agents](http://arxiv.org/abs/2503.10628v1) | Tianjiao Yu, Vedant Shah, Muntasir Wahed, Kiet A. Nguyen, Adheesh Juvekar, Tal August, Ismini Lourentzou | 2025-03-13 | Multimodal AI | Expressing confidence is challenging for embodied agents navigating dynamic multimodal environments, where uncertainty arises from both perception and decision-making processes. We present the first work investigating embodied confidence elicitation in open-ended multimodal environments. We introduce Elicitation Policies, which structure confidence assessment across inductive, deductive, and abductive reasoning, along with Execution Policies, which enhance confidence calibration through scenario reinterpretation, action sampling, and hypothetical reasoning. Evaluating agents in calibration and failure prediction tasks within the Minecraft environment, we show that structured reasoning approaches, such as Chain-of-Thoughts, improve confidence calibration. However, our findings also reveal persistent challenges in distinguishing uncertainty, particularly under abductive settings, underscoring the need for more sophisticated embodied confidence elicitation methods. | [üîó Paper](http://arxiv.org/abs/2503.10628v1) |
| [DriveLMM-o1: A Step-by-Step Reasoning Dataset and Large Multimodal Model for Driving Scenario Understanding](http://arxiv.org/abs/2503.10621v1) | Ayesha Ishaq, Jean Lahoud, Ketan More, Omkar Thawakar, Ritesh Thawkar, Dinura Dissanayake, Noor Ahsan, Yuhao Li, Fahad Shahbaz Khan, Hisham Cholakkal, Ivan Laptev, Rao Muhammad Anwer, Salman Khan | 2025-03-13 | Multimodal AI | While large multimodal models (LMMs) have demonstrated strong performance across various Visual Question Answering (VQA) tasks, certain challenges require complex multi-step reasoning to reach accurate answers. One particularly challenging task is autonomous driving, which demands thorough cognitive processing before decisions can be made. In this domain, a sequential and interpretive understanding of visual cues is essential for effective perception, prediction, and planning. Nevertheless, common VQA benchmarks often focus on the accuracy of the final answer while overlooking the reasoning process that enables the generation of accurate responses. Moreover, existing methods lack a comprehensive framework for evaluating step-by-step reasoning in realistic driving scenarios. To address this gap, we propose DriveLMM-o1, a new dataset and benchmark specifically designed to advance step-wise visual reasoning for autonomous driving. Our benchmark features over 18k VQA examples in the training set and more than 4k in the test set, covering diverse questions on perception, prediction, and planning, each enriched with step-by-step reasoning to ensure logical inference in autonomous driving scenarios. We further introduce a large multimodal model that is fine-tuned on our reasoning dataset, demonstrating robust performance in complex driving scenarios. In addition, we benchmark various open-source and closed-source methods on our proposed dataset, systematically comparing their reasoning capabilities for autonomous driving tasks. Our model achieves a +7.49% gain in final answer accuracy, along with a 3.62% improvement in reasoning score over the previous best open-source model. Our framework, dataset, and model are available at https://github.com/ayesha-ishaq/DriveLMM-o1. | [üîó Paper](http://arxiv.org/abs/2503.10621v1) |
| [R1-Onevision: Advancing Generalized Multimodal Reasoning through Cross-Modal Formalization](http://arxiv.org/abs/2503.10615v1) | Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, Bo Zhang, Wei Chen | 2025-03-13 | Multimodal AI, Fine-Tuning, Optimization, LLM, RLHF | Large Language Models have demonstrated remarkable reasoning capability in complex textual tasks. However, multimodal reasoning, which requires integrating visual and textual information, remains a significant challenge. Existing visual-language models often struggle to effectively analyze and reason visual content, resulting in suboptimal performance on complex reasoning tasks. Moreover, the absence of comprehensive benchmarks hinders the accurate assessment of multimodal reasoning capabilities. In this paper, we introduce R1-Onevision, a multimodal reasoning model designed to bridge the gap between visual perception and deep reasoning. To achieve this, we propose a cross-modal reasoning pipeline that transforms images into formal textural representations, enabling precise language-based reasoning. Leveraging this pipeline, we construct the R1-Onevision dataset which provides detailed, step-by-step multimodal reasoning annotations across diverse domains. We further develop the R1-Onevision model through supervised fine-tuning and reinforcement learning to cultivate advanced reasoning and robust generalization abilities. To comprehensively evaluate multimodal reasoning performance across different grades, we introduce R1-Onevision-Bench, a benchmark aligned with human educational stages, covering exams from junior high school to university and beyond. Experimental results show that R1-Onevision achieves state-of-the-art performance, outperforming models such as GPT-4o and Qwen2.5-VL on multiple challenging multimodal reasoning benchmarks. | [üîó Paper](http://arxiv.org/abs/2503.10615v1) |
## üîπ Optimization

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90% Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1](http://arxiv.org/abs/2503.10635v1) | Zhaoyi Li, Xiaohan Zhao, Dong-Dong Wu, Jiacheng Cui, Zhiqiang Shen | 2025-03-13 | Optimization, LLM, Multimodal AI | Despite promising performance on open-source large vision-language models (LVLMs), transfer-based targeted attacks often fail against black-box commercial LVLMs. Analyzing failed adversarial perturbations reveals that the learned perturbations typically originate from a uniform distribution and lack clear semantic details, resulting in unintended responses. This critical absence of semantic information leads commercial LVLMs to either ignore the perturbation entirely or misinterpret its embedded semantics, thereby causing the attack to fail. To overcome these issues, we notice that identifying core semantic objects is a key objective for models trained with various datasets and methodologies. This insight motivates our approach that refines semantic clarity by encoding explicit semantic details within local regions, thus ensuring interoperability and capturing finer-grained features, and by concentrating modifications on semantically rich areas rather than applying them uniformly. To achieve this, we propose a simple yet highly effective solution: at each optimization step, the adversarial image is cropped randomly by a controlled aspect ratio and scale, resized, and then aligned with the target image in the embedding space. Experimental results confirm our hypothesis. Our adversarial examples crafted with local-aggregated perturbations focused on crucial regions exhibit surprisingly good transferability to commercial LVLMs, including GPT-4.5, GPT-4o, Gemini-2.0-flash, Claude-3.5-sonnet, Claude-3.7-sonnet, and even reasoning models like o1, Claude-3.7-thinking and Gemini-2.0-flash-thinking. Our approach achieves success rates exceeding 90% on GPT-4.5, 4o, and o1, significantly outperforming all prior state-of-the-art attack methods. Our optimized adversarial examples under different configurations and training code are available at https://github.com/VILA-Lab/M-Attack. | [üîó Paper](http://arxiv.org/abs/2503.10635v1) |
| [LHM: Large Animatable Human Reconstruction Model from a Single Image in Seconds](http://arxiv.org/abs/2503.10625v1) | Lingteng Qiu, Xiaodong Gu, Peihao Li, Qi Zuo, Weichao Shen, Junfei Zhang, Kejie Qiu, Weihao Yuan, Guanying Chen, Zilong Dong, Liefeng Bo | 2025-03-13 | Optimization, LLM, Multimodal AI | Animatable 3D human reconstruction from a single image is a challenging problem due to the ambiguity in decoupling geometry, appearance, and deformation. Recent advances in 3D human reconstruction mainly focus on static human modeling, and the reliance of using synthetic 3D scans for training limits their generalization ability. Conversely, optimization-based video methods achieve higher fidelity but demand controlled capture conditions and computationally intensive refinement processes. Motivated by the emergence of large reconstruction models for efficient static reconstruction, we propose LHM (Large Animatable Human Reconstruction Model) to infer high-fidelity avatars represented as 3D Gaussian splatting in a feed-forward pass. Our model leverages a multimodal transformer architecture to effectively encode the human body positional features and image features with attention mechanism, enabling detailed preservation of clothing geometry and texture. To further boost the face identity preservation and fine detail recovery, we propose a head feature pyramid encoding scheme to aggregate multi-scale features of the head regions. Extensive experiments demonstrate that our LHM generates plausible animatable human in seconds without post-processing for face and hands, outperforming existing methods in both reconstruction accuracy and generalization ability. | [üîó Paper](http://arxiv.org/abs/2503.10625v1) |
| [Compositional Subspace Representation Fine-tuning for Adaptive Large Language Models](http://arxiv.org/abs/2503.10617v1) | Andy Zhou | 2025-03-13 | Optimization, LLM | Adapting large language models to multiple tasks can cause cross-skill interference, where improvements for one skill degrade another. While methods such as LoRA impose orthogonality constraints at the weight level, they do not fully address interference in hidden-state representations. We propose Compositional Subspace Representation Fine-tuning (CS-ReFT), a novel representation-based approach that learns multiple orthonormal subspace transformations, each specializing in a distinct skill, and composes them via a lightweight router. By isolating these subspace edits in the hidden state, rather than weight matrices, CS-ReFT prevents cross-task conflicts more effectively. On the AlpacaEval benchmark, applying CS-ReFT to Llama-2-7B achieves a 93.94% win rate, surpassing GPT-3.5 Turbo (86.30%) while requiring only 0.0098% of model parameters. These findings show that specialized representation edits, composed via a simple router, significantly enhance multi-task instruction following with minimal overhead. | [üîó Paper](http://arxiv.org/abs/2503.10617v1) |
## üîπ Scaling Laws

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [GroundingSuite: Measuring Complex Multi-Granular Pixel Grounding](http://arxiv.org/abs/2503.10596v1) | Rui Hu, Lianghui Zhu, Yuxuan Zhang, Tianheng Cheng, Lei Liu, Heng Liu, Longjin Ran, Xiaoxin Chen, Wenyu Liu, Xinggang Wang | 2025-03-13 | Scaling Laws, Training & Evaluation, Multimodal AI | Pixel grounding, encompassing tasks such as Referring Expression Segmentation (RES), has garnered considerable attention due to its immense potential for bridging the gap between vision and language modalities. However, advancements in this domain are currently constrained by limitations inherent in existing datasets, including limited object categories, insufficient textual diversity, and a scarcity of high-quality annotations. To mitigate these limitations, we introduce GroundingSuite, which comprises: (1) an automated data annotation framework leveraging multiple Vision-Language Model (VLM) agents; (2) a large-scale training dataset encompassing 9.56 million diverse referring expressions and their corresponding segmentations; and (3) a meticulously curated evaluation benchmark consisting of 3,800 images. The GroundingSuite training dataset facilitates substantial performance improvements, enabling models trained on it to achieve state-of-the-art results. Specifically, a cIoU of 68.9 on gRefCOCO and a gIoU of 55.3 on RefCOCOm. Moreover, the GroundingSuite annotation framework demonstrates superior efficiency compared to the current leading data annotation method, i.e., $4.5 \times$ faster than the GLaMM. | [üîó Paper](http://arxiv.org/abs/2503.10596v1) |
## üîπ Training & Evaluation

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [The Curse of Conditions: Analyzing and Improving Optimal Transport for Conditional Flow-Based Generation](http://arxiv.org/abs/2503.10636v2) | Ho Kei Cheng, Alexander Schwing | 2025-03-13 | Training & Evaluation | Minibatch optimal transport coupling straightens paths in unconditional flow matching. This leads to computationally less demanding inference as fewer integration steps and less complex numerical solvers can be employed when numerically solving an ordinary differential equation at test time. However, in the conditional setting, minibatch optimal transport falls short. This is because the default optimal transport mapping disregards conditions, resulting in a conditionally skewed prior distribution during training. In contrast, at test time, we have no access to the skewed prior, and instead sample from the full, unbiased prior distribution. This gap between training and testing leads to a subpar performance. To bridge this gap, we propose conditional optimal transport C^2OT that adds a conditional weighting term in the cost matrix when computing the optimal transport assignment. Experiments demonstrate that this simple fix works with both discrete and continuous conditions in 8gaussians-to-moons, CIFAR-10, ImageNet-32x32, and ImageNet-256x256. Our method performs better overall compared to the existing baselines across different function evaluation budgets. Code is available at https://hkchengrex.github.io/C2OT | [üîó Paper](http://arxiv.org/abs/2503.10636v2) |
| [Hierarchical Self-Supervised Adversarial Training for Robust Vision Models in Histopathology](http://arxiv.org/abs/2503.10629v1) | Hashmat Shadab Malik, Shahina Kunhimon, Muzammal Naseer, Fahad Shahbaz Khan, Salman Khan | 2025-03-13 | Training & Evaluation, Model Evaluation, Responsible AI, Fine-Tuning, Security & Adversarial ML, AI Safety | Adversarial attacks pose significant challenges for vision models in critical fields like healthcare, where reliability is essential. Although adversarial training has been well studied in natural images, its application to biomedical and microscopy data remains limited. Existing self-supervised adversarial training methods overlook the hierarchical structure of histopathology images, where patient-slide-patch relationships provide valuable discriminative signals. To address this, we propose Hierarchical Self-Supervised Adversarial Training (HSAT), which exploits these properties to craft adversarial examples using multi-level contrastive learning and integrate it into adversarial training for enhanced robustness. We evaluate HSAT on multiclass histopathology dataset OpenSRH and the results show that HSAT outperforms existing methods from both biomedical and natural image domains. HSAT enhances robustness, achieving an average gain of 54.31% in the white-box setting and reducing performance drops to 3-4% in the black-box setting, compared to 25-30% for the baseline. These results set a new benchmark for adversarial training in this domain, paving the way for more robust models. Our Code for training and evaluation is available at https://github.com/HashmatShadab/HSAT. | [üîó Paper](http://arxiv.org/abs/2503.10629v1) |
## üîπ Model Evaluation

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Knot reconstruction of the scalar primordial power spectrum with Planck, ACT, and SPT CMB data](http://arxiv.org/abs/2503.10609v1) | Antonio Raffaelli, Mario Ballardini | 2025-03-13 | Model Evaluation, Responsible AI | We investigate a non-parametric Bayesian method for reconstructing the primordial power spectrum (PPS) of scalar perturbations using temperature and polarisation data from the {\em Planck}, ACT, and SPT CMB experiments. This reconstruction method is based on linear splines for the PPS between nodes in $k$-space whose amplitudes and positions are allowed to vary. All three data sets consistently show no significant deviations from a power-law form in the range $0.005 \lesssim k\,\mathrm{Mpc} \lesssim 0.16$ independent of the number of knots adopted to perform the reconstruction. The addition of high-resolution CMB measurements from ACT and SPT slightly improves the range of scales of the scalar PPS which are well constrained around a power law up to $k \simeq 0.25\,\mathrm{Mpc}^{-1}$ and $k \simeq 0.2\,\mathrm{Mpc}^{-1}$, respectively. At large scales, a potential oscillatory feature in the primordial power spectrum appears when we consider six or more nodes. We test the robustness of the methodology and our results by varying the detailed number of knots from $N=2$ to $N=10$. We have used the reconstructed scalar PPS to derive several quantities related to inflationary related to inflationary dynamics, such as the effective scalar spectral index, which describes the dependence of the PPS on the scales and parameters associated with the effective field theory of inflation, to provide information on possible departures from the standard single-field canonical case. Finally, we investigate whether the excess of smoothing in the region of the region of the acoustic peaks of the CMB anisotropy temperature power spectrum in the \textit{Planck} PR3 data is degenerate with our reconstructions of the PPS, but find no significant correlation between them. | [üîó Paper](http://arxiv.org/abs/2503.10609v1) |
## üîπ Prompt Engineering

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [GoT: Unleashing Reasoning Capability of Multimodal Large Language Model for Visual Generation and Editing](http://arxiv.org/abs/2503.10639v1) | Rongyao Fang, Chengqi Duan, Kun Wang, Linjiang Huang, Hao Li, Shilin Yan, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, Xihui Liu, Hongsheng Li | 2025-03-13 | Prompt Engineering, Diffusion Models, LLM, Multimodal AI | Current image generation and editing methods primarily process textual prompts as direct inputs without reasoning about visual composition and explicit operations. We present Generation Chain-of-Thought (GoT), a novel paradigm that enables generation and editing through an explicit language reasoning process before outputting images. This approach transforms conventional text-to-image generation and editing into a reasoning-guided framework that analyzes semantic relationships and spatial arrangements. We define the formulation of GoT and construct large-scale GoT datasets containing over 9M samples with detailed reasoning chains capturing semantic-spatial relationships. To leverage the advantages of GoT, we implement a unified framework that integrates Qwen2.5-VL for reasoning chain generation with an end-to-end diffusion model enhanced by our novel Semantic-Spatial Guidance Module. Experiments show our GoT framework achieves excellent performance on both generation and editing tasks, with significant improvements over baselines. Additionally, our approach enables interactive visual generation, allowing users to explicitly modify reasoning steps for precise image adjustments. GoT pioneers a new direction for reasoning-driven visual generation and editing, producing images that better align with human intent. To facilitate future research, we make our datasets, code, and pretrained models publicly available at https://github.com/rongyaofang/GoT. | [üîó Paper](http://arxiv.org/abs/2503.10639v1) |
| [SciVerse: Unveiling the Knowledge Comprehension and Visual Reasoning of LMMs on Multi-modal Scientific Problems](http://arxiv.org/abs/2503.10627v1) | Ziyu Guo, Ray Zhang, Hao Chen, Jialin Gao, Dongzhi Jiang, Jiaze Wang, Pheng-Ann Heng | 2025-03-13 | Prompt Engineering, Training & Evaluation | The rapid advancement of Large Multi-modal Models (LMMs) has enabled their application in scientific problem-solving, yet their fine-grained capabilities remain under-explored. In this paper, we introduce SciVerse, a multi-modal scientific evaluation benchmark to thoroughly assess LMMs across 5,735 test instances in five distinct versions. We aim to investigate three key dimensions of LMMs: scientific knowledge comprehension, multi-modal content interpretation, and Chain-of-Thought (CoT) reasoning. To unveil whether LMMs possess sufficient scientific expertise, we first transform each problem into three versions containing different levels of knowledge required for solving, i.e., Knowledge-free, -lite, and -rich. Then, to explore how LMMs interpret multi-modal scientific content, we annotate another two versions, i.e., Vision-rich and -only, marking more question information from texts to diagrams. Comparing the results of different versions, SciVerse systematically examines the professional knowledge stock and visual perception skills of LMMs in scientific domains. In addition, to rigorously assess CoT reasoning, we propose a new scientific CoT evaluation strategy, conducting a step-wise assessment on knowledge and logical errors in model outputs. Our extensive evaluation of different LMMs on SciVerse reveals critical limitations in their scientific proficiency and provides new insights into future developments. Project page: https://sciverse-cuhk.github.io | [üîó Paper](http://arxiv.org/abs/2503.10627v1) |
## üîπ Ongoing Learning

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision Transformers?](http://arxiv.org/abs/2503.10632v1) | Subhajit Maity, Killian Hitsman, Xin Li, Aritra Dutta | 2025-03-13 | Ongoing Learning | Kolmogorov-Arnold networks (KANs) are a remarkable innovation consisting of learnable activation functions with the potential to capture more complex relationships from data. Although KANs are useful in finding symbolic representations and continual learning of one-dimensional functions, their effectiveness in diverse machine learning (ML) tasks, such as vision, remains questionable. Presently, KANs are deployed by replacing multilayer perceptrons (MLPs) in deep network architectures, including advanced architectures such as vision Transformers (ViTs). In this paper, we are the first to design a general learnable Kolmogorov-Arnold Attention (KArAt) for vanilla ViTs that can operate on any choice of basis. However, the computing and memory costs of training them motivated us to propose a more modular version, and we designed particular learnable attention, called Fourier-KArAt. Fourier-KArAt and its variants either outperform their ViT counterparts or show comparable performance on CIFAR-10, CIFAR-100, and ImageNet-1K datasets. We dissect these architectures' performance and generalization capacity by analyzing their loss landscapes, weight distributions, optimizer path, attention visualization, and spectral behavior, and contrast them with vanilla ViTs. The goal of this paper is not to produce parameter- and compute-efficient attention, but to encourage the community to explore KANs in conjunction with more advanced architectures that require a careful understanding of learnable activations. Our open-source code and implementation details are available on: https://subhajitmaity.me/KArAt | [üîó Paper](http://arxiv.org/abs/2503.10632v1) |
| [ETCH: Generalizing Body Fitting to Clothed Humans via Equivariant Tightness](http://arxiv.org/abs/2503.10624v1) | Boqian Li, Haiwen Feng, Zeyu Cai, Michael J. Black, Yuliang Xiu | 2025-03-13 | Ongoing Learning, Optimization | Fitting a body to a 3D clothed human point cloud is a common yet challenging task. Traditional optimization-based approaches use multi-stage pipelines that are sensitive to pose initialization, while recent learning-based methods often struggle with generalization across diverse poses and garment types. We propose Equivariant Tightness Fitting for Clothed Humans, or ETCH, a novel pipeline that estimates cloth-to-body surface mapping through locally approximate SE(3) equivariance, encoding tightness as displacement vectors from the cloth surface to the underlying body. Following this mapping, pose-invariant body features regress sparse body markers, simplifying clothed human fitting into an inner-body marker fitting task. Extensive experiments on CAPE and 4D-Dress show that ETCH significantly outperforms state-of-the-art methods -- both tightness-agnostic and tightness-aware -- in body fitting accuracy on loose clothing (16.7% ~ 69.5%) and shape accuracy (average 49.9%). Our equivariant tightness design can even reduce directional errors by (67.2% ~ 89.8%) in one-shot (or out-of-distribution) settings. Qualitative results demonstrate strong generalization of ETCH, regardless of challenging poses, unseen shapes, loose clothing, and non-rigid dynamics. We will release the code and models soon for research purposes at https://boqian-li.github.io/ETCH/. | [üîó Paper](http://arxiv.org/abs/2503.10624v1) |
| [OCCUQ: Exploring Efficient Uncertainty Quantification for 3D Occupancy Prediction](http://arxiv.org/abs/2503.10605v1) | Severin Heidrich, Till Beemelmanns, Alexey Nekrasov, Bastian Leibe, Lutz Eckstein | 2025-03-13 | Ongoing Learning, Model Evaluation, Training & Evaluation, Responsible AI | Autonomous driving has the potential to significantly enhance productivity and provide numerous societal benefits. Ensuring robustness in these safety-critical systems is essential, particularly when vehicles must navigate adverse weather conditions and sensor corruptions that may not have been encountered during training. Current methods often overlook uncertainties arising from adversarial conditions or distributional shifts, limiting their real-world applicability. We propose an efficient adaptation of an uncertainty estimation technique for 3D occupancy prediction. Our method dynamically calibrates model confidence using epistemic uncertainty estimates. Our evaluation under various camera corruption scenarios, such as fog or missing cameras, demonstrates that our approach effectively quantifies epistemic uncertainty by assigning higher uncertainty values to unseen data. We introduce region-specific corruptions to simulate defects affecting only a single camera and validate our findings through both scene-level and region-level assessments. Our results show superior performance in Out-of-Distribution (OoD) detection and confidence calibration compared to common baselines such as Deep Ensembles and MC-Dropout. Our approach consistently demonstrates reliable uncertainty measures, indicating its potential for enhancing the robustness of autonomous driving systems in real-world scenarios. Code and dataset are available at https://github.com/ika-rwth-aachen/OCCUQ . | [üîó Paper](http://arxiv.org/abs/2503.10605v1) |
## üîπ Responsible AI

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [TruthPrInt: Mitigating LVLM Object Hallucination Via Latent Truthful-Guided Pre-Intervention](http://arxiv.org/abs/2503.10602v1) | Jinhao Duan, Fei Kong, Hao Cheng, James Diffenderfer, Bhavya Kailkhura, Lichao Sun, Xiaofeng Zhu, Xiaoshuang Shi, Kaidi Xu | 2025-03-13 | Responsible AI, Model Evaluation, Multimodal AI | Object Hallucination (OH) has been acknowledged as one of the major trustworthy challenges in Large Vision-Language Models (LVLMs). Recent advancements in Large Language Models (LLMs) indicate that internal states, such as hidden states, encode the "overall truthfulness" of generated responses. However, it remains under-explored how internal states in LVLMs function and whether they could serve as "per-token" hallucination indicators, which is essential for mitigating OH. In this paper, we first conduct an in-depth exploration of LVLM internal states in relation to OH issues and discover that (1) LVLM internal states are high-specificity per-token indicators of hallucination behaviors. Moreover, (2) different LVLMs encode universal patterns of hallucinations in common latent subspaces, indicating that there exist "generic truthful directions" shared by various LVLMs. Based on these discoveries, we propose Truthful-Guided Pre-Intervention (TruthPrInt) that first learns the truthful direction of LVLM decoding and then applies truthful-guided inference-time intervention during LVLM decoding. We further propose ComnHallu to enhance both cross-LVLM and cross-data hallucination detection transferability by constructing and aligning hallucination latent subspaces. We evaluate TruthPrInt in extensive experimental settings, including in-domain and out-of-domain scenarios, over popular LVLMs and OH benchmarks. Experimental results indicate that TruthPrInt significantly outperforms state-of-the-art methods. Codes will be available at https://github.com/jinhaoduan/TruthPrInt. | [üîó Paper](http://arxiv.org/abs/2503.10602v1) |
## üîπ Fine-Tuning

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Transformers without Normalization](http://arxiv.org/abs/2503.10622v1) | Jiachen Zhu, Xinlei Chen, Kaiming He, Yann LeCun, Zhuang Liu | 2025-03-13 | Fine-Tuning | Normalization layers are ubiquitous in modern neural networks and have long been considered essential. This work demonstrates that Transformers without normalization can achieve the same or better performance using a remarkably simple technique. We introduce Dynamic Tanh (DyT), an element-wise operation $DyT($x$) = \tanh(\alpha $x$)$, as a drop-in replacement for normalization layers in Transformers. DyT is inspired by the observation that layer normalization in Transformers often produces tanh-like, $S$-shaped input-output mappings. By incorporating DyT, Transformers without normalization can match or exceed the performance of their normalized counterparts, mostly without hyperparameter tuning. We validate the effectiveness of Transformers with DyT across diverse settings, ranging from recognition to generation, supervised to self-supervised learning, and computer vision to language models. These findings challenge the conventional understanding that normalization layers are indispensable in modern neural networks, and offer new insights into their role in deep networks. | [üîó Paper](http://arxiv.org/abs/2503.10622v1) |
## üîπ General AI

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Charting and Navigating Hugging Face's Model Atlas](http://arxiv.org/abs/2503.10633v1) | Eliahu Horwitz, Nitzan Kurer, Jonathan Kahana, Liel Amar, Yedid Hoshen | 2025-03-13 | General AI | As there are now millions of publicly available neural networks, searching and analyzing large model repositories becomes increasingly important. Navigating so many models requires an atlas, but as most models are poorly documented charting such an atlas is challenging. To explore the hidden potential of model repositories, we chart a preliminary atlas representing the documented fraction of Hugging Face. It provides stunning visualizations of the model landscape and evolution. We demonstrate several applications of this atlas including predicting model attributes (e.g., accuracy), and analyzing trends in computer vision models. However, as the current atlas remains incomplete, we propose a method for charting undocumented regions. Specifically, we identify high-confidence structural priors based on dominant real-world model training practices. Leveraging these priors, our approach enables accurate mapping of previously undocumented areas of the atlas. We publicly release our datasets, code, and interactive atlas. | [üîó Paper](http://arxiv.org/abs/2503.10633v1) |
| [Fast Sideband Control of a Weakly Coupled Multimode Bosonic Memory](http://arxiv.org/abs/2503.10623v1) | Jordan Huang, Thomas J. DiNapoli, Gavin Rockwood, Ming Yuan, Prathyankara Narasimhan, Eesh Gupta, Mustafa Bal, Francesco Crisa, Sabrina Garattoni, Yao Lu, Liang Jiang, Srivatsan Chakram | 2025-03-13 | General AI | Circuit quantum electrodynamics (cQED) with superconducting cavities coupled to nonlinear circuits like transmons offers a promising platform for hardware-efficient quantum information processing. We address critical challenges in realizing this architecture by weakening the dispersive coupling while also demonstrating fast, high-fidelity multimode control by dynamically amplifying gate speeds through transmon-mediated sideband interactions. This approach enables transmon-cavity SWAP gates, for which we achieve speeds up to 30 times larger than the bare dispersive coupling. Combined with transmon rotations, this allows for efficient, universal state preparation in a single cavity mode, though achieving unitary gates and extending control to multiple modes remains a challenge. In this work, we overcome this by introducing two sideband control strategies: (1) a shelving technique that prevents unwanted transitions by temporarily storing populations in sideband-transparent transmon states and (2) a method that exploits the dispersive shift to synchronize sideband transition rates across chosen photon-number pairs to implement transmon-cavity SWAP gates that are selective on photon number. We leverage these protocols to prepare Fock and binomial code states across any of ten modes of a multimode cavity with millisecond cavity coherence times. We demonstrate the encoding of a qubit from a transmon into arbitrary vacuum and Fock state superpositions, as well as entangled NOON states of cavity mode pairs\textemdash a scheme extendable to arbitrary multimode Fock encodings. Furthermore, we implement a new binomial encoding gate that converts arbitrary transmon superpositions into binomial code states in $\qty{4}{\micro\second}$ (less than $1/\chi$), achieving an average post-selected final state fidelity of $\qty{96.3}{\percent}$ across different fiducial input states. | [üîó Paper](http://arxiv.org/abs/2503.10623v1) |
| [From TOWER to SPIRE: Adding the Speech Modality to a Text-Only LLM](http://arxiv.org/abs/2503.10620v1) | Kshitij Ambilduke, Ben Peters, Sonal Sannigrahi, Anil Keshwani, Tsz Kin Lam, Bruno Martins, Marcely Zanon Boito, Andr√© F. T. Martins | 2025-03-13 | General AI | Large language models (LLMs) have shown remarkable performance and generalization capabilities across multiple languages and tasks, making them very attractive targets for multi-modality integration (e.g., images or speech). In this work, we extend an existing LLM to the speech modality via speech discretization and continued pre-training. In particular, we are interested in multilingual LLMs, such as TOWER, as their pre-training setting allows us to treat discretized speech input as an additional translation language. The resulting open-source model, SPIRE, is able to transcribe and translate English speech input while maintaining TOWER's original performance on translation-related tasks, showcasing that discretized speech input integration as an additional language is feasible during LLM adaptation. We make our code and models available to the community. | [üîó Paper](http://arxiv.org/abs/2503.10620v1) |
| [Approximation technique for preserving the minimum principle on the entropy for the compressible Euler Equations](http://arxiv.org/abs/2503.10612v1) | Bennett Clayton, Eric J. Tovar | 2025-03-13 | General AI | This paper is concerned with constructing an invariant-domain preserving approximation technique for the compressible Euler equations that preserves the minimum principle on the physical entropy. We show that any numerical method that can be written as a convex combination of good auxiliary states will satisfy the minimum principle on the physical entropy provided the equation of state satisfies some mild assumptions. Furthermore, we derive a wave speed estimate in an extended Riemann problem necessary for constructing the auxiliary states with desired properties. Finally, we numerically illustrate the proposed methodology. | [üîó Paper](http://arxiv.org/abs/2503.10612v1) |
| [Characterization of geodesic completeness for landmark space](http://arxiv.org/abs/2503.10611v1) | Karen Habermann, Stephen C. Preston, Stefan Sommer | 2025-03-13 | General AI | We provide a full characterization of geodesic completeness for spaces of configurations of landmarks with smooth Riemannian metrics that satisfy a rotational and translation invariance and which are induced from metrics on subgroups of the diffeomorphism group for the shape domain. These spaces are widely used for applications in shape analysis, for example, for measuring shape changes in medical imaging and morphometrics in biology. For statistics of such data to be well-defined, it is imperative to know if geodesics exist for all times. We extend previously known sufficient conditions for geodesic completeness based on the regularity of the metric to give a full characterization for smooth Riemannian metrics with a rotational and translation invariance by means of an integrability criterion that involves only the behavior of the cometric kernel as landmarks approach collision. We further use the integrability criterion for geodesic completeness and previous work on stochastic completeness to construct a family of Riemannian landmark manifolds that are geodesically complete but stochastically incomplete. | [üîó Paper](http://arxiv.org/abs/2503.10611v1) |
| [Searching for strong lensing by late-type galaxies in UNIONS](http://arxiv.org/abs/2503.10610v1) | J. A. Acevedo Barroso, B. Cl√©ment, F. Courbin, R. Gavazzi, C. Lemon, K. Rojas, D. Scott, S. Gwyn, F. Hammer, M. J. Hudson, E. A. Magnier | 2025-03-13 | General AI | Recent wide-field galaxy surveys have led to an explosion in numbers of galaxy-scale strong gravitational lens candidates. However, the vast majority feature massive luminous red galaxies as the main deflectors, with late-type galaxies being vastly under-represented. This work presents a dedicated search for lensing by edge-on late-type galaxies in the Ultraviolet Near Infrared Optical Northern Survey (UNIONS). The search covers $3600$ deg$^2$ of $r$-band observations taken from the Canada-France-Hawaii Telescope. We consider all sources with magnitudes in the range $17 < r < 20.5$, without any colour preselection, yielding a parent sample of seven million sources. We characterise our parent sample via the visual inspection of $120\,000$ sources selected at random. From it, we estimate, with a 68\% confidence interval, that 1 in every $30\,000$ sources is an edge-on lens candidate, with at least eight high-quality candidates in the parent sample. This corresponds to 1 candidate per $17\,000$ edge-on late-type galaxies. Our search relies on a convolutional neural network (CNN) to select a reduced sample of candidates, followed by a visual inspection to curate the final sample. The CNN is trained from scratch using simulated $r$-band observations of edge-on lenses, and real observations of non-lenses. We find 61 good edge-on lens candidates using the CNN. Moreover, combining the CNN candidates with those found serendipitously, and those identified while characterising the parent sample, we discovered 4 grade A, 20 grade B, and 58 grade C edge-on lens candidates; effectively doubling the known sample of these systems. We also discovered 16 grade A, 16 grade B, and 18 grade C lens candidates of other types. Finally, based on the characterisation of the parent sample, we estimate that our search found around 60\% of the bright grade A and B edge-on lens candidates within the parent sample. | [üîó Paper](http://arxiv.org/abs/2503.10610v1) |
| [Hierarchical Bayesian inference for uncertainty quantification of thermal grease rheology](http://arxiv.org/abs/2503.10608v1) | Pranay P. Nagrani, Akshay J. Thomas, Amy M. Marconnet, Ivan C. Christov | 2025-03-13 | General AI | Rheologically complex soft solids such as thermal greases consist of filler particles within a polymer matrix. These materials find applications in improving the conformity of solid-solid contacts and enhancing heat transfer. Complex soft solids exhibit a transient non-Newtonian rheological response, including thixotropy and viscoelasticity. Previously, stress relaxation and buildup in sheared commercial thermal greases were successfully captured using a nonlinear elasto-visco-plastic (NEVP) model and a thixo-elasto-visco-plastic (TEVP). However, the previous model calibration methods ignored parameter uncertainty, providing only single values of the rheological parameters, and did not quantitatively address the chosen model's identifiability from the data or credibility of the calibration. We address these limitations via hierarchical Bayesian inference, accounting for uncertainties arising from epistemic and aleatoric sources. Importantly, the hierarchical approach allows us to assimilate experiments measuring the stress responses at various startup shear rates by allowing the models' parameters to vary across different shear rates. Then, a global distribution and the associated uncertainty are obtained by pooling. We also propagate uncertainties to the transient shear stress response predicted by the models. Overall, we demonstrate that the chosen NEVP and NEVP models are identifiable from rheometric startup data. However, for the TEVP model, the uncertainty of the parameters is lower (narrower distributions) when higher shear rates are used for inference. | [üîó Paper](http://arxiv.org/abs/2503.10608v1) |
| [Utilizing discrete variable representations for decoherence-accurate numerical simulation of superconducting circuits](http://arxiv.org/abs/2503.10607v1) | Brittany Richman, C. J. Lobb, Jacob M. Taylor | 2025-03-13 | General AI | Given the prevalence of superconducting platforms for uses in quantum computing and quantum sensing, the simulation of quantum superconducting circuits has become increasingly important for identifying system characteristics and modeling their relevant dynamics. Various numerical tools and software packages have been developed with this purpose in mind, typically utilizing the harmonic oscillator basis or the charge basis to represent a Hamiltonian. In this work, we instead consider the use of discrete variable representations (DVRs) to model superconducting circuits. In particular, we use `sinc DVRs' of both charge number and phase to approximate the eigenenergies of several prototypical examples, exploring their use and effectiveness in the numerical analysis of superconducting circuits. We find that not only are these DVRs capable of achieving decoherence-accurate simulation, i.e., accuracy at the resolution of experiments subject to decay, decoherence, and dephasing, they also demonstrate improvements in efficiency with smaller basis sizes and better convergence over standard approaches, showing that DVRs are an advantageous alternative for representing superconducting circuits. | [üîó Paper](http://arxiv.org/abs/2503.10607v1) |
| [Spherical accretion of a collisionless kinetic gas into a generic static black hole](http://arxiv.org/abs/2503.10606v1) | Mehrab Momennia, Olivier Sarbach | 2025-03-13 | General AI | We present a nontrivial extension of the problem of spherical accretion of a collisionless kinetic gas into the standard Schwarzschild black hole. This extension consists of replacing the Schwarzschild black hole by generic static and spherically symmetric black hole spacetimes with the aim of studying the effects of either modified gravitational theories beyond Einstein gravity or matter sources coupled to general relativity on the accretion process. This generalization also allows us to investigate the accretion into other types of black hole spacetimes, such as ones inspired by loop quantum gravity and string theory. To do so, we take into account a large class of static and spherically symmetric black holes whose spacetime is asymptotically flat with a positive total mass, has a regular Killing horizon, and satisfies appropriate monotonicity conditions of the metric functions. We provide the most general solution of the collisionless Boltzmann equation on such spacetimes by expressing the one-particle distribution function in terms of suitable symplectic coordinates on the cotangent bundle, and we calculate the relevant observables, such as particle current density and energy-momentum-stress tensor. Specializing to the case where the gas is described by an isotropic ideal fluid at rest at infinity, we compute the mass accretion rate and compression ratio, and we show that the tangential pressure is larger than the radial one at the horizon, indicating that the behavior of a collisionless gas is different from the one of an isotropic perfect fluid. As an example, we apply our generic formulae to two special black hole spacetimes, namely the Reissner-Nordstr\"om black hole and a loop quantum corrected black hole. We explore the effects of the free parameters on the observables and accretion rate, and we compare the results with those corresponding to the Schwarzschild black hole. | [üîó Paper](http://arxiv.org/abs/2503.10606v1) |
| [Performance of the spin qubit shuttling architecture for a surface code implementation](http://arxiv.org/abs/2503.10601v1) | Berat Yenilen, Arnau Sala, Hendrik Bluhm, Markus M√ºller, Manuel Rispler | 2025-03-13 | General AI | Qubit shuttling promises to advance some quantum computing platforms to the qubit register sizes needed for effective quantum error correction (QEC), but also introduces additional errors whose impact must be evaluated. The established method to investigate the performance of QEC codes in a realistic scenario is to employ a standard noise model known as circuit-level noise, where all quantum operations are modeled as noisy. In the present work, we take this noise model and single out the effect of shuttling errors by introducing them as an additional so-called error location. This hardware abstraction is motivated by the SpinBus architecture and allows a systematic numerical investigation to map out the resulting two-dimensional parameter space. To this end, we take the Surface code and perform large scale simulations, most notably extracting the threshold across said two-dimensional parameter space. We study two scenarios for shuttling errors, depolarization on the one hand and dephasing on the other hand. For a purely dephasing shuttling error, we find a threshold of several percent, provided that all other operations have a high fidelity. The qubit overhead needed to reach a logical error rate of $10^{-12}$ (known as the "teraquop" regime~\cite{Gidney2021Jul}) increases only moderately for shuttling error rates up to about 1 \% per shuttling operation. The error rates at which practically useful, i.e. well below threshold error correction is predicted to be possible are comfortably higher than what is expected to be achievable for spin qubits. Our results thus show that it is reasonable to expect shuttling operations to fall below threshold already at surprisingly large error rates. With realistic efforts in the near term, this offers positive prospects for spin qubit based quantum processors as a viable avenue for scalable fault-tolerant error-corrected quantum computing. | [üîó Paper](http://arxiv.org/abs/2503.10601v1) |
| [Current-linear emergent induction of pinned skyrmion textures in an oxide bilayer](http://arxiv.org/abs/2503.10600v1) | Ludwig Scheuchenpflug, Sebastian Esser, Robert Gruhl, Max Hirschberger, Philipp Gegenwart | 2025-03-13 | General AI | Emergent electromagnetic induction (EEMI) induced through current-driven spin dynamics was recently predicted and subsequently observed in helical spin magnets, opening a new direction in spintronics and paving the way towards further miniaturization of electronic circuit elements. In contrast to conventional inductors consisting of coil-like structures whose inductance $L$ shows a linear dependence on the cross-section $A$, emergent inductors exhibit an inverse ($\propto {A}^{-1}$) proportionality, favorable for the miniaturization of electrical devices. However, the expected current-linear response of the EEMI voltage has not been demonstrated. Magnetic skyrmions hold promise as a simple platform to study the conceptual foundations of EEMI from current-driven spin dynamics. We fabricated devices of thin film bilayers of ferromagnetic SrRuO$_3$ and paramagnetic SrIrO$_3$, which are known to host interfacial N\'eel skyrmions detected by the appearance of a topological Hall-effect (THE). A large, positive and current-linear inductive response is found to accompany the THE. In our experiment, the current-induced dynamics of pinned magnetic skyrmions creates a voltage both parallel and perpendicular to the applied electric current flow, corresponding to longitudinal and transverse induction, respectively. This is the first report of transverse EEMI, indicating an angle of $80^{\circ}$ between skyrmion motion and the applied current. Our observation of current-linear longitudinal and transverse EEMI is a hallmark of pinned dynamics of magnetic skyrmion textures in oxide heterostructures. | [üîó Paper](http://arxiv.org/abs/2503.10600v1) |
| [Thermodynamic correlation inequalities for finite times and transients](http://arxiv.org/abs/2503.10599v1) | Cai Dieball, Alja≈æ Godec | 2025-03-13 | General AI | Recently, a thermodynamic bound on correlation times was formulated in [A. Dechant, J. Garnier-Brun, S.-i. Sasa, Phys. Rev. Lett. 131, 167101 (2023)], showing how the decay of correlations in Langevin dynamics is bounded by short-time fluctuations and dissipation. Whereas these original results only address very long observation times in steady-state dynamics, we here generalize the respective inequalities to finite observations and general initial conditions. We utilize the connection between correlations and the fluctuations of time-integrated density functionals and generalize the direct stochastic calculus approach from [C. Dieball and A. Godec, Phys. Rev. Lett. 130, 087101 (2023)] which paves the way for further generalizations. We address the connection between short and long time scales, as well as the saturation of the bounds via complementary spectral-theoretic arguments. Motivated by the spectral insight, we formulate all results also for complex-valued observables. | [üîó Paper](http://arxiv.org/abs/2503.10599v1) |
| [Cosmological Dressing Rules](http://arxiv.org/abs/2503.10598v1) | Chandramouli Chowdhury, Arthur Lipstein, Joe Marshall, Jiajie Mei, Ivo Sachs | 2025-03-13 | General AI | The basic observables in cosmology are known as in-in correlators. Recent calculations have revealed that in-in correlators in four dimensional de Sitter space exhibit hidden simplicity stemming from a close relation to scattering amplitudes in flat space. In this paper we explain how to make this property manifest by dressing flat space Feynman diagrams with certain auxiliary propagators. These dressing rules hold for any order in perturbation theory and can be derived for a broad range of scalar theories, including those with infrared divergences. In the latter case we show that they reproduce the same infrared divergences predicted by the Schwinger-Keldysh formalism. | [üîó Paper](http://arxiv.org/abs/2503.10598v1) |
| [GroomLight: Hybrid Inverse Rendering for Relightable Human Hair Appearance Modeling](http://arxiv.org/abs/2503.10597v1) | Yang Zheng, Menglei Chai, Delio Vicini, Yuxiao Zhou, Yinghao Xu, Leonidas Guibas, Gordon Wetzstein, Thabo Beeler | 2025-03-13 | General AI | We present GroomLight, a novel method for relightable hair appearance modeling from multi-view images. Existing hair capture methods struggle to balance photorealistic rendering with relighting capabilities. Analytical material models, while physically grounded, often fail to fully capture appearance details. Conversely, neural rendering approaches excel at view synthesis but generalize poorly to novel lighting conditions. GroomLight addresses this challenge by combining the strengths of both paradigms. It employs an extended hair BSDF model to capture primary light transport and a light-aware residual model to reconstruct the remaining details. We further propose a hybrid inverse rendering pipeline to optimize both components, enabling high-fidelity relighting, view synthesis, and material editing. Extensive evaluations on real-world hair data demonstrate state-of-the-art performance of our method. | [üîó Paper](http://arxiv.org/abs/2503.10597v1) |
| [Compact Two-Loop QCD Corrections for $Vjj$ Production in Proton Collisions](http://arxiv.org/abs/2503.10595v1) | Giuseppe De Laurentis, Harald Ita, Ben Page, Vasily Sotnikov | 2025-03-13 | General AI | We present compact two-loop QCD corrections in the leading-color approximation for the production of an electroweak vector boson, $V = \{W^{\pm}, Z,\gamma^\star\}$, in association with two light jets ($Vjj$) at hadron colliders. Leptonic decays of the electroweak boson are included at the amplitude level. Working in the analytic-reconstruction approach, we develop two techniques to build compact partial-fraction forms for individual rational functions. One approach exploits their analytic structure. In the other, we iteratively construct subtraction terms that match the rational functions in singular limits. Moreover, we show how the singular behavior of the rational functions can be systematically used to find a more compact basis of the space that they span. We apply our techniques to the $Vjj$ amplitudes, yielding a representation that is three orders of magnitude smaller than previous results. We then use these compact expressions to provide an efficient and stable C++ numerical implementation suitable for phenomenological applications. | [üîó Paper](http://arxiv.org/abs/2503.10595v1) |
| [Poly-MgNet: Polynomial Building Blocks in Multigrid-Inspired ResNets](http://arxiv.org/abs/2503.10594v1) | Antonia van Betteray, Matthias Rottmann, Karsten Kahl | 2025-03-13 | General AI | The structural analogies of ResNets and Multigrid (MG) methods such as common building blocks like convolutions and poolings where already pointed out by He et al.\ in 2016. Multigrid methods are used in the context of scientific computing for solving large sparse linear systems arising from partial differential equations. MG methods particularly rely on two main concepts: smoothing and residual restriction / coarsening. Exploiting these analogies, He and Xu developed the MgNet framework, which integrates MG schemes into the design of ResNets. In this work, we introduce a novel neural network building block inspired by polynomial smoothers from MG theory. Our polynomial block from an MG perspective naturally extends the MgNet framework to Poly-Mgnet and at the same time reduces the number of weights in MgNet. We present a comprehensive study of our polynomial block, analyzing the choice of initial coefficients, the polynomial degree, the placement of activation functions, as well as of batch normalizations. Our results demonstrate that constructing (quadratic) polynomial building blocks based on real and imaginary polynomial roots enhances Poly-MgNet's capacity in terms of accuracy. Furthermore, our approach achieves an improved trade-off of model accuracy and number of weights compared to ResNet as well as compared to specific configurations of MgNet. | [üîó Paper](http://arxiv.org/abs/2503.10594v1) |
| [Chiral Magnetic Effect enhancement at lower collision energies](http://arxiv.org/abs/2503.10593v1) | Sebastian Grieninger, Sergio Morales-Tejera, Pau G. Romeu | 2025-03-13 | General AI | We extend previous holographic studies of the Chiral Magnetic Effect (CME) by incorporating a time-dependent magnetic field. Various magnetic field profiles proposed in the literature are implemented, and their impact on the CME signal is analyzed in both static and expanding backgrounds. Interestingly, the integrated chiral magnetic current can exhibit a non-monotonic dependence on the collision energy. Our results suggest that the CME signal is enhanced at collision energies below $\sqrt{s}=200$ GeV. In addition, we derive a quasi-equilibrium formula for the chiral magnetic effect in the expanding background that is valid at late times. | [üîó Paper](http://arxiv.org/abs/2503.10593v1) |
| [Analysis and sample-size determination for $2^K$ audit experiments with binary response and application to identification of effect of racial discrimination on access to justice](http://arxiv.org/abs/2503.10591v1) | Nicole Pashley, Brian Libgober, Tirthankar Dasgupta | 2025-03-13 | General AI | Social scientists have increasingly turned to audit experiments to investigate discrimination in the market for jobs, loans, housing and other opportunities. In a typical audit experiment, researchers assign ``signals'' (the treatment) to subjects at random and compare success rates across treatment conditions. In the recent past there has been increased interest in using randomized multifactor designs for audit experiments, popularly called factorial experiments, in which combinations of multiple signals are assigned to subjects. Although social scientists have manipulated multiple factors like race, gender and income, the analyses have been mostly exploratory in nature. In this paper we lay out a comprehensive methodology for design and analysis of $2^K$ factorial designs with binary response using model-free, randomization-based Neymanian inference and demonstrate its application by analyzing the audit experiment reported in Libgober (2020). Specifically, we integrate and extend several sections of the randomization-based, finite-population literature for binary outcomes, including sample size and power calculations, and non-linear factorial estimators, extending results. | [üîó Paper](http://arxiv.org/abs/2503.10591v1) |
| [Brauer's 14th Problem and Dyson's 10-Way](http://arxiv.org/abs/2503.10590v1) | Dmitriy Rumynin | 2025-03-13 | General AI | We consider Brauer's 14th Problem in the context of "Real" structures on finite groups and their antilinear representations. The problem is to count the number of characters of different type using "group theory". While the original Brauer's problem deals only with the three types (real, complex and quaternionic), here we consider the ten types coming from the Dyson's 10-way. | [üîó Paper](http://arxiv.org/abs/2503.10590v1) |
