# üìå AI Research Papers (August18 to August24)

## üîπ LLM

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Visual Autoregressive Modeling for Instruction-Guided Image Editing](http://arxiv.org/abs/2508.15772v1) | Qingyang Mao, Qi Cai, Yehao Li, Yingwei Pan, Mingyue Cheng, Ting Yao, Qi Liu, Tao Mei | 2025-08-21 | LLM, Diffusion Models | Recent advances in diffusion models have brought remarkable visual fidelity to instruction-guided image editing. However, their global denoising process inherently entangles the edited region with the entire image context, leading to unintended spurious modifications and compromised adherence to editing instructions. In contrast, autoregressive models offer a distinct paradigm by formulating image synthesis as a sequential process over discrete visual tokens. Their causal and compositional mechanism naturally circumvents the adherence challenges of diffusion-based methods. In this paper, we present VAREdit, a visual autoregressive (VAR) framework that reframes image editing as a next-scale prediction problem. Conditioned on source image features and text instructions, VAREdit generates multi-scale target features to achieve precise edits. A core challenge in this paradigm is how to effectively condition the source image tokens. We observe that finest-scale source features cannot effectively guide the prediction of coarser target features. To bridge this gap, we introduce a Scale-Aligned Reference (SAR) module, which injects scale-matched conditioning information into the first self-attention layer. VAREdit demonstrates significant advancements in both editing adherence and efficiency. On standard benchmarks, it outperforms leading diffusion-based methods by 30\%+ higher GPT-Balance score. Moreover, it completes a $512\times512$ editing in 1.2 seconds, making it 2.2$\times$ faster than the similarly sized UltraEdit. The models are available at https://github.com/HiDream-ai/VAREdit. | [üîó Paper](http://arxiv.org/abs/2508.15772v1) |
## üîπ Diffusion Models

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [CineScale: Free Lunch in High-Resolution Cinematic Visual Generation](http://arxiv.org/abs/2508.15774v1) | Haonan Qiu, Ning Yu, Ziqi Huang, Paul Debevec, Ziwei Liu | 2025-08-21 | Diffusion Models, Multimodal AI, Optimization | Visual diffusion models achieve remarkable progress, yet they are typically trained at limited resolutions due to the lack of high-resolution data and constrained computation resources, hampering their ability to generate high-fidelity images or videos at higher resolutions. Recent efforts have explored tuning-free strategies to exhibit the untapped potential higher-resolution visual generation of pre-trained models. However, these methods are still prone to producing low-quality visual content with repetitive patterns. The key obstacle lies in the inevitable increase in high-frequency information when the model generates visual content exceeding its training resolution, leading to undesirable repetitive patterns deriving from the accumulated errors. In this work, we propose CineScale, a novel inference paradigm to enable higher-resolution visual generation. To tackle the various issues introduced by the two types of video generation architectures, we propose dedicated variants tailored to each. Unlike existing baseline methods that are confined to high-resolution T2I and T2V generation, CineScale broadens the scope by enabling high-resolution I2V and V2V synthesis, built atop state-of-the-art open-source video generation frameworks. Extensive experiments validate the superiority of our paradigm in extending the capabilities of higher-resolution visual generation for both image and video models. Remarkably, our approach enables 8k image generation without any fine-tuning, and achieves 4k video generation with only minimal LoRA fine-tuning. Generated video samples are available at our website: https://eyeline-labs.github.io/CineScale/. | [üîó Paper](http://arxiv.org/abs/2508.15774v1) |
| [Skyrmion Lattice Order Controlled by Confinement Geometry](http://arxiv.org/abs/2508.15758v1) | Raphael Gruber, Jan Roth√∂rl, Simon M. Fr√∂hlich, Maarten A. Brems, Fabian Kammerbauer, Maria-Andromachi Syskaki, Elizabeth M. Jefremovas, Sachin Krishnia, Asle Sudb√∏, Peter Virnau, Mathias Kl√§ui | 2025-08-21 | Diffusion Models | Magnetic skyrmions forming two-dimensional (2D) lattices provide a versatile platform for investigating phase transitions predicted by Kosterlitz-Thouless-Halperin-Nelson-Young (KTHNY) theory. While 2D melting in skyrmion systems has been demonstrated, achieving controlled ordering in skyrmion lattices remains challenging due to pinning effects from a non-uniform energy landscape, which often results in polycrystalline structures. Skyrmions in thin films, however, offer thermal diffusion with high tunability and can be directly imaged via Kerr microscopy, enabling real-time observation of their dynamics. To regulate lattice order in such flexible systems, we introduce geometric confinements of varying shapes. Combining Kerr microscopy experiments with Thiele model simulations, we demonstrate that confinement geometry critically influences lattice order. Specifically, hexagonal confinements commensurate with the skyrmion lattice stabilize monodomain hexagonal ordering, while incommensurate geometries induce domain formation and reduce overall order. Understanding these boundary-driven effects is essential for advancing the study of 2D phase behavior and for the design of skyrmion-based spintronic applications, ranging from memory devices to unconventional computing architectures. | [üîó Paper](http://arxiv.org/abs/2508.15758v1) |
## üîπ RLHF

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Discovering Hidden Algebraic Structures via Transformers with Rank-Aware
  Beam GRPO](http://arxiv.org/abs/2508.15766v1) | Jaeha Lee, Gio Huh, Ning Su, Tony Yue YU | 2025-08-21 | RLHF, LLM, Scaling Laws, Optimization | Recent efforts have extended the capabilities of transformers in logical reasoning and symbolic computations. In this work, we investigate their capacity for non-linear latent pattern discovery in the context of functional decomposition, focusing on the challenging algebraic task of multivariate polynomial decomposition. This problem, with widespread applications in science and engineering, is proved to be NP-hard, and demands both precision and insight. Our contributions are threefold: First, we develop a synthetic data generation pipeline providing fine-grained control over problem complexity. Second, we train transformer models via supervised learning and evaluate them across four key dimensions involving scaling behavior and generalizability. Third, we propose Beam Grouped Relative Policy Optimization (BGRPO), a rank-aware reinforcement learning method suitable for hard algebraic problems. Finetuning with BGRPO improves accuracy while reducing beam width by up to half, resulting in approximately 75% lower inference compute. Additionally, our model demonstrates competitive performance in polynomial simplification, outperforming Mathematica in various cases. | [üîó Paper](http://arxiv.org/abs/2508.15766v1) |
| [Distributed Detection of Adversarial Attacks in Multi-Agent
  Reinforcement Learning with Continuous Action Space](http://arxiv.org/abs/2508.15764v1) | Kiarash Kazari, Ezzeldin Shereen, Gy√∂rgy D√°n | 2025-08-21 | RLHF, Security & Adversarial ML, AI Safety | We address the problem of detecting adversarial attacks against cooperative multi-agent reinforcement learning with continuous action space. We propose a decentralized detector that relies solely on the local observations of the agents and makes use of a statistical characterization of the normal behavior of observable agents. The proposed detector utilizes deep neural networks to approximate the normal behavior of agents as parametric multivariate Gaussian distributions. Based on the predicted density functions, we define a normality score and provide a characterization of its mean and variance. This characterization allows us to employ a two-sided CUSUM procedure for detecting deviations of the normality score from its mean, serving as a detector of anomalous behavior in real-time. We evaluate our scheme on various multi-agent PettingZoo benchmarks against different state-of-the-art attack methods, and our results demonstrate the effectiveness of our method in detecting impactful adversarial attacks. Particularly, it outperforms the discrete counterpart by achieving AUC-ROC scores of over 0.95 against the most impactful attacks in all evaluated environments. | [üîó Paper](http://arxiv.org/abs/2508.15764v1) |
| [Waver: Wave Your Way to Lifelike Video Generation](http://arxiv.org/abs/2508.15761v1) | Yifu Zhang, Hao Yang, Yuqi Zhang, Yifei Hu, Fengda Zhu, Chuang Lin, Xiaofeng Mei, Yi Jiang, Zehuan Yuan, Bingyue Peng | 2025-08-21 | RLHF, Diffusion Models, Multimodal AI | We present Waver, a high-performance foundation model for unified image and video generation. Waver can directly generate videos with durations ranging from 5 to 10 seconds at a native resolution of 720p, which are subsequently upscaled to 1080p. The model simultaneously supports text-to-video (T2V), image-to-video (I2V), and text-to-image (T2I) generation within a single, integrated framework. We introduce a Hybrid Stream DiT architecture to enhance modality alignment and accelerate training convergence. To ensure training data quality, we establish a comprehensive data curation pipeline and manually annotate and train an MLLM-based video quality model to filter for the highest-quality samples. Furthermore, we provide detailed training and inference recipes to facilitate the generation of high-quality videos. Building on these contributions, Waver excels at capturing complex motion, achieving superior motion amplitude and temporal consistency in video synthesis. Notably, it ranks among the Top 3 on both the T2V and I2V leaderboards at Artificial Analysis (data as of 2025-07-30 10:00 GMT+8), consistently outperforming existing open-source models and matching or surpassing state-of-the-art commercial solutions. We hope this technical report will help the community more efficiently train high-quality video generation models and accelerate progress in video generation technologies. Official page: https://github.com/FoundationVision/Waver. | [üîó Paper](http://arxiv.org/abs/2508.15761v1) |
## üîπ Optimization

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass](http://arxiv.org/abs/2508.15769v1) | Yanxu Meng, Haoning Wu, Ya Zhang, Weidi Xie | 2025-08-21 | Optimization | 3D content generation has recently attracted significant research interest due to its applications in VR/AR and embodied AI. In this work, we address the challenging task of synthesizing multiple 3D assets within a single scene image. Concretely, our contributions are fourfold: (i) we present SceneGen, a novel framework that takes a scene image and corresponding object masks as input, simultaneously producing multiple 3D assets with geometry and texture. Notably, SceneGen operates with no need for optimization or asset retrieval; (ii) we introduce a novel feature aggregation module that integrates local and global scene information from visual and geometric encoders within the feature extraction module. Coupled with a position head, this enables the generation of 3D assets and their relative spatial positions in a single feedforward pass; (iii) we demonstrate SceneGen's direct extensibility to multi-image input scenarios. Despite being trained solely on single-image inputs, our architectural design enables improved generation performance with multi-image inputs; and (iv) extensive quantitative and qualitative evaluations confirm the efficiency and robust generation abilities of our approach. We believe this paradigm offers a novel solution for high-quality 3D content generation, potentially advancing its practical applications in downstream tasks. The code and model will be publicly available at: https://mengmouxu.github.io/SceneGen. | [üîó Paper](http://arxiv.org/abs/2508.15769v1) |
| [A framework for robust quantum speedups in practical correlated
  electronic structure and dynamics](http://arxiv.org/abs/2508.15765v1) | Jielun Chen, Garnet Kin-Lic Chan | 2025-08-21 | Optimization | Proposed quantum advantage in electronic structure has so far required significant fine-tuning to find problems where classical heuristics fail. We describe how to obtain robust quantum speedups for correlated electronic structure and dynamics precisely in the regime where widely used classical heuristics are most successful. | [üîó Paper](http://arxiv.org/abs/2508.15765v1) |
## üîπ Scaling Laws

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Scaling Group Inference for Diverse and High-Quality Generation](http://arxiv.org/abs/2508.15773v1) | Gaurav Parmar, Or Patashnik, Daniil Ostashev, Kuan-Chieh Wang, Kfir Aberman, Srinivasa Narasimhan, Jun-Yan Zhu | 2025-08-21 | Scaling Laws, Diffusion Models, Multimodal AI | Generative models typically sample outputs independently, and recent inference-time guidance and scaling algorithms focus on improving the quality of individual samples. However, in real-world applications, users are often presented with a set of multiple images (e.g., 4-8) for each prompt, where independent sampling tends to lead to redundant results, limiting user choices and hindering idea exploration. In this work, we introduce a scalable group inference method that improves both the diversity and quality of a group of samples. We formulate group inference as a quadratic integer assignment problem: candidate outputs are modeled as graph nodes, and a subset is selected to optimize sample quality (unary term) while maximizing group diversity (binary term). To substantially improve runtime efficiency, we progressively prune the candidate set using intermediate predictions, allowing our method to scale up to large candidate sets. Extensive experiments show that our method significantly improves group diversity and quality compared to independent sampling baselines and recent inference algorithms. Our framework generalizes across a wide range of tasks, including text-to-image, image-to-image, image prompting, and video generation, enabling generative models to treat multiple outputs as cohesive groups rather than independent samples. | [üîó Paper](http://arxiv.org/abs/2508.15773v1) |
| [Evaluating classical simulations with a quantum processor](http://arxiv.org/abs/2508.15759v1) | Alberto Nocera, Jack Raymond, William Bernoudy, Mohammad H. Amin, Andrew D. King | 2025-08-21 | Scaling Laws | As simulations of quantum systems cross the limits of classical computability, both quantum and classical approaches become hard to verify. Scaling predictions are therefore based on local structure and asymptotic assumptions, typically with classical methods being used to evaluate quantum simulators where possible. Here, in contrast, we use a quantum annealing processor to produce a ground truth for evaluating classical tensor-network methods whose scaling has not yet been firmly established. Our observations run contrary to previous scaling predictions, demonstrating the need for caution when extrapolating the accuracy of classical simulations of quantum dynamics. Our results demonstrate that the virtuous cycle of competition between classical and quantum simulations can lend insight in both directions. | [üîó Paper](http://arxiv.org/abs/2508.15759v1) |
## üîπ Training & Evaluation

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Intern-S1: A Scientific Multimodal Foundation Model](http://arxiv.org/abs/2508.15763v1) | Lei Bai, Zhongrui Cai, Maosong Cao, Weihan Cao, Chiyu Chen, Haojiong Chen, Kai Chen, Pengcheng Chen, Ying Chen, Yongkang Chen, Yu Cheng, Yu Cheng, Pei Chu, Tao Chu, Erfei Cui, Ganqu Cui, Long Cui, Ziyun Cui, Nianchen Deng, Ning Ding, Nanqin Dong, Peijie Dong, Shihan Dou, Sinan Du, Haodong Duan, Caihua Fan, Ben Gao, Changjiang Gao, Jianfei Gao, Songyang Gao, Yang Gao, Zhangwei Gao, Jiaye Ge, Qiming Ge, Lixin Gu, Yuzhe Gu, Aijia Guo, Qipeng Guo, Xu Guo, Conghui He, Junjun He, Yili Hong, Siyuan Hou, Caiyu Hu, Hanglei Hu, Jucheng Hu, Ming Hu, Zhouqi Hua, Haian Huang, Junhao Huang, Xu Huang, Zixian Huang, Zhe Jiang, Lingkai Kong, Linyang Li, Peiji Li, Pengze Li, Shuaibin Li, Tianbin Li, Wei Li, Yuqiang Li, Dahua Lin, Junyao Lin, Tianyi Lin, Zhishan Lin, Hongwei Liu, Jiangning Liu, Jiyao Liu, Junnan Liu, Kai Liu, Kaiwen Liu, Kuikun Liu, Shichun Liu, Shudong Liu, Wei Liu, Xinyao Liu, Yuhong Liu, Zhan Liu, Yinquan Lu, Haijun Lv, Hongxia Lv, Huijie Lv, Qidang Lv, Ying Lv, Chengqi Lyu, Chenglong Ma, Jianpeng Ma, Ren Ma, Runmin Ma, Runyuan Ma, Xinzhu Ma, Yichuan Ma, Zihan Ma, Sixuan Mi, Junzhi Ning, Wenchang Ning, Xinle Pang, Jiahui Peng, Runyu Peng, Yu Qiao, Jiantao Qiu, Xiaoye Qu, Yuan Qu, Yuchen Ren, Fukai Shang, Wenqi Shao, Junhao Shen, Shuaike Shen, Chunfeng Song, Demin Song, Diping Song, Chenlin Su, Weijie Su, Weigao Sun, Yu Sun, Qian Tan, Cheng Tang, Huanze Tang, Kexian Tang, Shixiang Tang, Jian Tong, Aoran Wang, Bin Wang, Dong Wang, Lintao Wang, Rui Wang, Weiyun Wang, Wenhai Wang, Yi Wang, Ziyi Wang, Ling-I Wu, Wen Wu, Yue Wu, Zijian Wu, Linchen Xiao, Shuhao Xing, Chao Xu, Huihui Xu, Jun Xu, Ruiliang Xu, Wanghan Xu, GanLin Yang, Yuming Yang, Haochen Ye, Jin Ye, Shenglong Ye, Jia Yu, Jiashuo Yu, Jing Yu, Fei Yuan, Bo Zhang, Chao Zhang, Chen Zhang, Hongjie Zhang, Jin Zhang, Qiaosheng Zhang, Qiuyinzhe Zhang, Songyang Zhang, Taolin Zhang, Wenlong Zhang, Wenwei Zhang, Yechen Zhang, Ziyang Zhang, Haiteng Zhao, Qian Zhao, Xiangyu Zhao, Xiangyu Zhao, Bowen Zhou, Dongzhan Zhou, Peiheng Zhou, Yuhao Zhou, Yunhua Zhou, Dongsheng Zhu, Lin Zhu, Yicheng Zou | 2025-08-21 | Training & Evaluation, LLM, RLHF, Scaling Laws, Multimodal AI | In recent years, a plethora of open-source foundation models have emerged, achieving remarkable progress in some widely attended fields, with performance being quite close to that of closed-source models. However, in high-value but more challenging scientific professional fields, either the fields still rely on expert models, or the progress of general foundation models lags significantly compared to those in popular areas, far from sufficient for transforming scientific research and leaving substantial gap between open-source models and closed-source models in these scientific domains. To mitigate this gap and explore a step further toward Artificial General Intelligence (AGI), we introduce Intern-S1, a specialized generalist equipped with general understanding and reasoning capabilities with expertise to analyze multiple science modal data. Intern-S1 is a multimodal Mixture-of-Experts (MoE) model with 28 billion activated parameters and 241 billion total parameters, continually pre-trained on 5T tokens, including over 2.5T tokens from scientific domains. In the post-training stage, Intern-S1 undergoes offline and then online reinforcement learning (RL) in InternBootCamp, where we propose Mixture-of-Rewards (MoR) to synergize the RL training on more than 1000 tasks simultaneously. Through integrated innovations in algorithms, data, and training systems, Intern-S1 achieved top-tier performance in online RL training.On comprehensive evaluation benchmarks, Intern-S1 demonstrates competitive performance on general reasoning tasks among open-source models and significantly outperforms open-source models in scientific domains, surpassing closed-source state-of-the-art models in professional tasks, such as molecular synthesis planning, reaction condition prediction, predicting thermodynamic stabilities for crystals. Our models are available at https://huggingface.co/internlm/Intern-S1. | [üîó Paper](http://arxiv.org/abs/2508.15763v1) |
| [Language-Guided Tuning: Enhancing Numeric Optimization with Textual
  Feedback](http://arxiv.org/abs/2508.15757v1) | Yuxing Lu, Yucheng Hu, Nan Sun, Xukai Zhao | 2025-08-21 | Training & Evaluation, AI Safety, Optimization | Configuration optimization remains a critical bottleneck in machine learning, requiring coordinated tuning across model architecture, training strategy, feature engineering, and hyperparameters. Traditional approaches treat these dimensions independently and lack interpretability, while recent automated methods struggle with dynamic adaptability and semantic reasoning about optimization decisions. We introduce Language-Guided Tuning (LGT), a novel framework that employs multi-agent Large Language Models to intelligently optimize configurations through natural language reasoning. We apply textual gradients - qualitative feedback signals that complement numerical optimization by providing semantic understanding of training dynamics and configuration interdependencies. LGT coordinates three specialized agents: an Advisor that proposes configuration changes, an Evaluator that assesses progress, and an Optimizer that refines the decision-making process, creating a self-improving feedback loop. Through comprehensive evaluation on six diverse datasets, LGT demonstrates substantial improvements over traditional optimization methods, achieving performance gains while maintaining high interpretability. | [üîó Paper](http://arxiv.org/abs/2508.15757v1) |
## üîπ Autonomous Agents

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on
  Challenging Queries](http://arxiv.org/abs/2508.15760v1) | Ming Yin, Dinghan Shen, Silei Xu, Jianbing Han, Sixun Dong, Mian Zhang, Yebowen Hu, Shujian Liu, Simin Ma, Song Wang, Sathish Reddy Indurthi, Xun Wang, Yiran Chen, Kaiqiang Song | 2025-08-21 | Autonomous Agents, Training & Evaluation, Model Evaluation | Tool calling has emerged as a critical capability for AI agents to interact with the real world and solve complex tasks. While the Model Context Protocol (MCP) provides a powerful standardized framework for tool integration, there is a significant gap in benchmarking how well AI agents can effectively solve multi-step tasks using diverse MCP tools in realistic, dynamic scenarios. In this work, we present LiveMCP-101, a benchmark of 101 carefully curated real-world queries, refined through iterative LLM rewriting and manual review, that require coordinated use of multiple MCP tools including web search, file operations, mathematical reasoning, and data analysis. Moreover, we introduce a novel evaluation approach that leverages ground-truth execution plans rather than raw API outputs, better reflecting the evolving nature of real-world environments. Experiments show that even frontier LLMs achieve a success rate below 60\%, highlighting major challenges in tool orchestration. Detailed ablations and error analysis further reveal distinct failure modes and inefficiencies in token usage, pointing to concrete directions for advancing current models. LiveMCP-101 sets a rigorous standard for evaluating real-world agent capabilities, advancing toward autonomous AI systems that reliably execute complex tasks through tool use. | [üîó Paper](http://arxiv.org/abs/2508.15760v1) |
## üîπ General AI

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Overview of complex organic molecule observations in protostellar
  systems](http://arxiv.org/abs/2508.15771v1) | P. Nazari | 2025-08-21 | General AI | Complex organic molecules (COMs) have been detected abundantly at various stages of star formation, particularly in the warm protostellar phase. The progress in gas-phase measurements has been accelerated by the advent of the Atacama Large Millimeter/submillimeter Array and in ice measurements by the James Webb Space Telescope. Particularly, the community has moved from single-source studies of COMs to statistical analyses because of these powerful instruments. In this article, I review surveys that consider COMs in the gas and ice. The two takeaways from this review include; 1. Gas-phase abundance ratios for some COMs show a small difference across many objects and the ice abundance ratios show similar or higher values to the gas, both pointing to the importance of ice chemistry in COM formation, 2. Some COM ratios show larger differences across many objects which could be due to either chemical or physical effects, thus both factors need to be considered when interpreting the data. | [üîó Paper](http://arxiv.org/abs/2508.15771v1) |
| [Quantum cohomology of variations of GIT quotients and flips](http://arxiv.org/abs/2508.15770v1) | Zhaoxing Gu, Song Yu, Tony Yue YU | 2025-08-21 | General AI | We prove a decomposition theorem for the quantum cohomology of variations of GIT quotients. More precisely, for any reductive group $G$ and a simple $G$-VGIT wall-crossing $X_- \dashrightarrow X_+$ with a wall $S$, we show that the quantum $D$-module of $X_-$ can be decomposed into a direct sum of that of $X_+$ and copies of that of $S$. As an application, we obtain a decomposition theorem for the quantum cohomology of local models of standard flips in birational geometry. | [üîó Paper](http://arxiv.org/abs/2508.15770v1) |
| [3I/ATLAS (C/2025 N1): Direct Spacecraft Exploration of a Possible Relic
  of Planetary Formation at "Cosmic Noon"](http://arxiv.org/abs/2508.15768v1) | T. Marshall Eubanks, Bruce G. Bills, Adam Hibberd, W. Paul Blase, Andreas M. Hein, Robert G. Kennedy III, Adrien Coffinet, Jean Schneider, Pierre Kervella, Carlos Gomez de Olea Ballester | 2025-08-21 | General AI | The interstellar object 3I/ATLAS (also C/2025 N1 (ATLAS), henceforth, 3I), discovered by the ATLAS Chile telescope on 2025 July 1, was rapidly revealed to be the third known interstellar object (ISO) transiting the solar system, with an incoming velocity at infinity of 57.9763 $\pm$ 0.0044 km s$^{-1}$. An examination of 3I's pre-encounter kinematics shows that it is likely to be an object from the galactic thick disk, and thus a remnant of the Galaxy's ``cosmic noon'' period of intense star formation $\sim$9 - 13 gigayears ago. This kinematic assignment of 3I to the thick disk can be tested observationally in the transit of 3I through the solar system. Unfortunately for terrestrial observers, the 3I perihelion will happen when it is on the other side of the Sun as seen from Earth, at a solar elongation of 12.80 degrees, rendering observation from Earth (or near-Earth space telescopes) hard or impossible. With a retrograde orbit inclined 175.114 degrees (only 4.886 degrees from the ecliptic plane), and a trajectory passing inside the orbit of Mars, 3I will pass relatively close to a number of already launched interplanetary spacecraft. We find a strong science case for observations in the periods of the close approaches of the Psyche spacecraft on 2025 September 4, at 0.302 AU, the martian spacecraft array on 2025 October 3, and the Juice spacecraft on 2025 November 4. In addition, the Europa Clipper, Hera and even the more distant Lucy spacecraft may pass through 3I's cometary tail in the period after its perihelion passage, potentially directly observing the conditions and composition there. Spacecraft observations could, to the extent they are possible, provide the only source of spectral and imaging data during the 3I perihelion passage. | [üîó Paper](http://arxiv.org/abs/2508.15768v1) |
| [ATLAS: Decoupling Skeletal and Shape Parameters for Expressive
  Parametric Human Modeling](http://arxiv.org/abs/2508.15767v1) | Jinhyung Park, Javier Romero, Shunsuke Saito, Fabian Prada, Takaaki Shiratori, Yichen Xu, Federica Bogo, Shoou-I Yu, Kris Kitani, Rawal Khirodkar | 2025-08-21 | General AI | Parametric body models offer expressive 3D representation of humans across a wide range of poses, shapes, and facial expressions, typically derived by learning a basis over registered 3D meshes. However, existing human mesh modeling approaches struggle to capture detailed variations across diverse body poses and shapes, largely due to limited training data diversity and restrictive modeling assumptions. Moreover, the common paradigm first optimizes the external body surface using a linear basis, then regresses internal skeletal joints from surface vertices. This approach introduces problematic dependencies between internal skeleton and outer soft tissue, limiting direct control over body height and bone lengths. To address these issues, we present ATLAS, a high-fidelity body model learned from 600k high-resolution scans captured using 240 synchronized cameras. Unlike previous methods, we explicitly decouple the shape and skeleton bases by grounding our mesh representation in the human skeleton. This decoupling enables enhanced shape expressivity, fine-grained customization of body attributes, and keypoint fitting independent of external soft-tissue characteristics. ATLAS outperforms existing methods by fitting unseen subjects in diverse poses more accurately, and quantitative evaluations show that our non-linear pose correctives more effectively capture complex poses compared to linear models. | [üîó Paper](http://arxiv.org/abs/2508.15767v1) |
| [Bayesian Hierarchical Methods for Surveillance of Cervical Dystonia
  Treatments](http://arxiv.org/abs/2508.15762v1) | D. Baidoo, E. Kubuafor, S. F. Osarfo, F. A. Agyei-Owusu, J. A. Frimpong, R. Amevor, A. Duah, F. Aboagye | 2025-08-21 | General AI | Cervical dystonia, a debilitating neurological disorder marked by involuntary muscle contractions and chronic pain, presents significant treatment challenges despite advances in botulinum toxin therapy. While botulinum toxin type B has emerged as one of the leading treatments, comparative efficacy across doses and the influence of demographic factors for personalized medicine remain understudied. This study aimed to: (1) compare the efficacy of different botulinum toxin type B doses using Bayesian methods, (2) evaluate demographic and clinical factors affecting treatment response, and (3) establish a probabilistic framework for personalized cervical dystonia management. We analyzed data from a multicenter randomized controlled trial involving 109 patients assigned to placebo, 5,000 units, or 10,000 units of botulinum toxin type B groups. The primary outcome was the Toronto Western Spasmodic Torticollis Rating Scale measured over 16 weeks. Bayesian hierarchical modeling assessed treatment effects while accounting for patient heterogeneity. Lower botulinum toxin type B doses (5,000 units) showed greater overall Toronto Western Spasmodic Torticollis Rating Scale score reductions (treatment effect: -2.39, 95% Probability Interval: -4.10 to -0.70). Male patients demonstrated better responses (5.2% greater improvement) than female patients. Substantial between-patient variability and site-specific effects were observed, highlighting the need for personalized protocols. The study confirms botulinum toxin type B's dose-dependent efficacy while identifying key modifiable factors in treatment response. Bayesian methods provided nuanced insights into uncertainty and heterogeneity, paving the way for personalized medicine in cervical dystonia management. | [üîó Paper](http://arxiv.org/abs/2508.15762v1) |
| [Robust Data Interpretation for Perturbed Nulling Interferometers via
  Proper Handling of Correlated Errors](http://arxiv.org/abs/2508.15756v1) | Philipp A. Huber, Felix A. Dannert, Romain Laugier, Taro Matsuo, Loes W. Rutten, Adrian M. Glauser, Sascha P. Quanz | 2025-08-21 | General AI | The detection and atmospheric characterization of potentially habitable, temperate terrestrial exoplanets using a space-based mid-infrared nulling interferometer is a major goal of contemporary astrophysics. A central part of the analysis of such an instrument are correlated errors arising from perturbations in the system. While previous studies have often treated their effects in a limited manner, we aim to treat them comprehensively here and argue that data whitening based on the covariance of these errors is a suitable method to mitigate their impact. We present a framework that quantitatively connects instrumental perturbations to performance metrics and develop two computational tools to support our analysis: PHRINGE, for the generation of synthetic nulling data, and LIFEsimMC, a new Monte Carlo-based end-to-end simulator for the Large Interferometer For Exoplanets (LIFE). Applying our framework to a reference observation of an Earth twin orbiting a Sun twin at 10 pc, we find that whitening is not only essential for a correct interpretation of the detection metric used in hypothesis testing, but also improves the estimates of the planetary properties. Moreover, our approach enables an estimation of the spectral covariance of the extracted planetary spectra, providing valuable additional input for future atmospheric retrievals. We therefore recommend incorporating the framework into performance assessments and requirement derivations for future nulling interferometers. | [üîó Paper](http://arxiv.org/abs/2508.15756v1) |
| [Neural Robot Dynamics](http://arxiv.org/abs/2508.15755v1) | Jie Xu, Eric Heiden, Iretiayo Akinola, Dieter Fox, Miles Macklin, Yashraj Narang | 2025-08-21 | General AI | Accurate and efficient simulation of modern robots remains challenging due to their high degrees of freedom and intricate mechanisms. Neural simulators have emerged as a promising alternative to traditional analytical simulators, capable of efficiently predicting complex dynamics and adapting to real-world data; however, existing neural simulators typically require application-specific training and fail to generalize to novel tasks and/or environments, primarily due to inadequate representations of the global state. In this work, we address the problem of learning generalizable neural simulators for robots that are structured as articulated rigid bodies. We propose NeRD (Neural Robot Dynamics), learned robot-specific dynamics models for predicting future states for articulated rigid bodies under contact constraints. NeRD uniquely replaces the low-level dynamics and contact solvers in an analytical simulator and employs a robot-centric and spatially-invariant simulation state representation. We integrate the learned NeRD models as an interchangeable backend solver within a state-of-the-art robotics simulator. We conduct extensive experiments to show that the NeRD simulators are stable and accurate over a thousand simulation steps; generalize across tasks and environment configurations; enable policy learning exclusively in a neural engine; and, unlike most classical simulators, can be fine-tuned from real-world data to bridge the gap between simulation and reality. | [üîó Paper](http://arxiv.org/abs/2508.15755v1) |
