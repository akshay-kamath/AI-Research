# ðŸ“Œ AI Research Papers (June16 to June22)

## ðŸ”¹ Diffusion Models

| ðŸ“„ Title | ðŸ–Š Authors | ðŸ“… Date | ðŸ· Tags | ðŸ“œ Summary | ðŸ”— Link |
|---------|---------|---------|---------|---------|---------|
| [Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual
  Tokens](http://arxiv.org/abs/2506.17218v1) | Zeyuan Yang, Xueyang Yu, Delin Chen, Maohao Shen, Chuang Gan | 2025-06-20 | Diffusion Models, Multimodal AI, Optimization | Vision-language models (VLMs) excel at multimodal understanding, yet their text-only decoding forces them to verbalize visual reasoning, limiting performance on tasks that demand visual imagination. Recent attempts train VLMs to render explicit images, but the heavy image-generation pre-training often hinders the reasoning ability. Inspired by the way humans reason with mental imagery-the internal construction and manipulation of visual cues-we investigate whether VLMs can reason through interleaved multimodal trajectories without producing explicit images. To this end, we present a Machine Mental Imagery framework, dubbed as Mirage, which augments VLM decoding with latent visual tokens alongside ordinary text. Concretely, whenever the model chooses to ``think visually'', it recasts its hidden states as next tokens, thereby continuing a multimodal trajectory without generating pixel-level images. Begin by supervising the latent tokens through distillation from ground-truth image embeddings, we then switch to text-only supervision to make the latent trajectory align tightly with the task objective. A subsequent reinforcement learning stage further enhances the multimodal reasoning capability. Experiments on diverse benchmarks demonstrate that Mirage unlocks stronger multimodal reasoning without explicit image generation. | [ðŸ”— Paper](http://arxiv.org/abs/2506.17218v1) |
## ðŸ”¹ RLHF

| ðŸ“„ Title | ðŸ–Š Authors | ðŸ“… Date | ðŸ· Tags | ðŸ“œ Summary | ðŸ”— Link |
|---------|---------|---------|---------|---------|---------|
| [No Free Lunch: Rethinking Internal Feedback for LLM Reasoning](http://arxiv.org/abs/2506.17219v1) | Yanzhi Zhang, Zhaoxi Zhang, Haoxiang Guan, Yilin Cheng, Yitong Duan, Chen Wang, Yue Wang, Shuxin Zheng, Jiyan He | 2025-06-20 | RLHF | Reinforcement learning has emerged as a powerful paradigm for post-training large language models (LLMs) to improve reasoning. Approaches like Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning with Verifiable Rewards (RLVR) have shown strong results, but they require extensive external supervision. We investigate an alternative class of methods, Reinforcement Learning from Internal Feedback (RLIF), which relies solely on intrinsic model-derived signals instead of external rewards. In particular, we leverage unsupervised reward proxies such as token-level entropy, trajectory-level entropy, and self-certainty. Our theoretical analysis shows these internal objectives are partially equivalent, and we empirically evaluate various RLIF strategies on challenging math reasoning benchmarks. Experimental results demonstrate that RLIF can boost the reasoning performance of base LLMs at the beginning phase of the training, matching or surpassing RLVR techniques on these tasks. However, when training progresses, performance degrades even below the model before training. Moreover, we find that RLIF yields little improvement for instruction-tuned models, indicating diminishing returns of intrinsic feedback once an LLM is already instruction-tuned. We further analyze this limitation by mixing model weights and explain the reason of RLIF's training behaviors, providing practical guidelines for integrating internal feedback signals into LLM training. We hope our analysis of internal feedback will inform more principled and effective strategies for LLM post-training. | [ðŸ”— Paper](http://arxiv.org/abs/2506.17219v1) |
## ðŸ”¹ Optimization

| ðŸ“„ Title | ðŸ–Š Authors | ðŸ“… Date | ðŸ· Tags | ðŸ“œ Summary | ðŸ”— Link |
|---------|---------|---------|---------|---------|---------|
| [VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning](http://arxiv.org/abs/2506.17221v1) | Zhangyang Qi, Zhixiong Zhang, Yizhou Yu, Jiaqi Wang, Hengshuang Zhao | 2025-06-20 | Optimization, Multimodal AI, LLM | Vision-Language Navigation (VLN) is a core challenge in embodied AI, requiring agents to navigate real-world environments using natural language instructions. Current language model-based navigation systems operate on discrete topological graphs, limiting path planning to predefined node connections. We propose VLN-R1, an end-to-end framework that leverages Large Vision-Language Models (LVLM) to directly translate egocentric video streams into continuous navigation actions, adopting GRPO-based training inspired by DeepSeek-R1. To enable effective training, we first construct the VLN-Ego dataset using a 3D simulator, Habitat, and propose Long-Short Memory Sampling to balance historical and current observations. While large language models can supervise complete textual instructions, they lack fine-grained action-level control. Our framework employs a two-stage training approach: a) Supervised fine-tuning (SFT) to align the model's action sequence text predictions with expert demonstrations, followed by b) Reinforcement fine-tuning (RFT) enhanced with a Time-Decayed Reward (TDR) mechanism that strategically weights multi-step future actions. Experimental results show VLN-R1 achieves strong performance on VLN-CE benchmark. VLN-R1 proves LVLMs can drive embodied navigation and enhance task-specific reasoning through data-efficient, reward-driven post-training. | [ðŸ”— Paper](http://arxiv.org/abs/2506.17221v1) |
## ðŸ”¹ Training & Evaluation

| ðŸ“„ Title | ðŸ–Š Authors | ðŸ“… Date | ðŸ· Tags | ðŸ“œ Summary | ðŸ”— Link |
|---------|---------|---------|---------|---------|---------|
| [Emergent Temporal Correspondences from Video Diffusion Transformers](http://arxiv.org/abs/2506.17220v1) | Jisu Nam, Soowon Son, Dahyun Chung, Jiyoung Kim, Siyoon Jin, Junhwa Hur, Seungryong Kim | 2025-06-20 | Training & Evaluation, Diffusion Models, Prompt Engineering, Multimodal AI | Recent advancements in video diffusion models based on Diffusion Transformers (DiTs) have achieved remarkable success in generating temporally coherent videos. Yet, a fundamental question persists: how do these models internally establish and represent temporal correspondences across frames? We introduce DiffTrack, the first quantitative analysis framework designed to answer this question. DiffTrack constructs a dataset of prompt-generated video with pseudo ground-truth tracking annotations and proposes novel evaluation metrics to systematically analyze how each component within the full 3D attention mechanism of DiTs (e.g., representations, layers, and timesteps) contributes to establishing temporal correspondences. Our analysis reveals that query-key similarities in specific, but not all, layers play a critical role in temporal matching, and that this matching becomes increasingly prominent during the denoising process. We demonstrate practical applications of DiffTrack in zero-shot point tracking, where it achieves state-of-the-art performance compared to existing vision foundation and self-supervised video models. Further, we extend our findings to motion-enhanced video generation with a novel guidance method that improves temporal consistency of generated videos without additional training. We believe our work offers crucial insights into the inner workings of video DiTs and establishes a foundation for further research and applications leveraging their temporal understanding. | [ðŸ”— Paper](http://arxiv.org/abs/2506.17220v1) |
## ðŸ”¹ Model Evaluation

| ðŸ“„ Title | ðŸ–Š Authors | ðŸ“… Date | ðŸ· Tags | ðŸ“œ Summary | ðŸ”— Link |
|---------|---------|---------|---------|---------|---------|
| [Bias hardened estimators of patchy screening profiles](http://arxiv.org/abs/2506.17217v1) | Noah Sailer, Boryana Hadzhiyska, Simone Ferraro | 2025-06-20 | Model Evaluation, Responsible AI | Detecting anisotropic screening of the cosmic microwave background (CMB) holds the promise of revealing the distribution of gas in the Universe, characterizing the complex processes of galaxy formation and feedback, and studying the epoch of reionization. Estimators for inhomogeneous screening, including some recently proposed small-scale (stacked) estimators, are quadratic or higher order in the CMB temperature or polarization fields and are therefore subject to contamination from CMB lensing. We review the origin of this lensing bias and show that, when stacking on unWISE galaxies, the expected lensing bias dominates the signal if left unmitigated. Hardening techniques that null the lensing bias have been proposed for standard quadratic estimators, whereas only approximate methods have been proposed for stacked estimators. We review these techniques and apply the former to stacked estimators, presenting several strategies (including the optimal strategy) to null lensing contamination when stacking on any large-scale structure (LSS) tracer. | [ðŸ”— Paper](http://arxiv.org/abs/2506.17217v1) |
## ðŸ”¹ General AI

| ðŸ“„ Title | ðŸ–Š Authors | ðŸ“… Date | ðŸ· Tags | ðŸ“œ Summary | ðŸ”— Link |
|---------|---------|---------|---------|---------|---------|
| [Impact of the large-scale cosmic web on the X-ray emitting
  circumgalactic medium](http://arxiv.org/abs/2506.17222v1) | Soumya Shreeram, Daniela GalÃ¡rraga-Espinosa, Johan Comparat, Andrea Merloni, Daisuke Nagai, CÃ©line Peroux, Ilaria Marini, CÃ©line Gouin, Kirpal Nandra, Yi Zhang, Gabriele Ponti, Anna Olechowska | 2025-06-20 | General AI | The hot circumgalactic medium (CGM), probed by X-ray observations, plays a central role in understanding gas flows that drive a galaxy's evolution. While CGM properties have been widely studied, the influence of a galaxy's large-scale cosmic environment on the hot gas content remains less explored. We investigate how the large-scale cosmic web affects the X-ray surface brightness (XSB) profiles of galaxies in the context of cosmological simulations. We use our novel IllustrisTNG-based lightcone, spanning $0.03 \leq z \leq 0.3$, first developed in our previous work, and generate self-consistent mock X-ray observations, using intrinsic gas cell information. We apply the filament finder DisPerSE on the galaxy distributions to identify the cosmic filaments within the lightcone. We classify central galaxies into five distinct large-scale environment (LSE) categories: clusters and massive groups, cluster outskirts, filaments, filament-void transition regions, and voids/walls. We find that the X-ray surface brightness profiles (XSB) of central galaxies of dark matter halos in filaments with $M_{\rm 200m} >10^{12}\ M_\odot$ are X-ray brighter than those in voids and walls, with $20-45%$ deviations in the radial range of $\sim (0.3-0.5) R_{\rm 200m}$. We investigate the source of this enhancement and find that the filament galaxies show higher average gas densities, temperatures, and metallicities compared to voids/walls galaxies. Our results demonstrate that the impact of the large-scale cosmic environment is imprinted on the hot CGM's X-ray emission. Future theoretical work on studying the effect of assembly history, connectivity, and gas accretion on galaxies in filaments and voids would help to further our understanding of the impact of the environment on X-ray observations. | [ðŸ”— Paper](http://arxiv.org/abs/2506.17222v1) |
| [The full automorphism groups of the five symmetric $(15,8,4)$-designs](http://arxiv.org/abs/2506.17216v1) | Mark Pankov, Krzysztof Petelczyc, Mariusz Å»ynel | 2025-06-20 | General AI | It is clear that the full automorphism group of the $(15,8,4)$-design of points and hyperplane complements of ${\rm PG}(3,2)$ is ${\rm GL}(4,2)$. Using methods of point-line geometries, we determine the full automorphism groups of the remaining four symmetric $(15,8,4)$-designs. | [ðŸ”— Paper](http://arxiv.org/abs/2506.17216v1) |
| [Hierarchical constraints on gravitational waves from horizonless compact
  objects](http://arxiv.org/abs/2506.17215v1) | Rajrupa Mondal, Julian Westerweck, Yotam Sherf, Collin D. Capano | 2025-06-20 | General AI | We use the data of several promising gravitational wave observations to obtain increasingly stringent bounds on near-horizon deviations of their sources from the Kerr geometry. A range of horizonless compact objects proposed as alternatives to black holes of general relativity would possess a modified gravitational wave emission after the merger. Modelling these objects by introducing reflection of gravitational waves near the horizon, we can measure deviations from Kerr in terms of a single additional parameter, the location of the reflection. We quote bounds on deviations for 5 events in addition to previous results obtained for GW150914. Additionally, we improve upon previous results by hierarchically combining information from all analysed events, yielding a bound on deviations of less than $1.3 \times 10^{-26}$ meters above the horizon. | [ðŸ”— Paper](http://arxiv.org/abs/2506.17215v1) |
| [Long-term Traffic Simulation with Interleaved Autoregressive Motion and
  Scenario Generation](http://arxiv.org/abs/2506.17213v1) | Xiuyu Yang, Shuhan Tan, Philipp KrÃ¤henbÃ¼hl | 2025-06-20 | General AI | An ideal traffic simulator replicates the realistic long-term point-to-point trip that a self-driving system experiences during deployment. Prior models and benchmarks focus on closed-loop motion simulation for initial agents in a scene. This is problematic for long-term simulation. Agents enter and exit the scene as the ego vehicle enters new regions. We propose InfGen, a unified next-token prediction model that performs interleaved closed-loop motion simulation and scene generation. InfGen automatically switches between closed-loop motion simulation and scene generation mode. It enables stable long-term rollout simulation. InfGen performs at the state-of-the-art in short-term (9s) traffic simulation, and significantly outperforms all other methods in long-term (30s) simulation. The code and model of InfGen will be released at https://orangesodahub.github.io/InfGen | [ðŸ”— Paper](http://arxiv.org/abs/2506.17213v1) |
| [Regularized Targeted Maximum Likelihood Estimation in Highly Adaptive
  Lasso Implied Working Models](http://arxiv.org/abs/2506.17214v1) | Yi Li, Sky Qiu, Zeyi Wang, Mark van der Laan | 2025-06-20 | General AI | We address the challenge of performing Targeted Maximum Likelihood Estimation (TMLE) after an initial Highly Adaptive Lasso (HAL) fit. Existing approaches that utilize the data-adaptive working model selected by HAL-such as the relaxed HAL update-can be simple and versatile but may become computationally unstable when the HAL basis expansions introduce collinearity. Undersmoothed HAL may fail to solve the efficient influence curve (EIC) at the desired level without overfitting, particularly in complex settings like survival-curve estimation. A full HAL-TMLE, which treats HAL as the initial estimator and then targets in the nonparametric or semiparametric model, typically demands costly iterative clever-covariate calculations in complex set-ups like survival analysis and longitudinal mediation analysis. To overcome these limitations, we propose two new HAL-TMLEs that operate within the finite-dimensional working model implied by HAL: Delta-method regHAL-TMLE and Projection-based regHAL-TMLE. We conduct extensive simulations to demonstrate the performance of our proposed methods. | [ðŸ”— Paper](http://arxiv.org/abs/2506.17214v1) |
