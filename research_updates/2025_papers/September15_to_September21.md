# ðŸ“Œ AI Research Papers (September15 to September21)

## ðŸ”¹ Diffusion Models

| ðŸ“„ Title | ðŸ–Š Authors | ðŸ“… Date | ðŸ· Tags | ðŸ“œ Summary | ðŸ”— Link |
|---------|---------|---------|---------|---------|---------|
| [Lightweight and Accurate Multi-View Stereo with Confidence-Aware
  Diffusion Model](http://arxiv.org/abs/2509.15220v1) | Fangjinhua Wang, Qingshan Xu, Yew-Soon Ong, Marc Pollefeys | 2025-09-18 | Diffusion Models | To reconstruct the 3D geometry from calibrated images, learning-based multi-view stereo (MVS) methods typically perform multi-view depth estimation and then fuse depth maps into a mesh or point cloud. To improve the computational efficiency, many methods initialize a coarse depth map and then gradually refine it in higher resolutions. Recently, diffusion models achieve great success in generation tasks. Starting from a random noise, diffusion models gradually recover the sample with an iterative denoising process. In this paper, we propose a novel MVS framework, which introduces diffusion models in MVS. Specifically, we formulate depth refinement as a conditional diffusion process. Considering the discriminative characteristic of depth estimation, we design a condition encoder to guide the diffusion process. To improve efficiency, we propose a novel diffusion network combining lightweight 2D U-Net and convolutional GRU. Moreover, we propose a novel confidence-based sampling strategy to adaptively sample depth hypotheses based on the confidence estimated by diffusion model. Based on our novel MVS framework, we propose two novel MVS methods, DiffMVS and CasDiffMVS. DiffMVS achieves competitive performance with state-of-the-art efficiency in run-time and GPU memory. CasDiffMVS achieves state-of-the-art performance on DTU, Tanks & Temples and ETH3D. Code is available at: https://github.com/cvg/diffmvs. | [ðŸ”— Paper](http://arxiv.org/abs/2509.15220v1) |
| [Out-of-Sight Trajectories: Tracking, Fusion, and Prediction](http://arxiv.org/abs/2509.15219v1) | Haichao Zhang, Yi Xu, Yun Fu | 2025-09-18 | Diffusion Models | Trajectory prediction is a critical task in computer vision and autonomous systems, playing a key role in autonomous driving, robotics, surveillance, and virtual reality. Existing methods often rely on complete and noise-free observational data, overlooking the challenges associated with out-of-sight objects and the inherent noise in sensor data caused by limited camera coverage, obstructions, and the absence of ground truth for denoised trajectories. These limitations pose safety risks and hinder reliable prediction in real-world scenarios. In this extended work, we present advancements in Out-of-Sight Trajectory (OST), a novel task that predicts the noise-free visual trajectories of out-of-sight objects using noisy sensor data. Building on our previous research, we broaden the scope of Out-of-Sight Trajectory Prediction (OOSTraj) to include pedestrians and vehicles, extending its applicability to autonomous driving, robotics, surveillance, and virtual reality. Our enhanced Vision-Positioning Denoising Module leverages camera calibration to establish a vision-positioning mapping, addressing the lack of visual references, while effectively denoising noisy sensor data in an unsupervised manner. Through extensive evaluations on the Vi-Fi and JRDB datasets, our approach achieves state-of-the-art performance in both trajectory denoising and prediction, significantly surpassing previous baselines. Additionally, we introduce comparisons with traditional denoising methods, such as Kalman filtering, and adapt recent trajectory prediction models to our task, providing a comprehensive benchmark. This work represents the first initiative to integrate vision-positioning projection for denoising noisy sensor trajectories of out-of-sight agents, paving the way for future advances. The code and preprocessed datasets are available at github.com/Hai-chao-Zhang/OST | [ðŸ”— Paper](http://arxiv.org/abs/2509.15219v1) |
## ðŸ”¹ Multimodal AI

| ðŸ“„ Title | ðŸ–Š Authors | ðŸ“… Date | ðŸ· Tags | ðŸ“œ Summary | ðŸ”— Link |
|---------|---------|---------|---------|---------|---------|
| [Two Web Toolkits for Multimodal Piano Performance Dataset Acquisition
  and Fingering Annotation](http://arxiv.org/abs/2509.15222v1) | Junhyung Park, Yonghyun Kim, Joonhyung Bae, Kirak Kim, Taegyun Kwon, Alexander Lerch, Juhan Nam | 2025-09-18 | Multimodal AI | Piano performance is a multimodal activity that intrinsically combines physical actions with the acoustic rendition. Despite growing research interest in analyzing the multimodal nature of piano performance, the laborious process of acquiring large-scale multimodal data remains a significant bottleneck, hindering further progress in this field. To overcome this barrier, we present an integrated web toolkit comprising two graphical user interfaces (GUIs): (i) PiaRec, which supports the synchronized acquisition of audio, video, MIDI, and performance metadata. (ii) ASDF, which enables the efficient annotation of performer fingering from the visual data. Collectively, this system can streamline the acquisition of multimodal piano performance datasets. | [ðŸ”— Paper](http://arxiv.org/abs/2509.15222v1) |
| [ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform
  Data](http://arxiv.org/abs/2509.15221v2) | Zhaoyang Liu, Jingjing Xie, Zichen Ding, Zehao Li, Bowen Yang, Zhenyu Wu, Xuehui Wang, Qiushi Sun, Shi Liu, Weiyun Wang, Shenglong Ye, Qingyun Li, Xuan Dong, Yue Yu, Chenyu Lu, YunXiang Mo, Yao Yan, Zeyue Tian, Xiao Zhang, Yuan Huang, Yiqian Liu, Weijie Su, Gen Luo, Xiangyu Yue, Biqing Qi, Kai Chen, Bowen Zhou, Yu Qiao, Qifeng Chen, Wenhai Wang | 2025-09-18 | Multimodal AI, Scaling Laws | Vision-Language Models (VLMs) have enabled computer use agents (CUAs) that operate GUIs autonomously, showing great potential, yet progress is limited by the lack of large-scale, open-source computer use data and foundation models. In this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It offers a large-scale dataset spanning 6 operating systems and 3 task domains, built via a closed-loop pipeline uniting automated agents with human experts. Trained on this scaled-up data, ScaleCUA can operate seamlessly across platforms. Specifically, it delivers strong gains over baselines (+26.6 on WebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art results (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on WebArena-Lite-v2). These findings underscore the power of data-driven scaling for general-purpose computer use agents. We will release data, models, and code to advance future research: https://github.com/OpenGVLab/ScaleCUA. | [ðŸ”— Paper](http://arxiv.org/abs/2509.15221v2) |
## ðŸ”¹ Optimization

| ðŸ“„ Title | ðŸ–Š Authors | ðŸ“… Date | ðŸ· Tags | ðŸ“œ Summary | ðŸ”— Link |
|---------|---------|---------|---------|---------|---------|
| [Depth AnyEvent: A Cross-Modal Distillation Paradigm for Event-Based
  Monocular Depth Estimation](http://arxiv.org/abs/2509.15224v1) | Luca Bartolomei, Enrico Mannocci, Fabio Tosi, Matteo Poggi, Stefano Mattoccia | 2025-09-18 | Optimization, Responsible AI, Model Evaluation | Event cameras capture sparse, high-temporal-resolution visual information, making them particularly suitable for challenging environments with high-speed motion and strongly varying lighting conditions. However, the lack of large datasets with dense ground-truth depth annotations hinders learning-based monocular depth estimation from event data. To address this limitation, we propose a cross-modal distillation paradigm to generate dense proxy labels leveraging a Vision Foundation Model (VFM). Our strategy requires an event stream spatially aligned with RGB frames, a simple setup even available off-the-shelf, and exploits the robustness of large-scale VFMs. Additionally, we propose to adapt VFMs, either a vanilla one like Depth Anything v2 (DAv2), or deriving from it a novel recurrent architecture to infer depth from monocular event cameras. We evaluate our approach with synthetic and real-world datasets, demonstrating that i) our cross-modal paradigm achieves competitive performance compared to fully supervised methods without requiring expensive depth annotations, and ii) our VFM-based models achieve state-of-the-art performance. | [ðŸ”— Paper](http://arxiv.org/abs/2509.15224v1) |
## ðŸ”¹ Training & Evaluation

| ðŸ“„ Title | ðŸ–Š Authors | ðŸ“… Date | ðŸ· Tags | ðŸ“œ Summary | ðŸ”— Link |
|---------|---------|---------|---------|---------|---------|
| [LNE-Blocking: An Efficient Framework for Contamination Mitigation
  Evaluation on Large Language Models](http://arxiv.org/abs/2509.15218v1) | Ruijie Hou, Yueyang Jiao, Hanxu Hu, Yingming Li, Wai Lam, Huajian Zhang, Hongyuan Lu | 2025-09-18 | Training & Evaluation | The problem of data contamination is now almost inevitable during the development of large language models (LLMs), with the training data commonly integrating those evaluation benchmarks even unintentionally. This problem subsequently makes it hard to benchmark LLMs fairly. Instead of constructing contamination-free datasets (quite hard), we propose a novel framework, \textbf{LNE-Blocking}, to restore model performance prior to contamination on potentially leaked datasets. Our framework consists of two components: contamination detection and disruption operation. For the prompt, the framework first uses the contamination detection method, \textbf{LNE}, to assess the extent of contamination in the model. Based on this, it adjusts the intensity of the disruption operation, \textbf{Blocking}, to elicit non-memorized responses from the model. Our framework is the first to efficiently restore the model's greedy decoding performance. This comes with a strong performance on multiple datasets with potential leakage risks, and it consistently achieves stable recovery results across different models and varying levels of data contamination. We release the code at https://github.com/RuijieH/LNE-Blocking to facilitate research. | [ðŸ”— Paper](http://arxiv.org/abs/2509.15218v1) |
## ðŸ”¹ Prompt Engineering

| ðŸ“„ Title | ðŸ–Š Authors | ðŸ“… Date | ðŸ· Tags | ðŸ“œ Summary | ðŸ”— Link |
|---------|---------|---------|---------|---------|---------|
| [Lost in Translation? Vocabulary Alignment for Source-Free Domain
  Adaptation in Open-Vocabulary Semantic Segmentation](http://arxiv.org/abs/2509.15225v1) | Silvio Mazzucco, Carl Persson, Mattia Segu, Pier Luigi Dovesi, Federico Tombari, Luc Van Gool, Matteo Poggi | 2025-09-18 | Prompt Engineering, Optimization, RLHF | We introduce VocAlign, a novel source-free domain adaptation framework specifically designed for VLMs in open-vocabulary semantic segmentation. Our method adopts a student-teacher paradigm enhanced with a vocabulary alignment strategy, which improves pseudo-label generation by incorporating additional class concepts. To ensure efficiency, we use Low-Rank Adaptation (LoRA) to fine-tune the model, preserving its original capabilities while minimizing computational overhead. In addition, we propose a Top-K class selection mechanism for the student model, which significantly reduces memory requirements while further improving adaptation performance. Our approach achieves a notable 6.11 mIoU improvement on the CityScapes dataset and demonstrates superior performance on zero-shot segmentation benchmarks, setting a new standard for source-free adaptation in the open-vocabulary setting. | [ðŸ”— Paper](http://arxiv.org/abs/2509.15225v1) |
| [Calibration-Aware Prompt Learning for Medical Vision-Language Models](http://arxiv.org/abs/2509.15226v1) | Abhishek Basu, Fahad Shamshad, Ashshak Sharifdeen, Karthik Nandakumar, Muhammad Haris Khan | 2025-09-18 | Prompt Engineering, Multimodal AI | Medical Vision-Language Models (Med-VLMs) have demonstrated remarkable performance across diverse medical imaging tasks by leveraging large-scale image-text pretraining. However, their confidence calibration is largely unexplored, and so remains a significant challenge. As such, miscalibrated predictions can lead to overconfident errors, undermining clinical trust and decision-making reliability. To address this, we introduce CalibPrompt, the first framework to calibrate Med-VLMs during prompt tuning. CalibPrompt optimizes a small set of learnable prompts with carefully designed calibration objectives under scarce labeled data regime. First, we study a regularizer that attempts to align the smoothed accuracy with the predicted model confidences. Second, we introduce an angular separation loss to maximize textual feature proximity toward improving the reliability in confidence estimates of multimodal Med-VLMs. Extensive experiments on four publicly available Med-VLMs and five diverse medical imaging datasets reveal that CalibPrompt consistently improves calibration without drastically affecting clean accuracy. Our code is available at https://github.com/iabh1shekbasu/CalibPrompt. | [ðŸ”— Paper](http://arxiv.org/abs/2509.15226v1) |
## ðŸ”¹ Ongoing Learning

| ðŸ“„ Title | ðŸ–Š Authors | ðŸ“… Date | ðŸ· Tags | ðŸ“œ Summary | ðŸ”— Link |
|---------|---------|---------|---------|---------|---------|
| [Generalizable Geometric Image Caption Synthesis](http://arxiv.org/abs/2509.15217v1) | Yue Xin, Wenyuan Wang, Rui Pan, Ruida Wang, Howard Meng, Renjie Pi, Shizhe Diao, Tong Zhang | 2025-09-18 | Ongoing Learning, Multimodal AI, RLHF | Multimodal large language models have various practical applications that demand strong reasoning abilities. Despite recent advancements, these models still struggle to solve complex geometric problems. A key challenge stems from the lack of high-quality image-text pair datasets for understanding geometric images. Furthermore, most template-based data synthesis pipelines typically fail to generalize to questions beyond their predefined templates. In this paper, we bridge this gap by introducing a complementary process of Reinforcement Learning with Verifiable Rewards (RLVR) into the data generation pipeline. By adopting RLVR to refine captions for geometric images synthesized from 50 basic geometric relations and using reward signals derived from mathematical problem-solving tasks, our pipeline successfully captures the key features of geometry problem-solving. This enables better task generalization and yields non-trivial improvements. Furthermore, even in out-of-distribution scenarios, the generated dataset enhances the general reasoning capabilities of multimodal large language models, yielding accuracy improvements of $2.8\%\text{-}4.8\%$ in statistics, arithmetic, algebraic, and numerical tasks with non-geometric input images of MathVista and MathVerse, along with $2.4\%\text{-}3.9\%$ improvements in Art, Design, Tech, and Engineering tasks in MMMU. | [ðŸ”— Paper](http://arxiv.org/abs/2509.15217v1) |
## ðŸ”¹ General AI

| ðŸ“„ Title | ðŸ–Š Authors | ðŸ“… Date | ðŸ· Tags | ðŸ“œ Summary | ðŸ”— Link |
|---------|---------|---------|---------|---------|---------|
| [Parameter sensitivity of cosmic pairwise velocities in the non-linear
  regime of structure formation](http://arxiv.org/abs/2509.15223v1) | Jorge Enrique GarcÃ­a-Farieta, HÃ©ctor J. HortÃºa | 2025-09-18 | General AI | The peculiar velocities of dark matter tracers drive the growth of cosmic structures, providing a sensitive test of cosmological models and strengthening constraints on the nature of dark energy. In this work, we investigate the mean pairwise velocities, $v_{12}$, of dark matter tracers as a cosmological probe in the non-linear regime of cosmic structure formation. Using N-body dark matter-only simulations, we measure $v_{12}$ for pair separations up to 50 $h^{-1}$Mpc and model it by solving the pair conservation equation for a self-gravitating particle system, along with various prescriptions of the nonlinear matter power spectrum. We quantified the sensitivity of $v_{12}$ to variations in key cosmological parameters such as $\Omega_{\mathrm{m}}$, $\sigma_8$, $h$, $M_\nu$, and $w$. Our parameter inference analysis using MCMC shows sub-11% agreement with simulation data, with notable degeneracies, particularly between $\Omega_\mathrm{m}$ and $\sigma_8$. We further compute the stable clustering crossing scale across redshifts $z=0$, $0.5$, and $1$, assessing its dependence on cosmology. Among the tested power spectrum modeling approaches, we find that the CSSTEmu emulator provides the most accurate predictions, with deviations below 5% for $r > 10$ $h^{-1}$Mpc at $z=0.5$. Our results are validated using independent simulation suites, demonstrating that our framework offers a robust method for extracting cosmological constraints from upcoming peculiar velocity data. | [ðŸ”— Paper](http://arxiv.org/abs/2509.15223v1) |
