# ğŸ“Œ AI Research Papers (April14 to April20)

## ğŸ”¹ LLM

| ğŸ“„ Title | ğŸ–Š Authors | ğŸ“… Date | ğŸ· Tags | ğŸ“œ Summary | ğŸ”— Link |
|---------|---------|---------|---------|---------|---------|
| [CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for
  Language Model Pre-training](http://arxiv.org/abs/2504.13161v1) | Shizhe Diao, Yu Yang, Yonggan Fu, Xin Dong, Dan Su, Markus Kliegl, Zijia Chen, Peter Belcak, Yoshi Suhara, Hongxu Yin, Mostofa Patwary, Yingyan, Lin, Jan Kautz, Pavlo Molchanov | 2025-04-17 | LLM | Pre-training datasets are typically collected from web content and lack inherent domain divisions. For instance, widely used datasets like Common Crawl do not include explicit domain labels, while manually curating labeled datasets such as The Pile is labor-intensive. Consequently, identifying an optimal pre-training data mixture remains a challenging problem, despite its significant benefits for pre-training performance. To address these challenges, we propose CLustering-based Iterative Data Mixture Bootstrapping (CLIMB), an automated framework that discovers, evaluates, and refines data mixtures in a pre-training setting. Specifically, CLIMB embeds and clusters large-scale datasets in a semantic space and then iteratively searches for optimal mixtures using a smaller proxy model and a predictor. When continuously trained on 400B tokens with this mixture, our 1B model exceeds the state-of-the-art Llama-3.2-1B by 2.0%. Moreover, we observe that optimizing for a specific domain (e.g., Social Sciences) yields a 5% improvement over random sampling. Finally, we introduce ClimbLab, a filtered 1.2-trillion-token corpus with 20 clusters as a research playground, and ClimbMix, a compact yet powerful 400-billion-token dataset designed for efficient pre-training that delivers superior performance under an equal token budget. We analyze the final data mixture, elucidating the characteristics of an optimal data mixture. Our data is available at: https://research.nvidia.com/labs/lpr/climb/ | [ğŸ”— Paper](http://arxiv.org/abs/2504.13161v1) |
## ğŸ”¹ Diffusion Models

| ğŸ“„ Title | ğŸ–Š Authors | ğŸ“… Date | ğŸ· Tags | ğŸ“œ Summary | ğŸ”— Link |
|---------|---------|---------|---------|---------|---------|
| [SemCORE: A Semantic-Enhanced Generative Cross-Modal Retrieval Framework
  with MLLMs](http://arxiv.org/abs/2504.13172v1) | Haoxuan Li, Yi Bin, Yunshan Ma, Guoqing Wang, Yang Yang, See-Kiong Ng, Tat-Seng Chua | 2025-04-17 | Diffusion Models | Cross-modal retrieval (CMR) is a fundamental task in multimedia research, focused on retrieving semantically relevant targets across different modalities. While traditional CMR methods match text and image via embedding-based similarity calculations, recent advancements in pre-trained generative models have established generative retrieval as a promising alternative. This paradigm assigns each target a unique identifier and leverages a generative model to directly predict identifiers corresponding to input queries without explicit indexing. Despite its great potential, current generative CMR approaches still face semantic information insufficiency in both identifier construction and generation processes. To address these limitations, we propose a novel unified Semantic-enhanced generative Cross-mOdal REtrieval framework (SemCORE), designed to unleash the semantic understanding capabilities in generative cross-modal retrieval task. Specifically, we first construct a Structured natural language IDentifier (SID) that effectively aligns target identifiers with generative models optimized for natural language comprehension and generation. Furthermore, we introduce a Generative Semantic Verification (GSV) strategy enabling fine-grained target discrimination. Additionally, to the best of our knowledge, SemCORE is the first framework to simultaneously consider both text-to-image and image-to-text retrieval tasks within generative cross-modal retrieval. Extensive experiments demonstrate that our framework outperforms state-of-the-art generative cross-modal retrieval methods. Notably, SemCORE achieves substantial improvements across benchmark datasets, with an average increase of 8.65 points in Recall@1 for text-to-image retrieval. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13172v1) |
## ğŸ”¹ RLHF

| ğŸ“„ Title | ğŸ–Š Authors | ğŸ“… Date | ğŸ· Tags | ğŸ“œ Summary | ğŸ”— Link |
|---------|---------|---------|---------|---------|---------|
| [Long Range Navigator (LRN): Extending robot planning horizons beyond
  metric maps](http://arxiv.org/abs/2504.13149v1) | Matt Schmittle, Rohan Baijal, Nathan Hatch, Rosario Scalise, Mateo Guaman Castro, Sidharth Talia, Khimya Khetarpal, Byron Boots, Siddhartha Srinivasa | 2025-04-17 | RLHF | A robot navigating an outdoor environment with no prior knowledge of the space must rely on its local sensing to perceive its surroundings and plan. This can come in the form of a local metric map or local policy with some fixed horizon. Beyond that, there is a fog of unknown space marked with some fixed cost. A limited planning horizon can often result in myopic decisions leading the robot off course or worse, into very difficult terrain. Ideally, we would like the robot to have full knowledge that can be orders of magnitude larger than a local cost map. In practice, this is intractable due to sparse sensing information and often computationally expensive. In this work, we make a key observation that long-range navigation only necessitates identifying good frontier directions for planning instead of full map knowledge. To this end, we propose Long Range Navigator (LRN), that learns an intermediate affordance representation mapping high-dimensional camera images to `affordable' frontiers for planning, and then optimizing for maximum alignment with the desired goal. LRN notably is trained entirely on unlabeled ego-centric videos making it easy to scale and adapt to new platforms. Through extensive off-road experiments on Spot and a Big Vehicle, we find that augmenting existing navigation stacks with LRN reduces human interventions at test-time and leads to faster decision making indicating the relevance of LRN. https://personalrobotics.github.io/lrn | [ğŸ”— Paper](http://arxiv.org/abs/2504.13149v1) |
## ğŸ”¹ Multimodal AI

| ğŸ“„ Title | ğŸ–Š Authors | ğŸ“… Date | ğŸ· Tags | ğŸ“œ Summary | ğŸ”— Link |
|---------|---------|---------|---------|---------|---------|
| [Perception Encoder: The best visual embeddings are not at the output of
  the network](http://arxiv.org/abs/2504.13181v1) | Daniel Bolya, Po-Yao Huang, Peize Sun, Jang Hyun Cho, Andrea Madotto, Chen Wei, Tengyu Ma, Jiale Zhi, Jathushan Rajasegaran, Hanoona Rasheed, Junke Wang, Marco Monteiro, Hu Xu, Shiyu Dong, Nikhila Ravi, Daniel Li, Piotr DollÃ¡r, Christoph Feichtenhofer | 2025-04-17 | Multimodal AI, Scaling Laws, RLHF, Prompt Engineering | We introduce Perception Encoder (PE), a state-of-the-art encoder for image and video understanding trained via simple vision-language learning. Traditionally, vision encoders have relied on a variety of pretraining objectives, each tailored to specific downstream tasks such as classification, captioning, or localization. Surprisingly, after scaling our carefully tuned image pretraining recipe and refining with our robust video data engine, we find that contrastive vision-language training alone can produce strong, general embeddings for all of these downstream tasks. There is only one caveat: these embeddings are hidden within the intermediate layers of the network. To draw them out, we introduce two alignment methods, language alignment for multimodal language modeling, and spatial alignment for dense prediction. Together with the core contrastive checkpoint, our PE family of models achieves state-of-the-art performance on a wide variety of tasks, including zero-shot image and video classification and retrieval; document, image, and video Q&A; and spatial tasks such as detection, depth estimation, and tracking. To foster further research, we are releasing our models, code, and a novel dataset of synthetically and human-annotated videos. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13181v1) |
| [ODHSR: Online Dense 3D Reconstruction of Humans and Scenes from
  Monocular Videos](http://arxiv.org/abs/2504.13167v2) | Zetong Zhang, Manuel Kaufmann, Lixin Xue, Jie Song, Martin R. Oswald | 2025-04-17 | Multimodal AI, Ongoing Learning | Creating a photorealistic scene and human reconstruction from a single monocular in-the-wild video figures prominently in the perception of a human-centric 3D world. Recent neural rendering advances have enabled holistic human-scene reconstruction but require pre-calibrated camera and human poses, and days of training time. In this work, we introduce a novel unified framework that simultaneously performs camera tracking, human pose estimation and human-scene reconstruction in an online fashion. 3D Gaussian Splatting is utilized to learn Gaussian primitives for humans and scenes efficiently, and reconstruction-based camera tracking and human pose estimation modules are designed to enable holistic understanding and effective disentanglement of pose and appearance. Specifically, we design a human deformation module to reconstruct the details and enhance generalizability to out-of-distribution poses faithfully. Aiming to learn the spatial correlation between human and scene accurately, we introduce occlusion-aware human silhouette rendering and monocular geometric priors, which further improve reconstruction quality. Experiments on the EMDB and NeuMan datasets demonstrate superior or on-par performance with existing methods in camera tracking, human pose estimation, novel view synthesis and runtime. Our project page is at https://eth-ait.github.io/ODHSR. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13167v2) |
| [Personalized Text-to-Image Generation with Auto-Regressive Models](http://arxiv.org/abs/2504.13162v1) | Kaiyue Sun, Xian Liu, Yao Teng, Xihui Liu | 2025-04-17 | Multimodal AI, Optimization, Diffusion Models, LLM | Personalized image synthesis has emerged as a pivotal application in text-to-image generation, enabling the creation of images featuring specific subjects in diverse contexts. While diffusion models have dominated this domain, auto-regressive models, with their unified architecture for text and image modeling, remain underexplored for personalized image generation. This paper investigates the potential of optimizing auto-regressive models for personalized image synthesis, leveraging their inherent multimodal capabilities to perform this task. We propose a two-stage training strategy that combines optimization of text embeddings and fine-tuning of transformer layers. Our experiments on the auto-regressive model demonstrate that this method achieves comparable subject fidelity and prompt following to the leading diffusion-based personalization methods. The results highlight the effectiveness of auto-regressive models in personalized image generation, offering a new direction for future research in this area. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13162v1) |
| [St4RTrack: Simultaneous 4D Reconstruction and Tracking in the World](http://arxiv.org/abs/2504.13152v1) | Haiwen Feng, Junyi Zhang, Qianqian Wang, Yufei Ye, Pengcheng Yu, Michael J. Black, Trevor Darrell, Angjoo Kanazawa | 2025-04-17 | Multimodal AI | Dynamic 3D reconstruction and point tracking in videos are typically treated as separate tasks, despite their deep connection. We propose St4RTrack, a feed-forward framework that simultaneously reconstructs and tracks dynamic video content in a world coordinate frame from RGB inputs. This is achieved by predicting two appropriately defined pointmaps for a pair of frames captured at different moments. Specifically, we predict both pointmaps at the same moment, in the same world, capturing both static and dynamic scene geometry while maintaining 3D correspondences. Chaining these predictions through the video sequence with respect to a reference frame naturally computes long-range correspondences, effectively combining 3D reconstruction with 3D tracking. Unlike prior methods that rely heavily on 4D ground truth supervision, we employ a novel adaptation scheme based on a reprojection loss. We establish a new extensive benchmark for world-frame reconstruction and tracking, demonstrating the effectiveness and efficiency of our unified, data-driven framework. Our code, model, and benchmark will be released. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13152v1) |
| [PCBEAR: Pose Concept Bottleneck for Explainable Action Recognition](http://arxiv.org/abs/2504.13140v1) | Jongseo Lee, Wooil Lee, Gyeong-Moon Park, Seong Tae Kim, Jinwoo Choi | 2025-04-17 | Multimodal AI, AI Safety | Human action recognition (HAR) has achieved impressive results with deep learning models, but their decision-making process remains opaque due to their black-box nature. Ensuring interpretability is crucial, especially for real-world applications requiring transparency and accountability. Existing video XAI methods primarily rely on feature attribution or static textual concepts, both of which struggle to capture motion dynamics and temporal dependencies essential for action understanding. To address these challenges, we propose Pose Concept Bottleneck for Explainable Action Recognition (PCBEAR), a novel concept bottleneck framework that introduces human pose sequences as motion-aware, structured concepts for video action recognition. Unlike methods based on pixel-level features or static textual descriptions, PCBEAR leverages human skeleton poses, which focus solely on body movements, providing robust and interpretable explanations of motion dynamics. We define two types of pose-based concepts: static pose concepts for spatial configurations at individual frames, and dynamic pose concepts for motion patterns across multiple frames. To construct these concepts, PCBEAR applies clustering to video pose sequences, allowing for automatic discovery of meaningful concepts without manual annotation. We validate PCBEAR on KTH, Penn-Action, and HAA500, showing that it achieves high classification performance while offering interpretable, motion-driven explanations. Our method provides both strong predictive performance and human-understandable insights into the model's reasoning process, enabling test-time interventions for debugging and improving model behavior. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13140v1) |
## ğŸ”¹ Optimization

| ğŸ“„ Title | ğŸ–Š Authors | ğŸ“… Date | ğŸ· Tags | ğŸ“œ Summary | ğŸ”— Link |
|---------|---------|---------|---------|---------|---------|
| [ViTa-Zero: Zero-shot Visuotactile Object 6D Pose Estimation](http://arxiv.org/abs/2504.13179v1) | Hongyu Li, James Akl, Srinath Sridhar, Tye Brady, Taskin Padir | 2025-04-17 | Optimization, Prompt Engineering | Object 6D pose estimation is a critical challenge in robotics, particularly for manipulation tasks. While prior research combining visual and tactile (visuotactile) information has shown promise, these approaches often struggle with generalization due to the limited availability of visuotactile data. In this paper, we introduce ViTa-Zero, a zero-shot visuotactile pose estimation framework. Our key innovation lies in leveraging a visual model as its backbone and performing feasibility checking and test-time optimization based on physical constraints derived from tactile and proprioceptive observations. Specifically, we model the gripper-object interaction as a spring-mass system, where tactile sensors induce attractive forces, and proprioception generates repulsive forces. We validate our framework through experiments on a real-world robot setup, demonstrating its effectiveness across representative visual backbones and manipulation scenarios, including grasping, object picking, and bimanual handover. Compared to the visual models, our approach overcomes some drastic failure modes while tracking the in-hand object pose. In our experiments, our approach shows an average increase of 55% in AUC of ADD-S and 60% in ADD, along with an 80% lower position error compared to FoundationPose. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13179v1) |
| [PerceptionLM: Open-Access Data and Models for Detailed Visual
  Understanding](http://arxiv.org/abs/2504.13180v1) | Jang Hyun Cho, Andrea Madotto, Effrosyni Mavroudi, Triantafyllos Afouras, Tushar Nagarajan, Muhammad Maaz, Yale Song, Tengyu Ma, Shuming Hu, Suyog Jain, Miguel Martin, Huiyu Wang, Hanoona Rasheed, Peize Sun, Po-Yao Huang, Daniel Bolya, Nikhila Ravi, Shashank Jain, Tammy Stark, Shane Moon, Babak Damavandi, Vivian Lee, Andrew Westbury, Salman Khan, Philipp KrÃ¤henbÃ¼hl, Piotr DollÃ¡r, Lorenzo Torresani, Kristen Grauman, Christoph Feichtenhofer | 2025-04-17 | Optimization, Multimodal AI | Vision-language models are integral to computer vision research, yet many high-performing models remain closed-source, obscuring their data, design and training recipe. The research community has responded by using distillation from black-box models to label training data, achieving strong benchmark results, at the cost of measurable scientific progress. However, without knowing the details of the teacher model and its data sources, scientific progress remains difficult to measure. In this paper, we study building a Perception Language Model (PLM) in a fully open and reproducible framework for transparent research in image and video understanding. We analyze standard training pipelines without distillation from proprietary models and explore large-scale synthetic data to identify critical data gaps, particularly in detailed video understanding. To bridge these gaps, we release 2.8M human-labeled instances of fine-grained video question-answer pairs and spatio-temporally grounded video captions. Additionally, we introduce PLM-VideoBench, a suite for evaluating challenging video understanding tasks focusing on the ability to reason about "what", "where", "when", and "how" of a video. We make our work fully reproducible by providing data, training recipes, code & models. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13180v1) |
| [Aligning Constraint Generation with Design Intent in Parametric CAD](http://arxiv.org/abs/2504.13178v1) | Evan Casey, Tianyu Zhang, Shu Ishida, John Roger Thompson, Amir Khasahmadi, Joseph George Lambourne, Pradeep Kumar Jayaraman, Karl D. D. Willis | 2025-04-17 | Optimization, RLHF, Fine-Tuning, LLM | We adapt alignment techniques from reasoning LLMs to the task of generating engineering sketch constraints found in computer-aided design (CAD) models. Engineering sketches consist of geometric primitives (e.g. points, lines) connected by constraints (e.g. perpendicular, tangent) that define the relationships between them. For a design to be easily editable, the constraints must effectively capture design intent, ensuring the geometry updates predictably when parameters change. Although current approaches can generate CAD designs, an open challenge remains to align model outputs with design intent, we label this problem `design alignment'. A critical first step towards aligning generative CAD models is to generate constraints which fully-constrain all geometric primitives, without over-constraining or distorting sketch geometry. Using alignment techniques to train an existing constraint generation model with feedback from a constraint solver, we are able to fully-constrain 93% of sketches compared to 34% when using a na\"ive supervised fine-tuning (SFT) baseline and only 8.9% without alignment. Our approach can be applied to any existing constraint generation model and sets the stage for further research bridging alignment strategies between the language and design domains. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13178v1) |
| [IMAGGarment-1: Fine-Grained Garment Generation for Controllable Fashion
  Design](http://arxiv.org/abs/2504.13176v1) | Fei Shen, Jian Yu, Cong Wang, Xin Jiang, Xiaoyu Du, Jinhui Tang | 2025-04-17 | Optimization | This paper presents IMAGGarment-1, a fine-grained garment generation (FGG) framework that enables high-fidelity garment synthesis with precise control over silhouette, color, and logo placement. Unlike existing methods that are limited to single-condition inputs, IMAGGarment-1 addresses the challenges of multi-conditional controllability in personalized fashion design and digital apparel applications. Specifically, IMAGGarment-1 employs a two-stage training strategy to separately model global appearance and local details, while enabling unified and controllable generation through end-to-end inference. In the first stage, we propose a global appearance model that jointly encodes silhouette and color using a mixed attention module and a color adapter. In the second stage, we present a local enhancement model with an adaptive appearance-aware module to inject user-defined logos and spatial constraints, enabling accurate placement and visual consistency. To support this task, we release GarmentBench, a large-scale dataset comprising over 180K garment samples paired with multi-level design conditions, including sketches, color references, logo placements, and textual prompts. Extensive experiments demonstrate that our method outperforms existing baselines, achieving superior structural stability, color fidelity, and local controllability performance. The code and model are available at https://github.com/muzishen/IMAGGarment-1. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13176v1) |
| [It's All Connected: A Journey Through Test-Time Memorization,
  Attentional Bias, Retention, and Online Optimization](http://arxiv.org/abs/2504.13173v1) | Ali Behrouz, Meisam Razaviyayn, Peilin Zhong, Vahab Mirrokni | 2025-04-17 | Optimization, Responsible AI, Model Evaluation | Designing efficient and effective architectural backbones has been in the core of research efforts to enhance the capability of foundation models. Inspired by the human cognitive phenomenon of attentional bias-the natural tendency to prioritize certain events or stimuli-we reconceptualize neural architectures, including Transformers, Titans, and modern linear recurrent neural networks as associative memory modules that learn a mapping of keys and values using an internal objective, referred to as attentional bias. Surprisingly, we observed that most existing sequence models leverage either (1) dot-product similarity, or (2) L2 regression objectives as their attentional bias. Going beyond these objectives, we present a set of alternative attentional bias configurations along with their effective approximations to stabilize their training procedure. We then reinterpret forgetting mechanisms in modern deep learning architectures as a form of retention regularization, providing a novel set of forget gates for sequence models. Building upon these insights, we present Miras, a general framework to design deep learning architectures based on four choices of: (i) associative memory architecture, (ii) attentional bias objective, (iii) retention gate, and (iv) memory learning algorithm. We present three novel sequence models-Moneta, Yaad, and Memora-that go beyond the power of existing linear RNNs while maintaining a fast parallelizable training process. Our experiments show different design choices in Miras yield models with varying strengths. For example, certain instances of Miras achieve exceptional performance in special tasks such as language modeling, commonsense reasoning, and recall intensive tasks, even outperforming Transformers and other modern linear recurrent models. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13173v1) |
| [AerialMegaDepth: Learning Aerial-Ground Reconstruction and View
  Synthesis](http://arxiv.org/abs/2504.13157v1) | Khiem Vuong, Anurag Ghosh, Deva Ramanan, Srinivasa Narasimhan, Shubham Tulsiani | 2025-04-17 | Optimization, Prompt Engineering | We explore the task of geometric reconstruction of images captured from a mixture of ground and aerial views. Current state-of-the-art learning-based approaches fail to handle the extreme viewpoint variation between aerial-ground image pairs. Our hypothesis is that the lack of high-quality, co-registered aerial-ground datasets for training is a key reason for this failure. Such data is difficult to assemble precisely because it is difficult to reconstruct in a scalable way. To overcome this challenge, we propose a scalable framework combining pseudo-synthetic renderings from 3D city-wide meshes (e.g., Google Earth) with real, ground-level crowd-sourced images (e.g., MegaDepth). The pseudo-synthetic data simulates a wide range of aerial viewpoints, while the real, crowd-sourced images help improve visual fidelity for ground-level images where mesh-based renderings lack sufficient detail, effectively bridging the domain gap between real images and pseudo-synthetic renderings. Using this hybrid dataset, we fine-tune several state-of-the-art algorithms and achieve significant improvements on real-world, zero-shot aerial-ground tasks. For example, we observe that baseline DUSt3R localizes fewer than 5% of aerial-ground pairs within 5 degrees of camera rotation error, while fine-tuning with our data raises accuracy to nearly 56%, addressing a major failure point in handling large viewpoint changes. Beyond camera estimation and scene reconstruction, our dataset also improves performance on downstream tasks like novel-view synthesis in challenging aerial-ground scenarios, demonstrating the practical value of our approach in real-world applications. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13157v1) |
| [Training-Free Hierarchical Scene Understanding for Gaussian Splatting
  with Superpoint Graphs](http://arxiv.org/abs/2504.13153v1) | Shaohui Dai, Yansong Qu, Zheyan Li, Xinyang Li, Shengchuan Zhang, Liujuan Cao | 2025-04-17 | Optimization | Bridging natural language and 3D geometry is a crucial step toward flexible, language-driven scene understanding. While recent advances in 3D Gaussian Splatting (3DGS) have enabled fast and high-quality scene reconstruction, research has also explored incorporating open-vocabulary understanding into 3DGS. However, most existing methods require iterative optimization over per-view 2D semantic feature maps, which not only results in inefficiencies but also leads to inconsistent 3D semantics across views. To address these limitations, we introduce a training-free framework that constructs a superpoint graph directly from Gaussian primitives. The superpoint graph partitions the scene into spatially compact and semantically coherent regions, forming view-consistent 3D entities and providing a structured foundation for open-vocabulary understanding. Based on the graph structure, we design an efficient reprojection strategy that lifts 2D semantic features onto the superpoints, avoiding costly multi-view iterative training. The resulting representation ensures strong 3D semantic coherence and naturally supports hierarchical understanding, enabling both coarse- and fine-grained open-vocabulary perception within a unified semantic field. Extensive experiments demonstrate that our method achieves state-of-the-art open-vocabulary segmentation performance, with semantic field reconstruction completed over $30\times$ faster. Our code will be available at https://github.com/Atrovast/THGS. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13153v1) |
| [MIB: A Mechanistic Interpretability Benchmark](http://arxiv.org/abs/2504.13151v1) | Aaron Mueller, Atticus Geiger, Sarah Wiegreffe, Dana Arad, IvÃ¡n Arcuschin, Adam Belfki, Yik Siu Chan, Jaden Fiotto-Kaufman, Tal Haklay, Michael Hanna, Jing Huang, Rohan Gupta, Yaniv Nikankin, Hadas Orgad, Nikhil Prakash, Anja Reusch, Aruna Sankaranarayanan, Shun Shao, Alessandro Stolfo, Martin Tutek, Amir Zur, David Bau, Yonatan Belinkov | 2025-04-17 | Optimization, AI Safety, RLHF, Training & Evaluation | How can we know whether new mechanistic interpretability methods achieve real improvements? In pursuit of meaningful and lasting evaluation standards, we propose MIB, a benchmark with two tracks spanning four tasks and five models. MIB favors methods that precisely and concisely recover relevant causal pathways or specific causal variables in neural language models. The circuit localization track compares methods that locate the model components - and connections between them - most important for performing a task (e.g., attribution patching or information flow routes). The causal variable localization track compares methods that featurize a hidden vector, e.g., sparse autoencoders (SAEs) or distributed alignment search (DAS), and locate model features for a causal variable relevant to the task. Using MIB, we find that attribution and mask optimization methods perform best on circuit localization. For causal variable localization, we find that the supervised DAS method performs best, while SAE features are not better than neurons, i.e., standard dimensions of hidden vectors. These findings illustrate that MIB enables meaningful comparisons of methods, and increases our confidence that there has been real progress in the field. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13151v1) |
| [Antidistillation Sampling](http://arxiv.org/abs/2504.13146v1) | Yash Savani, Asher Trockman, Zhili Feng, Avi Schwarzschild, Alexander Robey, Marc Finzi, J. Zico Kolter | 2025-04-17 | Optimization | Frontier models that generate extended reasoning traces inadvertently produce rich token sequences that can facilitate model distillation. Recognizing this vulnerability, model owners may seek sampling strategies that limit the effectiveness of distillation without compromising model performance. \emph{Antidistillation sampling} provides exactly this capability. By strategically modifying a model's next-token probability distribution, antidistillation sampling poisons reasoning traces, rendering them significantly less effective for distillation while preserving the model's practical utility. For further details, see https://antidistillation.com. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13146v1) |
| [Exploring Expert Failures Improves LLM Agent Tuning](http://arxiv.org/abs/2504.13145v1) | Li-Cheng Lan, Andrew Bai, Minhao Cheng, Ruochen Wang, Cho-Jui Hsieh, Tianyi Zhou | 2025-04-17 | Optimization, LLM, Ongoing Learning | Large Language Models (LLMs) have shown tremendous potential as agents, excelling at tasks that require multiple rounds of reasoning and interactions. Rejection Sampling Fine-Tuning (RFT) has emerged as an effective method for finetuning LLMs as agents: it first imitates expert-generated successful trajectories and further improves agentic skills through iterative fine-tuning on successful, self-generated trajectories. However, since the expert (e.g., GPT-4) succeeds primarily on simpler subtasks and RFT inherently favors simpler scenarios, many complex subtasks remain unsolved and persistently out-of-distribution (OOD). Upon investigating these challenging subtasks, we discovered that previously failed expert trajectories can often provide valuable guidance, e.g., plans and key actions, that can significantly improve agent exploration efficiency and acquisition of critical skills. Motivated by these observations, we propose Exploring Expert Failures (EEF), which identifies beneficial actions from failed expert trajectories and integrates them into the training dataset. Potentially harmful actions are meticulously excluded to prevent contamination of the model learning process. By leveraging the beneficial actions in expert failures, EEF successfully solves some previously unsolvable subtasks and improves agent tuning performance. Remarkably, our approach achieved a 62\% win rate in WebShop, outperforming RFT (53. 6\%) and GPT-4 (35. 6\%), and to the best of our knowledge, setting a new state-of-the-art as the first method to surpass a score of 0.81 in WebShop and exceed 81 in SciWorld. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13145v1) |
## ğŸ”¹ Scaling Laws

| ğŸ“„ Title | ğŸ–Š Authors | ğŸ“… Date | ğŸ· Tags | ğŸ“œ Summary | ğŸ”— Link |
|---------|---------|---------|---------|---------|---------|
| [Sleep-time Compute: Beyond Inference Scaling at Test-time](http://arxiv.org/abs/2504.13171v1) | Kevin Lin, Charlie Snell, Yu Wang, Charles Packer, Sarah Wooders, Ion Stoica, Joseph E. Gonzalez | 2025-04-17 | Scaling Laws, Production and Deployment | Scaling test-time compute has emerged as a key ingredient for enabling large language models (LLMs) to solve difficult problems, but comes with high latency and inference cost. We introduce sleep-time compute, which allows models to "think" offline about contexts before queries are presented: by anticipating what queries users might ask and pre-computing useful quantities, we can significantly reduce the compute requirements at test-time. To demonstrate the efficacy of our method, we create modified versions of two reasoning tasks - Stateful GSM-Symbolic and Stateful AIME. We find that sleep-time compute can reduce the amount of test-time compute needed to achieve the same accuracy by ~ 5x on Stateful GSM-Symbolic and Stateful AIME and that by scaling sleep-time compute we can further increase accuracy by up to 13% on Stateful GSM-Symbolic and 18% on Stateful AIME. Furthermore, we introduce Multi-Query GSM-Symbolic, which extends GSM-Symbolic by including multiple related queries per context. By amortizing sleep-time compute across related queries about the same context using Multi-Query GSM-Symbolic, we can decrease the average cost per query by 2.5x. We then conduct additional analysis to understand when sleep-time compute is most effective, finding the predictability of the user query to be well correlated with the efficacy of sleep-time compute. Finally, we conduct a case-study of applying sleep-time compute to a realistic agentic SWE task. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13171v1) |
| [A New Semidefinite Relaxation for Linear and Piecewise-Affine Optimal
  Control with Time Scaling](http://arxiv.org/abs/2504.13170v1) | Lujie Yang, Tobia Marcucci, Pablo A. Parrilo, Russ Tedrake | 2025-04-17 | Scaling Laws | We introduce a semidefinite relaxation for optimal control of linear systems with time scaling. These problems are inherently nonconvex, since the system dynamics involves bilinear products between the discretization time step and the system state and controls. The proposed relaxation is closely related to the standard second-order semidefinite relaxation for quadratic constraints, but we carefully select a subset of the possible bilinear terms and apply a change of variables to achieve empirically tight relaxations while keeping the computational load light. We further extend our method to handle piecewise-affine (PWA) systems by formulating the PWA optimal-control problem as a shortest-path problem in a graph of convex sets (GCS). In this GCS, different paths represent different mode sequences for the PWA system, and the convex sets model the relaxed dynamics within each mode. By combining a tight convex relaxation of the GCS problem with our semidefinite relaxation with time scaling, we can solve PWA optimal-control problems through a single semidefinite program. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13170v1) |
| [Restoring Heisenberg scaling in time via autonomous quantum error
  correction](http://arxiv.org/abs/2504.13168v1) | Hyukgun Kwon, Uwe R. Fischer, Seung-Woo Lee, Liang Jiang | 2025-04-17 | Scaling Laws | We establish a sufficient condition under which autonomous quantum error correction (AutoQEC) can effectively restore Heisenberg scaling (HS) in quantum metrology. Specifically, we show that if all Lindblad operators associated with the noise commute with the signal Hamiltonian and a particular constrained linear equation admits a solution, then an ancilla-free AutoQEC scheme with finite $R$ (where $R$ represents the ratio between the engineered dissipation rate for AutoQEC and the noise rate,) can approximately preserve HS with desired small additive error $\epsilon > 0$ over any time interval $0 \leq t \leq T$. We emphasize that the error scales as $ \epsilon = O(\kappa T / R^c) $ with $c \geq 1$, indicating that the required $R$ decreases significantly with increasing $c$ to achieve a desired error. Furthermore, we discuss that if the sufficient condition is not satisfied, logical errors may be induced that cannot be efficiently corrected by the canonical AutoQEC framework. Finally, we numerically verify our analytical results by employing the concrete example of phase estimation under dephasing noise. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13168v1) |
## ğŸ”¹ Training & Evaluation

| ğŸ“„ Title | ğŸ–Š Authors | ğŸ“… Date | ğŸ· Tags | ğŸ“œ Summary | ğŸ”— Link |
|---------|---------|---------|---------|---------|---------|
| [Quantum algorithm for solving nonlinear differential equations based on
  physics-informed effective Hamiltonians](http://arxiv.org/abs/2504.13174v1) | Hsin-Yu Wu, Annie E. Paine, Evan Philip, Antonio A. Gentile, Oleksandr Kyriienko | 2025-04-17 | Training & Evaluation | We propose a distinct approach to solving linear and nonlinear differential equations (DEs) on quantum computers by encoding the problem into ground states of effective Hamiltonian operators. Our algorithm relies on constructing such operators in the Chebyshev space, where an effective Hamiltonian is a sum of global differential and data constraints. Once the effective Hamiltonian is formed, solutions of differential equations can be obtained using the ground state preparation techniques (e.g. imaginary-time evolution and quantum singular value transformation), bypassing variational search. Unlike approaches based on discrete grids, the algorithm enables evaluation of solutions beyond fixed grid points and implements constraints in the physics-informed way. Our proposal inherits the best traits from quantum machine learning-based DE solving (compact basis representation, automatic differentiation, nonlinearity) and quantum linear algebra-based approaches (fine-grid encoding, provable speed-up for state preparation), offering a robust strategy for quantum scientific computing in the early fault-tolerant era. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13174v1) |
## ğŸ”¹ Model Evaluation

| ğŸ“„ Title | ğŸ–Š Authors | ğŸ“… Date | ğŸ· Tags | ğŸ“œ Summary | ğŸ”— Link |
|---------|---------|---------|---------|---------|---------|
| [Generate, but Verify: Reducing Hallucination in Vision-Language Models
  with Retrospective Resampling](http://arxiv.org/abs/2504.13169v1) | Tsung-Han Wu, Heekyung Lee, Jiaxin Ge, Joseph E. Gonzalez, Trevor Darrell, David M. Chan | 2025-04-17 | Model Evaluation, Responsible AI, Multimodal AI | Vision-Language Models (VLMs) excel at visual understanding but often suffer from visual hallucinations, where they generate descriptions of nonexistent objects, actions, or concepts, posing significant risks in safety-critical applications. Existing hallucination mitigation methods typically follow one of two paradigms: generation adjustment, which modifies decoding behavior to align text with visual inputs, and post-hoc verification, where external models assess and correct outputs. While effective, generation adjustment methods often rely on heuristics and lack correction mechanisms, while post-hoc verification is complicated, typically requiring multiple models and tending to reject outputs rather than refine them. In this work, we introduce REVERSE, a unified framework that integrates hallucination-aware training with on-the-fly self-verification. By leveraging a new hallucination-verification dataset containing over 1.3M semi-synthetic samples, along with a novel inference-time retrospective resampling technique, our approach enables VLMs to both detect hallucinations during generation and dynamically revise those hallucinations. Our evaluations show that REVERSE achieves state-of-the-art hallucination reduction, outperforming the best existing methods by up to 12% on CHAIR-MSCOCO and 28% on HaloQuest. Our dataset, model, and code are available at: https://reverse-vlm.github.io. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13169v1) |
## ğŸ”¹ Prompt Engineering

| ğŸ“„ Title | ğŸ–Š Authors | ğŸ“… Date | ğŸ· Tags | ğŸ“œ Summary | ğŸ”— Link |
|---------|---------|---------|---------|---------|---------|
| [$\texttt{Complex-Edit}$: CoT-Like Instruction Generation for
  Complexity-Controllable Image Editing Benchmark](http://arxiv.org/abs/2504.13143v1) | Siwei Yang, Mude Hui, Bingchen Zhao, Yuyin Zhou, Nataniel Ruiz, Cihang Xie | 2025-04-17 | Prompt Engineering, LLM, Training & Evaluation | We introduce $\texttt{Complex-Edit}$, a comprehensive benchmark designed to systematically evaluate instruction-based image editing models across instructions of varying complexity. To develop this benchmark, we harness GPT-4o to automatically collect a diverse set of editing instructions at scale. Our approach follows a well-structured ``Chain-of-Edit'' pipeline: we first generate individual atomic editing tasks independently and then integrate them to form cohesive, complex instructions. Additionally, we introduce a suite of metrics to assess various aspects of editing performance, along with a VLM-based auto-evaluation pipeline that supports large-scale assessments. Our benchmark yields several notable insights: 1) Open-source models significantly underperform relative to proprietary, closed-source models, with the performance gap widening as instruction complexity increases; 2) Increased instructional complexity primarily impairs the models' ability to retain key elements from the input images and to preserve the overall aesthetic quality; 3) Decomposing a complex instruction into a sequence of atomic steps, executed in a step-by-step manner, substantially degrades performance across multiple metrics; 4) A straightforward Best-of-N selection strategy improves results for both direct editing and the step-by-step sequential approach; and 5) We observe a ``curse of synthetic data'': when synthetic data is involved in model training, the edited images from such models tend to appear increasingly synthetic as the complexity of the editing instructions rises -- a phenomenon that intriguingly also manifests in the latest GPT-4o outputs. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13143v1) |
## ğŸ”¹ Responsible AI

| ğŸ“„ Title | ğŸ–Š Authors | ğŸ“… Date | ğŸ· Tags | ğŸ“œ Summary | ğŸ”— Link |
|---------|---------|---------|---------|---------|---------|
| [Minute-long quantum coherence enabled by electrical depletion of
  magnetic noise](http://arxiv.org/abs/2504.13164v1) | Cyrus Zeledon, Benjamin Pingault, Jonathan C. Marcks, Mykyta Onizhuk, Yeghishe Tsaturyan, Yu-xin Wang, Benjamin S. Soloway, Hiroshi Abe, Misagh Ghezellou, Jawad Ul-Hassan, Takeshi Ohshima, Nguyen T. Son, F. Joseph Heremans, Giulia Galli, Christopher P. Anderson, David D. Awschalom | 2025-04-17 | Responsible AI, Model Evaluation | Integrating solid-state spin defects into classical electronic devices can enable new opportunities for quantum information processing that benefit from existing semiconductor technology. We show, through bias control of an isotopically purified silicon carbide (SiC) p-i-n diode, the depletion of not only electrical noise sources but also magnetic noise sources, resulting in record coherences for SiC electron spin qubits. We also uncover complementary improvements to the relaxation times of nuclear spin registers controllable by the defect, and measure diode-enhanced coherences. These improvements lead to record-long nuclear spin Hahn-echo times on the scale of minutes. These results demonstrate the power of materials control and electronic device integration to create highly coherent solid-state quantum network nodes and processors. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13164v1) |
| [Energy-Based Reward Models for Robust Language Model Alignment](http://arxiv.org/abs/2504.13134v1) | Anamika Lochab, Ruqi Zhang | 2025-04-17 | Responsible AI, Model Evaluation, Fine-Tuning, RLHF | Reward models (RMs) are essential for aligning Large Language Models (LLMs) with human preferences. However, they often struggle with capturing complex human preferences and generalizing to unseen data. To address these challenges, we introduce Energy-Based Reward Model (EBRM), a lightweight post-hoc refinement framework that enhances RM robustness and generalization. EBRM models the reward distribution explicitly, capturing uncertainty in human preferences and mitigating the impact of noisy or misaligned annotations. It achieves this through conflict-aware data filtering, label-noise-aware contrastive training, and hybrid initialization. Notably, EBRM enhances RMs without retraining, making it computationally efficient and adaptable across different models and tasks. Empirical evaluations on RM benchmarks demonstrate significant improvements in both robustness and generalization, achieving up to a 5.97% improvement in safety-critical alignment tasks compared to standard RMs. Furthermore, reinforcement learning experiments confirm that our refined rewards enhance alignment quality, effectively delaying reward hacking. These results demonstrate our approach as a scalable and effective enhancement for existing RMs and alignment pipelines. The code is available at EBRM. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13134v1) |
## ğŸ”¹ General AI

| ğŸ“„ Title | ğŸ–Š Authors | ğŸ“… Date | ğŸ· Tags | ğŸ“œ Summary | ğŸ”— Link |
|---------|---------|---------|---------|---------|---------|
| [Single-Shot Shape and Reflectance with Spatial Polarization Multiplexing](http://arxiv.org/abs/2504.13177v1) | Tomoki Ichikawa, Ryo Kawahara, Ko Nishino | 2025-04-17 | General AI | We propose spatial polarization multiplexing (SPM) for reconstructing object shape and reflectance from a single polarimetric image and demonstrate its application to dynamic surface recovery. Although single-pattern structured light enables single-shot shape reconstruction, the reflectance is challenging to recover due to the lack of angular sampling of incident light and the entanglement of the projected pattern and the surface color texture. We design a spatially multiplexed pattern of polarization that can be robustly and uniquely decoded for shape reconstruction by quantizing the AoLP values. At the same time, our spatial-multiplexing enables single-shot ellipsometry of linear polarization by projecting differently polarized light within a local region, which separates the specular and diffuse reflections for BRDF estimation. We achieve this spatial polarization multiplexing with a constrained de Bruijn sequence. Unlike single-pattern structured light with intensity and color, our polarization pattern is invisible to the naked eye and retains the natural surface appearance which is essential for accurate appearance modeling and also interaction with people. We experimentally validate our method on real data. The results show that our method can recover the shape, the Mueller matrix, and the BRDF from a single-shot polarimetric image. We also demonstrate the application of our method to dynamic surfaces. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13177v1) |
| [Novel Demonstration Generation with Gaussian Splatting Enables Robust
  One-Shot Manipulation](http://arxiv.org/abs/2504.13175v1) | Sizhe Yang, Wenye Yu, Jia Zeng, Jun Lv, Kerui Ren, Cewu Lu, Dahua Lin, Jiangmiao Pang | 2025-04-17 | General AI | Visuomotor policies learned from teleoperated demonstrations face challenges such as lengthy data collection, high costs, and limited data diversity. Existing approaches address these issues by augmenting image observations in RGB space or employing Real-to-Sim-to-Real pipelines based on physical simulators. However, the former is constrained to 2D data augmentation, while the latter suffers from imprecise physical simulation caused by inaccurate geometric reconstruction. This paper introduces RoboSplat, a novel method that generates diverse, visually realistic demonstrations by directly manipulating 3D Gaussians. Specifically, we reconstruct the scene through 3D Gaussian Splatting (3DGS), directly edit the reconstructed scene, and augment data across six types of generalization with five techniques: 3D Gaussian replacement for varying object types, scene appearance, and robot embodiments; equivariant transformations for different object poses; visual attribute editing for various lighting conditions; novel view synthesis for new camera perspectives; and 3D content generation for diverse object types. Comprehensive real-world experiments demonstrate that RoboSplat significantly enhances the generalization of visuomotor policies under diverse disturbances. Notably, while policies trained on hundreds of real-world demonstrations with additional 2D data augmentation achieve an average success rate of 57.2%, RoboSplat attains 87.8% in one-shot settings across six types of generalization in the real world. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13175v1) |
| [Topologically enabled superconductivity: possible implications for
  rhombohedral graphene](http://arxiv.org/abs/2504.13166v1) | Francesca Paoletti, Daniele Guerci, Giorgio Sangiovanni, Urban F. P. Seifert, Elio J. KÃ¶nig | 2025-04-17 | General AI | We present a topological mechanism for superconductivity emerging from Chern-2 insulators. While, naively, time-reversal symmetry breaking is expected to prevent superconductivity, it turns out that the opposite is the case: An explicit model calculation for a generalized attractive-U Haldane-Hubbard model demonstrates that superconductivity is only stabilized near the quantum anomalous Hall state, but not near a trivial, time-reversal symmetric band insulator. As standard Bardeen-Cooper-Schrieffer-like mean-field theory fails to capture any superconducting state, we explain this using an effective fractionalized field theory involving fermionic chargeons, bosonic colorons and an emergent U(1) gauge field. When the chargeons form a gapped topological band structure, the proliferation of single monopoles of this gauge field is forbidden. However, long-ranged monopole-antimonopole correlations emerge, and we argue that those correspond to superconducting order. Using random phase approximation on top of extensive slave-rotor mean-field calculations we characterize coherence length and stiffness of the superconductor. Thereby, we deduce the phase diagram in parameter space and furthermore discuss the effect of doping, temperature and an external magnetic field. We complement the fractionalized theory with calculations using an effective spin model and Gutzwiller projected wavefunctions. While mostly based on a simple toy model, we argue that our findings contribute to a better understanding of superconductivity emerging out of spin- and valley polarized rhombohedral graphene multilayers in a parameter regime with nearby quantum anomalous Hall insulators. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13166v1) |
| [RUKA: Rethinking the Design of Humanoid Hands with Learning](http://arxiv.org/abs/2504.13165v1) | Anya Zorin, Irmak Guzey, Billy Yan, Aadhithya Iyer, Lisa Kondrich, Nikhil X. Bhattasali, Lerrel Pinto | 2025-04-17 | General AI | Dexterous manipulation is a fundamental capability for robotic systems, yet progress has been limited by hardware trade-offs between precision, compactness, strength, and affordability. Existing control methods impose compromises on hand designs and applications. However, learning-based approaches present opportunities to rethink these trade-offs, particularly to address challenges with tendon-driven actuation and low-cost materials. This work presents RUKA, a tendon-driven humanoid hand that is compact, affordable, and capable. Made from 3D-printed parts and off-the-shelf components, RUKA has 5 fingers with 15 underactuated degrees of freedom enabling diverse human-like grasps. Its tendon-driven actuation allows powerful grasping in a compact, human-sized form factor. To address control challenges, we learn joint-to-actuator and fingertip-to-actuator models from motion-capture data collected by the MANUS glove, leveraging the hand's morphological accuracy. Extensive evaluations demonstrate RUKA's superior reachability, durability, and strength compared to other robotic hands. Teleoperation tasks further showcase RUKA's dexterous movements. The open-source design and assembly instructions of RUKA, code, and data are available at https://ruka-hand.github.io/. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13165v1) |
| [Experimental Verification of Electron-Photon Entanglement](http://arxiv.org/abs/2504.13163v1) | Alexander Preimesberger, Sergei Bogdanov, Isobel C. Bicket, Phila Rembold, Philipp Haslinger | 2025-04-17 | General AI | Entanglement, a key resource of emerging quantum technologies, describes correlations between particles that defy classical physics. It has been studied extensively on various platforms, but has remained elusive in electron microscopy. Transmission electron microscopes are well-established tools for materials characterisation with unparalleled spatial resolution. They provide control over the preparation and detection of high energy electrons, with largely unexploited potential in the study of many-body quantum correlations. Here, we demonstrate entanglement in electron-photon pairs generated via cathodoluminescence in a transmission electron microscope. Employing coincidence imaging techniques adapted from photonic quantum optics, we reconstruct both near- and far-field ``ghost'' images of periodic transmission masks. By measuring spatial and momentum correlations, we show a violation of the classical uncertainty bound: $\Delta x_-^2 \Delta k_+^2 = 0.502 \pm 0.047<1$. Hence, we demonstrate entanglement in position and momentum -- the continuous variables at the base of most imaging methods, bridging the fields of electron microscopy and quantum optics. Our work paves the way for exploring quantum correlations in free-electron systems and their application to quantum-enhanced imaging techniques on the nanoscale. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13163v1) |
| [Discovery and Dynamics of the Nontransiting Planet Kepler-139f](http://arxiv.org/abs/2504.13160v1) | Caleb Lammers, Joshua N. Winn | 2025-04-17 | General AI | Among the ways that an outer giant planet can alter the architecture of an inner planetary system is by tilting the orbits of the inner planets and reducing their mutual transit probabilities. Here, we report on an example of this phenomenon: we show that the Kepler-139 system contains a nontransiting planet just exterior to three transiting planets, and interior to a giant planet. This newly discovered planet, Kepler-139f, has an orbital period of $355 \pm 2$ days and a mass of $36 \pm 10 M_\oplus$ based on transit-timing and radial-velocity data. Through dynamical simulations, we show that gravitational perturbations on planet f's orbit from the outer giant planet reduce the probability for a randomly located observer to see transits of all four inner planets. Thus, Kepler-139 illustrates the role that outer giant planets can play in the apparent truncation of compact systems of multiple transiting planets. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13160v1) |
| [Digital Twin Generation from Visual Data: A Survey](http://arxiv.org/abs/2504.13159v1) | Andrew Melnik, Benjamin Alt, Giang Nguyen, Artur Wilkowski, Maciej StefaÅ„czyk, Qirui Wu, Sinan Harms, Helge Rhodin, Manolis Savva, Michael Beetz | 2025-04-17 | General AI | This survey explores recent developments in generating digital twins from videos. Such digital twins can be used for robotics application, media content creation, or design and construction works. We analyze various approaches, including 3D Gaussian Splatting, generative in-painting, semantic segmentation, and foundation models highlighting their advantages and limitations. Additionally, we discuss challenges such as occlusions, lighting variations, and scalability, as well as potential future research directions. This survey aims to provide a comprehensive overview of state-of-the-art methodologies and their implications for real-world applications. Awesome list: https://github.com/ndrwmlnk/awesome-digital-twins | [ğŸ”— Paper](http://arxiv.org/abs/2504.13159v1) |
| [Testing for dice control at craps](http://arxiv.org/abs/2504.13158v1) | Stewart N. Ethier | 2025-04-17 | General AI | Dice control involves "setting" the dice and then throwing them in a careful way, in the hope of influencing the outcomes and gaining an advantage at craps. How does one test for this ability? To specify the alternative hypothesis, we need a statistical model of dice control. Two have been suggested in the gambling literature, namely the Smith-Scott model and the Wong-Shackleford model. Both models are parameterized by $\theta\in[0,1]$, which measures the shooter's level of control. We propose and compare four test statistics: (a) the sample proportion of 7s; (b) the sample proportion of pass-line wins; (c) the sample mean of hand-length observations; and (d) the likelihood ratio statistic for a hand-length sample. We want to test $H_0:\theta = 0$ (no control) versus $H_1:\theta > 0$ (some control). We also want to test $H_0:\theta\le\theta_0$ versus $H_1:\theta>\theta_0$, where $\theta_0$ is the "break-even point." For the tests considered we estimate the power, either by normal approximation or by simulation. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13158v1) |
| [Gravitational wave anisotropies from axion inflation](http://arxiv.org/abs/2504.13156v1) | Sofia P. CorbÃ  | 2025-04-17 | General AI | An important prediction of inflation is the production of a primordial stochastic gravitational wave background. Observing this background is challenging due to the weakness of the signal and the simultaneous presence of an astrophysical background generated by many unresolved late-time sources. One possible way to distinguish between the two is to examine their anisotropies. In this paper we calculate the primordial correlation function of gravitational wave anisotropies in the cosmological background generated by axion inflation, where the inflaton is a pseudo-Nambu-Goldstone boson coupled to gauge fields. In this scenario, tensor modes arise not only from the standard amplification of vacuum fluctuations present in any inflationary model, but also from the inverse decay process of the produced gauge fields. The correlator of gravitational wave anisotropies consists therefore of two main components: the contribution from vacuum tensor modes and the contribution from tensor modes sourced by the gauge fields. Our analysis shows that, while the former, previously studied in the literature, is negligible, the one arising from the sourced tensor modes, normalized by the fractional energy density at interferometer frequencies, can reach values as large as $\mathcal{O}(10^{-1})$. This result shows that axion inflation can generate large anisotropies with the potential to be observed by gravitational wave detectors within a reasonable time frame. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13156v1) |
| [Compact KÃ¤hler manifolds with partially semi-positive curvature](http://arxiv.org/abs/2504.13155v1) | Shiyu Zhang, Xi Zhang | 2025-04-17 | General AI | In this paper, we establish a structure theorem for a compact K\"{a}hler manifold $X$ of rational dimension $\mathrm{rd}(X)\leq n-k$ under the mixed partially semi-positive curvature condition $\mathcal{S}_{a,b,k} \geq 0$, which is introduced as a unified framework for addressing two partially semi-positive curvature conditions -- namely, $k$-semi-positive Ricci curvature and semi-positive $k$-scalar curvature. As a main corollary, we show that a compact K\"{a}hler manifold $(X,g)$ with $k$-semi-positive Ricci curvature and $\mathrm{rd}(X)\leq n-k$ actually has semi-positive Ricci curvature and $\mathrm{rd}(X)\geq \nu(-K_X)$. Of independent interest, we also confirm the rational connectedness of compact K\"{a}hler manifolds with positive orthogonal Ricci curvature, among other results. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13155v1) |
| [Constraints on Anisotropic Cosmic Birefringence from CMB B-mode
  Polarization](http://arxiv.org/abs/2504.13154v1) | A. I. Lonappan, B. Keating, K. Arnold | 2025-04-17 | General AI | Cosmic birefringence$-$the rotation of the polarization plane of light as it traverses the universe$-$offers a direct observational window into parity-violating physics beyond the Standard Model. In this work, we revisit the anisotropic component of cosmic birefringence, which leads to the generation of $B$-mode polarization in the cosmic microwave background (CMB). Using an exact theoretical treatment beyond the thin last-scattering surface approximation, we constrain the amplitude of anisotropic birefringence with combined polarization data from SPTpol, ACT, POLARBEAR, and BICEP. The joint analysis yields a best-fit amplitude of $A_{\rm CB} = 0.42^{+0.40}_{-0.34} \times 10^{-4}$, consistent with zero within $2\sigma$, and we place a 95\% confidence-level upper bound of $A_{\rm CB} < 1 \times 10^{-4}$. The constraint is not dominated by any single experiment and remains robust under the inclusion of a possible isotropic rotation angle. These results provide leading constraints on anisotropic cosmic birefringence from CMB $B$-mode polarization and illustrate the potential of upcoming experiments to improve sensitivity to parity-violating effects in the early universe. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13154v1) |
| [Readable Twins of Unreadable Models](http://arxiv.org/abs/2504.13150v1) | Krzysztof Pancerz, Piotr Kulicki, MichaÅ‚ Kalisz, Andrzej Burda, Maciej StanisÅ‚awski, Jaromir SarzyÅ„ski | 2025-04-17 | General AI | Creating responsible artificial intelligence (AI) systems is an important issue in contemporary research and development of works on AI. One of the characteristics of responsible AI systems is their explainability. In the paper, we are interested in explainable deep learning (XDL) systems. On the basis of the creation of digital twins of physical objects, we introduce the idea of creating readable twins (in the form of imprecise information flow models) for unreadable deep learning models. The complete procedure for switching from the deep learning model (DLM) to the imprecise information flow model (IIFM) is presented. The proposed approach is illustrated with an example of a deep learning classification model for image recognition of handwritten digits from the MNIST data set. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13150v1) |
| [Relative entropy of squeezed states in Quantum Field Theory](http://arxiv.org/abs/2504.13148v1) | Marcelo S. Guimaraes, Itzhak Roditi, Silvio P. Sorella, Arthur F. Vieira | 2025-04-17 | General AI | Utilizing the Tomita-Takesaki modular theory, we derive a closed-form analytic expression for the Araki-Uhlmann relative entropy between a squeezed state and the vacuum state in a free relativistic massive scalar Quantum Field Theory within wedge regions of Minkowski spacetime. Similarly to the case of coherent states, this relative entropy is proportional to the smeared Pauli-Jordan distribution. Consequently, the Araki-Uhlmann entropy between a squeezed state and the vacuum satisfies all expected properties: it remains positive, increases with the size of the Minkowski region under consideration, and decreases as the mass parameter grows. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13148v1) |
| [Purely gravitational dark matter production in warm inflation](http://arxiv.org/abs/2504.13147v1) | Qing-Yang Wang, Tianyu Jia, Pei-Ran Chen, Yong Tang | 2025-04-17 | General AI | We consider an appealing scenario for the production of purely gravitational dark matter in the background of warm inflation, a mechanism that maintains stable thermal bath during inflation. Through systematic investigation of various gravitational production channels, we reveal distinctive features compared to the standard inflation scenario. Notably, the inflaton annihilation channel in warm inflation exhibits markedly different thermodynamics from the standard inflation paradigm, leading to a suppression on the production of sub-inflaton-mass dark matter. For the production channel of inflationary vacuum fluctuations, we find a correlation of $\rho_\chi\propto m_\chi^{5/2}$ for the conformally coupled dark matter, which expands the feasible range of dark matter mass. Our results also indicates that a minimum temperature threshold of $10^{-6}M_P$ is necessary for warm inflation, which allows adequate dark matter production. With observational constraints, our results provide stringent limits on the mass range of purely gravitational dark matter with sufficient density: $10^{-8}-10^{-2}M_P$ for minimal coupling and $10^{-14}-10^{-2}M_P$ for conformal coupling. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13147v1) |
| [Bayesian model-data comparison incorporating theoretical uncertainties](http://arxiv.org/abs/2504.13144v1) | Sunil Jaiswal, Chun Shen, Richard J. Furnstahl, Ulrich Heinz, Matthew T. Pratola | 2025-04-17 | General AI | Accurate comparisons between theoretical models and experimental data are critical for scientific progress. However, inferred model parameters can vary significantly with the chosen physics model, highlighting the importance of properly accounting for theoretical uncertainties. In this article, we explicitly incorporate these uncertainties using Gaussian processes that model the domain of validity of theoretical models, integrating prior knowledge about where a theory applies and where it does not. We demonstrate the effectiveness of this approach using two systems: a simple ball drop experiment and multi-stage heavy-ion simulations. In both cases incorporating model discrepancy leads to improved parameter estimates, with systematic improvements observed as additional experimental observables are integrated. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13144v1) |
| [Transfer Learning via Auxiliary Labels with Application to
  Cold-Hardiness Prediction](http://arxiv.org/abs/2504.13142v1) | Kristen Goebel, Paola Pesantez-Cabrera, Markus Keller, Alan Fern | 2025-04-17 | General AI | Cold temperatures can cause significant frost damage to fruit crops depending on their resilience, or cold hardiness, which changes throughout the dormancy season. This has led to the development of predictive cold-hardiness models, which help farmers decide when to deploy expensive frost-mitigation measures. Unfortunately, cold-hardiness data for model training is only available for some fruit cultivars due to the need for specialized equipment and expertise. Rather, farmers often do have years of phenological data (e.g. date of budbreak) that they regularly collect for their crops. In this work, we introduce a new transfer-learning framework, Transfer via Auxiliary Labels (TAL), that allows farmers to leverage the phenological data to produce more accurate cold-hardiness predictions, even when no cold-hardiness data is available for their specific crop. The framework assumes a set of source tasks (cultivars) where each has associated primary labels (cold hardiness) and auxiliary labels (phenology). However, the target task (new cultivar) is assumed to only have the auxiliary labels. The goal of TAL is to predict primary labels for the target task via transfer from the source tasks. Surprisingly, despite the vast literature on transfer learning, to our knowledge, the TAL formulation has not been previously addressed. Thus, we propose several new TAL approaches based on model selection and averaging that can leverage recent deep multi-task models for cold-hardiness prediction. Our results on real-world cold-hardiness and phenological data for multiple grape cultivars demonstrate that TAL can leverage the phenological data to improve cold-hardiness predictions in the absence of cold-hardiness data. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13142v1) |
| [Complexity at Scale: A Quantitative Analysis of an Alibaba Microservice
  Deployment](http://arxiv.org/abs/2504.13141v1) | Giles Winchester, George Parisis, Luc Berthouze | 2025-04-17 | General AI | Microservice architectures are increasingly prevalent in organisations providing online applications. Recent studies have begun to explore the characteristics of real-world large-scale microservice deployments; however, their operational complexities, and the degree to which this complexities are consistent across different deployments, remains under-explored. In this paper, we analyse a microservice dataset released by Alibaba along three dimensions of complexity: scale, heterogeneity, and dynamicity. We find that large-scale deployments can consist of tens of thousands of microservices, that support an even broader array of front-end functionality. Moreover, our analysis shows wide-spread long-tailed distributions of characteristics between microservices, such as share of workload and dependencies, highlighting inequality across the deployment. This diversity is also reflected in call graphs, where we find that whilst front-end services produce dominant call graphs, rarer non-dominant call graphs are prevalent and could involve dissimilar microservice calls. We also find that runtime dependencies between microservices deviate from the static view of system dependencies, and that the deployment undergoes daily changes to microservices. We discuss the implications of our findings for state-of-the-art research in microservice management and research testbed realism, and compare our results to previous descriptions of large-scale microservice deployments to begin to build an understanding of their commonalities. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13141v1) |
| [Syntactic and Semantic Control of Large Language Models via Sequential
  Monte Carlo](http://arxiv.org/abs/2504.13139v1) | JoÃ£o Loula, Benjamin LeBrun, Li Du, Ben Lipkin, Clemente Pasti, Gabriel Grand, Tianyu Liu, Yahya Emara, Marjorie Freedman, Jason Eisner, Ryan Cotterel, Vikash Mansinghka, Alexander K. Lew, Tim Vieira, Timothy J. O'Donnell | 2025-04-17 | General AI | A wide range of LM applications require generating text that conforms to syntactic or semantic constraints. Imposing such constraints can be naturally framed as probabilistic conditioning, but exact generation from the resulting distribution -- which can differ substantially from the LM's base distribution -- is generally intractable. In this work, we develop an architecture for controlled LM generation based on sequential Monte Carlo (SMC). Our SMC framework allows us to flexibly incorporate domain- and problem-specific constraints at inference time, and efficiently reallocate computational resources in light of new information during the course of generation. By comparing to a number of alternatives and ablations on four challenging domains -- Python code generation for data science, text-to-SQL, goal inference, and molecule synthesis -- we demonstrate that, with little overhead, our approach allows small open-source language models to outperform models over 8x larger, as well as closed-source, fine-tuned ones. In support of the probabilistic perspective, we show that these performance improvements are driven by better approximation to the posterior distribution. Our system builds on the framework of Lew et al. (2023) and integrates with its language model probabilistic programming language, giving users a simple, programmable way to apply SMC to a broad variety of controlled generation problems. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13139v1) |
| [Extending the Mott-Gurney law to one-dimensional nonplanar diodes using
  point transformations](http://arxiv.org/abs/2504.13138v1) | Allen L. Garner, N. R. Sree Harsha, Amanda M. Loveless | 2025-04-17 | General AI | Recent studies have applied variational calculus, conformal mapping, and point transformations to generalize the one-dimensional (1D) space-charge limited current density (SCLCD) and electron emission mechanisms to nonplanar geometries; however, these assessments have focused on extending the Child-Langmuir law (CLL) for SCLCD in vacuum. Since the charge in the diode is independent of coordinate system (i.e., covariant), we apply bijective point transformations to extend the Mott-Gurney law (MGL) for the SCLCD in a collisional or semiconductor gap to nonplanar 1D geometries. This yields a modified MGL that replaces the Cartesian gap distance with a canonical gap distance that may be written generally in terms of geometric scale factors that are known for multiple geometries. We tabulate results for common geometries. Such an approach may be applied to any current density, including non-space-charge limited gaps and SCLCD that may fall between the CLL and MGL. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13138v1) |
| [Integral formulas for hypersurfaces in cones and related questions](http://arxiv.org/abs/2504.13137v1) | Filomena Pacella, Giulio Tralli | 2025-04-17 | General AI | We discuss the validity of Minkowski integral identities for hypersurfaces inside a cone, intersecting the boundary of the cone orthogonally. In doing so we correct a formula provided in [3]. Then we study rigidity results for constant mean curvature graphs proving the precise statement of a result given in [9] and [10]. Finally we provide an integral estimate for stable constant mean curvature hypersurfaces in cones. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13137v1) |
| [Freezing of the renormalized one-loop primordial scalar power spectrum](http://arxiv.org/abs/2504.13136v1) | Matteo Braglia, Lucas Pinol | 2025-04-17 | General AI | By consistently using the effective field theory of inflationary fluctuations in the decoupling limit, we explicitly prove that the renormalized one-loop power spectrum of the primordial curvature perturbation freezes exactly on scales larger than its sound horizon. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13136v1) |
| [Probing CP-Violating Neutral Triple Gauge Couplings at Electron-Positron
  Colliders](http://arxiv.org/abs/2504.13135v1) | John Ellis, Hong-Jian He, Rui-Qing Xiao | 2025-04-17 | General AI | We study the forms of CP-violating (CPV) neutral triple gauge couplings (nTGCs) that can be realized via dimension-8 operators in the Standard Model Effective Field Theory (SMEFT). We present a new formulation of the CPV nTGC form factors that is compatible with the spontaneous breaking of the electroweak gauge symmetry, and show how these CPV form factors can be matched consistently with the corresponding dimension-8 CPV nTGC operators in the broken phase. We then study probes of the CPV nTGCs at future high-energy $e^+e^-$ colliders with centre-of-mass energies $\sqrt{s} = (0.25, 0.5,1, 3, 5)$TeV respectively, demonstrating that the $e^{\mp}$ beam polarizations can help to improve the sensitivities of probes of the nTGCs. We estimate that the sensitivities for probing the new physics scales of the nTGCs can range from ${O}(\rm{TeV})$ at a 250GeV $e^+e^-$ collider to ${O}(10\rm{TeV})$ at a 5TeV $e^+e^-$ collider, and that the sensitivities to form factors range from ${O}(10^{-4})$ to ${O}(10^{-8})$. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13135v1) |
| [Giant nematic response of the incommensurate charge density wave in the
  nickel-pnictide Ba$_{1-x}$Sr$_x$Ni$_2$As$_2$](http://arxiv.org/abs/2504.13133v1) | Thomas Johnson, Sangjun Lee, Camille Bernal-Choban, Xuefei Guo, Stella Sun, John Collini, Christopher Eckberg, Johnpierre Paglione, Rafael M. Fernandes, Eduardo Fradkin, Peter Abbamonte | 2025-04-17 | General AI | Electron nematicity-the breaking of rotational symmetry while preserving translational symmetry-is the quantum analogue of classical nematic liquid crystals. First predicted in 1998, electronic nematicity has been established in a variety of materials, including two-dimensional electron gases (2DEGs) in magnetic fields, copper-oxide superconductors, and Fe-based superconductors. A long-standing open question is what physical mechanisms drive electronic nematic order. In BaFe$_2$As$_2$ and highly underdoped YBa$_2$Cu$_3$O$_{6+y}$, strong evidence suggests that nematicity arises from vestigial spin-density-wave (SDW) order. However, evidence for nematicity associated with charge-density-wave (CDW) order has been less conclusive, particularly in systems near a superconducting state. Here, we present direct evidence for CDW-driven nematic fluctuations in the pnictide superconductor Ba$_{1-x}$Sr$_x$Ni$_2$As$_2$ (BSNA), a Ni-based homologue of Fe-based superconductors that exhibits CDW rather than SDW order. Previous elastoresistance studies have shown that BSNA displays a large nematic susceptibility-linked to a six-fold enhancement of superconductivity-within a region of the phase diagram occupied by an incommensurate CDW. Using x-ray scattering under uniaxial strain, we demonstrate that even minimal strain levels ($\epsilon \sim 10^{-4}$) significantly break the fourfold symmetry of the CDW. Within a Ginzburg-Landau framework, we define a nematic susceptibility based on the asymmetric response of symmetry-related CDW superlattice reflections, showing strong agreement with elastoresistivity measurements. Our study provides the first clear demonstration of a direct link between charge order and a nematic state, offering key insights into the intertwined superconducting phases of these materials. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13133v1) |
| [Cosmological Parameters Estimate from Persistent Radio Sources of Fast
  Radio Bursts](http://arxiv.org/abs/2504.13132v1) | Zi-Liang Zhang, Bing Zhang | 2025-04-17 | General AI | We introduce a novel method to constrain the Hubble constant ($H_0$) by combining fast radio bursts (FRBs) and their persistent radio sources (PRSs) through the observationally validated Yang relation, $ L_{\nu} \propto   \mathrm{RM}   $, which links PRS luminosity to the rotation measure (RM) of the associated FRB. Using a mock sample of PRSs, we demonstrate that the Yang relation can help to unravel the degeneracies among $H_0$, baryon density parameter $\Omega_b$, and baryon fraction in the intergalactic medium $f_{\mathrm{IGM}}$ in the traditional approach of using dispersion measure only to perform cosmological analyses. Our method employs a two-stage Markov Chain Monte Carlo (MCMC) analysis to constrain $H_0$. Using the available data of six observed PRS systems, we obtain a preliminary constraint of $H_0 = 75 \pm 30~\mathrm{km\,s^{-1}\,Mpc^{-1}}$. We briefly discuss possible refinements of the method by reducing residual degeneracies and systematic uncertainties using future data and physical modeling. Our results indicate that the Yang relation can potentially become a new probe for performing FRB cosmology. | [ğŸ”— Paper](http://arxiv.org/abs/2504.13132v1) |
