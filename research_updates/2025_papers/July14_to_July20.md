# üìå AI Research Papers (July14 to July20)

## üîπ RLHF

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement
  Learning](http://arxiv.org/abs/2507.14111v1) | Xiaoya Li, Xiaofei Sun, Albert Wang, Jiwei Li, Chris Shum | 2025-07-18 | RLHF, Optimization | The exponential growth in demand for GPU computing resources, driven by the rapid advancement of Large Language Models, has created an urgent need for automated CUDA optimization strategies. While recent advances in LLMs show promise for code generation, current SOTA models (e.g. R1, o1) achieve low success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an automated reinforcement learning framework for CUDA optimization.   CUDA-L1 achieves performance improvements on the CUDA optimization task: trained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250 CUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the model also demonstrates excellent portability across GPU architectures, achieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40, x14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100. Beyond these benchmark results, CUDA-L1 demonstrates several remarkable properties: 1) Discovers a variety of CUDA optimization techniques and learns to combine them strategically to achieve optimal performance; 2) Uncovers fundamental principles of CUDA optimization; 3) Identifies non-obvious performance bottlenecks and rejects seemingly beneficial optimizations that harm performance.   The capabilities of CUDA-L1 demonstrate that reinforcement learning can transform an initially poor-performing LLM into an effective CUDA optimizer through speedup-based reward signals alone, without human expertise or domain knowledge. More importantly, the trained RL model extend the acquired reasoning abilities to new kernels. This paradigm opens possibilities for automated optimization of CUDA operations, and holds promise to substantially promote GPU efficiency and alleviate the rising pressure on GPU computing resources. | [üîó Paper](http://arxiv.org/abs/2507.14111v1) |
| [Generative AI-Driven High-Fidelity Human Motion Simulation](http://arxiv.org/abs/2507.14097v1) | Hari Iyer, Neel Macwan, Atharva Jitendra Hude, Heejin Jeong, Shenghan Guo | 2025-07-18 | RLHF, Training & Evaluation | Human motion simulation (HMS) supports cost-effective evaluation of worker behavior, safety, and productivity in industrial tasks. However, existing methods often suffer from low motion fidelity. This study introduces Generative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and text-to-motion models to enhance simulation quality for physical tasks. G-AI-HMS tackles two key challenges: (1) translating task descriptions into motion-aware language using Large Language Models aligned with MotionGPT's training vocabulary, and (2) validating AI-enhanced motions against real human movements using computer vision. Posture estimation algorithms are applied to real-time videos to extract joint landmarks, and motion similarity metrics are used to compare them with AI-enhanced sequences. In a case study involving eight tasks, the AI-enhanced motions showed lower error than human created descriptions in most scenarios, performing better in six tasks based on spatial accuracy, four tasks based on alignment after pose normalization, and seven tasks based on overall temporal similarity. Statistical analysis showed that AI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and temporal misalignment while retaining comparable posture accuracy. | [üîó Paper](http://arxiv.org/abs/2507.14097v1) |
## üîπ Multimodal AI

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Franca: Nested Matryoshka Clustering for Scalable Visual Representation
  Learning](http://arxiv.org/abs/2507.14137v1) | Shashanka Venkataramanan, Valentinos Pariza, Mohammadreza Salehi, Lukas Knobel, Spyros Gidaris, Elias Ramzi, Andrei Bursuc, Yuki M. Asano | 2025-07-18 | Multimodal AI | We present Franca (pronounced Fran-ka): free one; the first fully open-source (data, code, weights) vision foundation model that matches and in many cases surpasses the performance of state-of-the-art proprietary models, e.g., DINOv2, CLIP, SigLIPv2, etc. Our approach is grounded in a transparent training pipeline inspired by Web-SSL and uses publicly available data: ImageNet-21K and a subset of ReLAION-2B. Beyond model release, we tackle critical limitations in SSL clustering methods. While modern models rely on assigning image features to large codebooks via clustering algorithms like Sinkhorn-Knopp, they fail to account for the inherent ambiguity in clustering semantics. To address this, we introduce a parameter-efficient, multi-head clustering projector based on nested Matryoshka representations. This design progressively refines features into increasingly fine-grained clusters without increasing the model size, enabling both performance and memory efficiency. Additionally, we propose a novel positional disentanglement strategy that explicitly removes positional biases from dense representations, thereby improving the encoding of semantic content. This leads to consistent gains on several downstream benchmarks, demonstrating the utility of cleaner feature spaces. Our contributions establish a new standard for transparent, high-performance vision models and open a path toward more reproducible and generalizable foundation models for the broader AI community. The code and model checkpoints are available at https://github.com/valeoai/Franca. | [üîó Paper](http://arxiv.org/abs/2507.14137v1) |
| [OpenBEATs: A Fully Open-Source General-Purpose Audio Encoder](http://arxiv.org/abs/2507.14129v1) | Shikhar Bharadwaj, Samuele Cornell, Kwanghee Choi, Satoru Fukayama, Hye-jin Shim, Soham Deshmukh, Shinji Watanabe | 2025-07-18 | Multimodal AI, Training & Evaluation | Masked token prediction has emerged as a powerful pre-training objective across language, vision, and speech, offering the potential to unify these diverse modalities through a single pre-training task. However, its application for general audio understanding remains underexplored, with BEATs being the only notable example. BEATs has seen limited modifications due to the absence of open-source pre-training code. Furthermore, BEATs was trained only on AudioSet, restricting its broader downstream applicability. To address these gaps, we present OpenBEATs, an open-source framework that extends BEATs via multi-domain audio pre-training. We conduct comprehensive evaluations across six types of tasks, twenty five datasets, and three audio domains, including audio reasoning tasks such as audio question answering, entailment, and captioning. OpenBEATs achieves state-of-the-art performance on six bioacoustics datasets, two environmental sound datasets and five reasoning datasets, performing better than models exceeding a billion parameters at one-fourth their parameter size. These results demonstrate the effectiveness of multi-domain datasets and masked token prediction task to learn general-purpose audio representations. To promote further research and reproducibility, we release all pre-training and evaluation code, pretrained and fine-tuned checkpoints, and training logs at https://shikhar-s.github.io/OpenBEATs | [üîó Paper](http://arxiv.org/abs/2507.14129v1) |
## üîπ Optimization

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Learning the non-Markovian features of subsystem dynamics](http://arxiv.org/abs/2507.14133v1) | Michele Coppola, Mari Carmen Ba√±uls, Zala Lenarƒçiƒç | 2025-07-18 | Optimization | The dynamics of local observables in a quantum many-body system can be formally described in the language of open systems. The problem is that the bath representing the complement of the local subsystem generally does not allow the common simplifications often crucial for such a framework. Leveraging tensor network calculations and optimization tools from machine learning, we extract and characterize the dynamical maps for single- and two-site subsystems embedded in an infinite quantum Ising chain after a global quench. We consider three paradigmatic regimes: integrable critical, integrable non-critical, and chaotic. For each we find the optimal time-local representation of the subsystem dynamics at different times. We explore the properties of the learned time-dependent Liouvillians and whether they can be used to forecast the long-time dynamics of local observables beyond the times accessible through direct quantum many-body numerical simulation. Our procedure naturally suggests a novel measure of non-Markovianity based on the distance between the quasi-exact dynamical map and the closest CP-divisible form and reveals that criticality leads to the closest Markovian representation at large times. | [üîó Paper](http://arxiv.org/abs/2507.14133v1) |
| [Integrating Forecasting Models Within Steady-State Analysis and
  Optimization](http://arxiv.org/abs/2507.14117v1) | Aayushya Agarwal, Larry Pileggi | 2025-07-18 | Optimization | Extreme weather variations and the increasing unpredictability of load behavior make it difficult to determine power grid dispatches that are robust to uncertainties. While machine learning (ML) methods have improved the ability to model uncertainty caused by loads and renewables, accurately integrating these forecasts and their sensitivities into steady-state analyses and decision-making strategies remains an open challenge. Toward this goal, we present a generalized methodology that seamlessly embeds ML-based forecasting engines within physics-based power flow and grid optimization tools. By coupling physics-based grid modeling with black-box ML methods, we accurately capture the behavior and sensitivity of loads and weather events by directly integrating the inputs and outputs of trained ML forecasting models into the numerical methods of power flow and grid optimization. Without fitting surrogate load models, our approach obtains the sensitivities directly from data to accurately predict the response of forecasted devices to changes in the grid. Our approach combines the sensitivities of forecasted devices attained via backpropagation and the sensitivities of physics-defined grid devices. We demonstrate the efficacy of our method by showcasing improvements in sensitivity calculations and leveraging them to design a robust power dispatch that improves grid reliability under stochastic weather events. Our approach enables the computation of system sensitivities to exogenous factors which supports broader analyses that improve grid reliability in the presence of load variability and extreme weather conditions. | [üîó Paper](http://arxiv.org/abs/2507.14117v1) |
## üîπ Scaling Laws

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Global $q$-dependent inverse transforms of intensity autocorrelation
  data](http://arxiv.org/abs/2507.14106v1) | Tobias Eklund, Christina M. Tonauer, Felix Lehmk√ºhler, Katrin Amann-Winkel | 2025-07-18 | Scaling Laws, Diffusion Models | We present a new analysis approach for intensity autocorrelation data, as measured with dynamic light scattering and X-ray photon correlation spectroscopy. Our analysis generalizes the established CONTIN and MULTIQ methods by direct nonlinear modeling of the $g_2$ function, enabling decomposition of complex dynamics without a priori knowledge of experimental scaling factors. We describe the mathematical formulation, implementation details, and strategies for solution, as well as demonstrate decompositions of soft matter dynamics data into distributions of diffusion rates/velocities. The open-source MATLAB implementation, including example data, is publicly available for adoption and further development. | [üîó Paper](http://arxiv.org/abs/2507.14106v1) |
## üîπ Training & Evaluation

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Kolmogorov Arnold Networks (KANs) for Imbalanced Data -- An Empirical
  Perspective](http://arxiv.org/abs/2507.14121v1) | Pankaj Yadav, Vivek Vijay | 2025-07-18 | Training & Evaluation | Kolmogorov Arnold Networks (KANs) are recent architectural advancement in neural computation that offer a mathematically grounded alternative to standard neural networks. This study presents an empirical evaluation of KANs in context of class imbalanced classification, using ten benchmark datasets. We observe that KANs can inherently perform well on raw imbalanced data more effectively than Multi-Layer Perceptrons (MLPs) without any resampling strategy. However, conventional imbalance strategies fundamentally conflict with KANs mathematical structure as resampling and focal loss implementations significantly degrade KANs performance, while marginally benefiting MLPs. Crucially, KANs suffer from prohibitive computational costs without proportional performance gains. Statistical validation confirms that MLPs with imbalance techniques achieve equivalence with KANs ( d  < 0.08 across metrics) at minimal resource costs. These findings reveal that KANs represent a specialized solution for raw imbalanced data where resources permit. But their severe performance-resource tradeoffs and incompatibility with standard resampling techniques currently limits practical deployment. We identify critical research priorities as developing KAN specific architectural modifications for imbalance learning, optimizing computational efficiency, and theoretical reconciling their conflict with data augmentation. This work establishes foundational insights for next generation KAN architectures in imbalanced classification scenarios. | [üîó Paper](http://arxiv.org/abs/2507.14121v1) |
| [NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining](http://arxiv.org/abs/2507.14119v1) | Maksim Kuprashevich, Grigorii Alekseenko, Irina Tolstykh, Georgii Fedorov, Bulat Suleimanov, Vladimir Dokholyan, Aleksandr Gordeev | 2025-07-18 | Training & Evaluation, LLM | Recent advances in generative modeling enable image editing assistants that follow natural language instructions without additional user input. Their supervised training requires millions of triplets: original image, instruction, edited image. Yet mining pixel-accurate examples is hard. Each edit must affect only prompt-specified regions, preserve stylistic coherence, respect physical plausibility, and retain visual appeal. The lack of robust automated edit-quality metrics hinders reliable automation at scale. We present an automated, modular pipeline that mines high-fidelity triplets across domains, resolutions, instruction complexities, and styles. Built on public generative models and running without human intervention, our system uses a task-tuned Gemini validator to score instruction adherence and aesthetics directly, removing any need for segmentation or grounding models. Inversion and compositional bootstrapping enlarge the mined set by approximately 2.2x, enabling large-scale high-fidelity training data. By automating the most repetitive annotation steps, the approach allows a new scale of training without human labeling effort. To democratize research in this resource-intensive area, we release NHR-Edit: an open dataset of 358k high-quality triplets. In the largest cross-dataset evaluation, it surpasses all public alternatives. We also release Bagel-NHR-Edit, an open-source fine-tuned Bagel model, which achieves state-of-the-art metrics in our experiments. | [üîó Paper](http://arxiv.org/abs/2507.14119v1) |
| [Automated Interpretation of Non-Destructive Evaluation Contour Maps
  Using Large Language Models for Bridge Condition Assessment](http://arxiv.org/abs/2507.14107v1) | Viraj Nishesh Darji, Callie C. Liao, Duoduo Liao | 2025-07-18 | Training & Evaluation | Bridge maintenance and safety are essential for transportation authorities, and Non-Destructive Evaluation (NDE) techniques are critical to assessing structural integrity. However, interpreting NDE data can be time-consuming and requires expertise, potentially delaying decision-making. Recent advancements in Large Language Models (LLMs) offer new ways to automate and improve this analysis. This pilot study introduces a holistic assessment of LLM capabilities for interpreting NDE contour maps and demonstrates the effectiveness of LLMs in providing detailed bridge condition analyses. It establishes a framework for integrating LLMs into bridge inspection workflows, indicating that LLM-assisted analysis can enhance efficiency without compromising accuracy. In this study, several LLMs are explored with prompts specifically designed to enhance the quality of image descriptions, which are applied to interpret five different NDE contour maps obtained through technologies for assessing bridge conditions. Each LLM model is evaluated based on its ability to produce detailed descriptions, identify defects, provide actionable recommendations, and demonstrate overall accuracy. The research indicates that four of the nine models provide better image descriptions, effectively covering a wide range of topics related to the bridge's condition. The outputs from these four models are summarized using five different LLMs to form a comprehensive overview of the bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more effective summaries. The findings suggest that LLMs have the potential to significantly improve efficiency and accuracy. This pilot study presents an innovative approach that leverages LLMs for image captioning in parallel and summarization, enabling faster decision-making in bridge maintenance and enhancing infrastructure management and safety assessments. | [üîó Paper](http://arxiv.org/abs/2507.14107v1) |
| [Project-connex Decompositions and Tractability of Aggregate Group-by
  Conjunctive Queries](http://arxiv.org/abs/2507.14101v1) | Diego Figueira, Cibele Freire | 2025-07-18 | Training & Evaluation | We introduce 'project-connex' tree-width as a measure of tractability for counting and aggregate conjunctive queries over semirings with 'group-by' projection (also known as 'AJAR' or 'FAQ' queries). This elementary measure allows to obtain comparable complexity bounds to the ones obtained by previous structural conditions tailored for efficient evaluation of semiring aggregate queries, enumeration algorithms of conjunctive queries, and tractability of counting answers to conjunctive queries.   Project-connex tree decompositions are defined as the natural extension of the known notion of 'free-connex' decompositions. They allow for a unified, simple and intuitive algorithmic manipulation for evaluation of aggregate queries and explain some existing tractability results on conjunctive query enumeration, counting conjunctive query evaluation, and evaluation of semiring aggregate queries. Using this measure we also recover results relating tractable classes of counting conjunctive queries and bounded free-connex tree-width, or the constant-time delay enumeration of semiring aggregate queries over bounded project-connex classes. We further show that project-connex tree decompositions can be obtained via algorithms for computing classical tree decompositions. | [üîó Paper](http://arxiv.org/abs/2507.14101v1) |
## üîπ Model Evaluation

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [The DBL Survey II: towards a mass-period distribution of double white
  dwarf binaries](http://arxiv.org/abs/2507.14123v1) | James Munday, Ingrid Pelisoli, Pier-Emmanuel Tremblay, David Jones, Gijs Nelemans, Mukremin Kilic, Tim Cunningham, Silvia Toonen, Alejandro Santos-Garc√≠a, Harry Dawson, Viktoria Pinter, Benjamin Godson, Llanos Martinez, Jaya Chand, Ross Dobson, Kiran Jhass, Shravya Shenoy | 2025-07-18 | Model Evaluation, Responsible AI | Double white dwarf binaries are an important remnant of binary evolution as they are possible type Ia supernova progenitors and strong sources of gravitational waves in the low-frequency regime. The double-lined double white dwarf (DBL) survey searches for compact double white dwarfs where both stars are spectrally disentangleable. Candidates are identified by being overluminous compared to the cooling sequence of a typical mass, single white dwarf. In this second DBL survey instalment, we present full orbital solutions of 15 double white dwarf binaries from our ongoing campaign to accurately measure a magnitude-limited mass-period distribution. 12 of these systems are fully solved for the first time. A long-standing bias in the full population has been evident, favouring systems with orbital periods up to a few hours, with little exploration of the majority of the compact double white dwarf population, whose orbital period distribution centres at approximately 20hr. The 15 systems in this study span the orbital period range 5-75hr, significantly augmenting the number of well-characterised systems over these periods, and in general have two similar mass stars combining to approximately 1.0 solar masses. We witness that the orbitally derived mass ratios generally show an excellent agreement with those deduced from atmospheric fits to double-lined spectra in previous work, emphasising the power of wide-scale spectroscopic surveys to efficiently locate the highest mass, double-lined double white dwarfs in the local Galaxy. | [üîó Paper](http://arxiv.org/abs/2507.14123v1) |
| [Anyonic analogue of optical Mach-Zehnder interferometer](http://arxiv.org/abs/2507.14115v1) | Navketan Batra, Zezhu Wei, Smitha Vishweshwara, D. E. Feldman | 2025-07-18 | Model Evaluation, Responsible AI | Anyonic interferometry is a direct probe of fractional statistics. We propose an interferometry geometry that parallels an optical Mach-Zehnder interferometer and offers several advantages over existing interferometry schemes. In contrast to the currently studied electronic Mach-Zehnder interferometer, our setup has no drain inside the device so that the trapped topological charge is time-independent. In contrast to electronic Fabry-P\'erot interferometry, anyons cannot go around the device more than once. Thus, the interference signal has a straightforward interpretation in terms of anyonic statistical phases. The proposed geometry suppresses the undesirable effects of bulk-edge coupling. Moreover, the setup allows for simple exact solutions for the electric current and noise for an arbitrary quasiparticle tunneling strength in a broad range of conditions. The structure of the solutions is similar to that for non-interacting electrons but reflects fractional charge and statistics. We present results for electric current and noise in Jain states and address thermal interferometry at zero voltage bias. | [üîó Paper](http://arxiv.org/abs/2507.14115v1) |
| [Apparent Horizons Associated with Dynamical Black Hole Entropy](http://arxiv.org/abs/2507.14105v1) | Hideo Furugori, Kanji Nishii, Daisuke Yoshida, Kaho Yoshimura | 2025-07-18 | Model Evaluation, Responsible AI | We define entropic marginally outer trapped surfaces (E-MOTSs) as a generalization of apparent horizons. We then show that, under first-order perturbations around a stationary black hole, the dynamical black hole entropy proposed by Hollands, Wald, and Zhang, defined on a background Killing horizon, can be expressed as the Wall entropy evaluated on an E-MOTS associated with it. Our result ensures that the Hollands-Wald-Zhang entropy reduces to the standard Wald entropy in each stationary regime of a dynamical black hole, thereby reinforcing the robustness of the dynamical entropy formulation. | [üîó Paper](http://arxiv.org/abs/2507.14105v1) |
| [Context-Aware Behavior Learning with Heuristic Motion Memory for
  Underwater Manipulation](http://arxiv.org/abs/2507.14099v1) | Markus Buchholz, Ignacio Carlucho, Michele Grimaldi, Maria Koskinopoulou, Yvan R. Petillot | 2025-07-18 | Model Evaluation, Responsible AI | Autonomous motion planning is critical for efficient and safe underwater manipulation in dynamic marine environments. Current motion planning methods often fail to effectively utilize prior motion experiences and adapt to real-time uncertainties inherent in underwater settings. In this paper, we introduce an Adaptive Heuristic Motion Planner framework that integrates a Heuristic Motion Space (HMS) with Bayesian Networks to enhance motion planning for autonomous underwater manipulation. Our approach employs the Probabilistic Roadmap (PRM) algorithm within HMS to optimize paths by minimizing a composite cost function that accounts for distance, uncertainty, energy consumption, and execution time. By leveraging HMS, our framework significantly reduces the search space, thereby boosting computational performance and enabling real-time planning capabilities. Bayesian Networks are utilized to dynamically update uncertainty estimates based on real-time sensor data and environmental conditions, thereby refining the joint probability of path success. Through extensive simulations and real-world test scenarios, we showcase the advantages of our method in terms of enhanced performance and robustness. This probabilistic approach significantly advances the capability of autonomous underwater robots, ensuring optimized motion planning in the face of dynamic marine challenges. | [üîó Paper](http://arxiv.org/abs/2507.14099v1) |
| [Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts
  (PLABA) track](http://arxiv.org/abs/2507.14096v1) | Brian Ondov, William Xia, Kush Attal, Ishita Unde, Jerry He, Hoa Dang, Ian Soboroff, Dina Demner-Fushman | 2025-07-18 | Model Evaluation, Training & Evaluation | Objective: Recent advances in language models have shown potential to adapt professional-facing biomedical literature to plain language, making it accessible to patients and caregivers. However, their unpredictability, combined with the high potential for harm in this domain, means rigorous evaluation is necessary. Our goals with this track were to stimulate research and to provide high-quality evaluation of the most promising systems.   Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts (PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included complete, sentence-level, rewriting of abstracts (Task 1) as well as identifying and replacing difficult terms (Task 2). For automatic evaluation of Task 1, we developed a four-fold set of professionally-written references. Submissions for both Tasks 1 and 2 were provided extensive manual evaluation from biomedical experts.   Results: Twelve teams spanning twelve countries participated in the track, with models from multilayer perceptrons to large pretrained transformers. In manual judgments of Task 1, top-performing models rivaled human levels of factual accuracy and completeness, but not simplicity or brevity. Automatic, reference-based metrics generally did not correlate well with manual judgments. In Task 2, systems struggled with identifying difficult terms and classifying how to replace them. When generating replacements, however, LLM-based systems did well in manually judged accuracy, completeness, and simplicity, though not in brevity.   Conclusion: The PLABA track showed promise for using Large Language Models to adapt biomedical literature for the general public, while also highlighting their deficiencies and the need for improved automatic benchmarking tools. | [üîó Paper](http://arxiv.org/abs/2507.14096v1) |
| [C-DOG: Training-Free Multi-View Multi-Object Association in Dense Scenes
  Without Visual Feature via Connected Œ¥-Overlap Graphs](http://arxiv.org/abs/2507.14095v1) | Yung-Hong Sun, Ting-Hung Lin, Jiangang Chen, Hongrui Jiang, Yu Hen Hu | 2025-07-18 | Model Evaluation, Responsible AI | Multi-view multi-object association is a fundamental step in 3D reconstruction pipelines, enabling consistent grouping of object instances across multiple camera views. Existing methods often rely on appearance features or geometric constraints such as epipolar consistency. However, these approaches can fail when objects are visually indistinguishable or observations are corrupted by noise. We propose C-DOG, a training-free framework that serves as an intermediate module bridging object detection (or pose estimation) and 3D reconstruction, without relying on visual features. It combines connected delta-overlap graph modeling with epipolar geometry to robustly associate detections across views. Each 2D observation is represented as a graph node, with edges weighted by epipolar consistency. A delta-neighbor-overlap clustering step identifies strongly consistent groups while tolerating noise and partial connectivity. To further improve robustness, we incorporate Interquartile Range (IQR)-based filtering and a 3D back-projection error criterion to eliminate inconsistent observations. Extensive experiments on synthetic benchmarks demonstrate that C-DOG outperforms geometry-based baselines and remains robust under challenging conditions, including high object density, without visual features, and limited camera overlap, making it well-suited for scalable 3D reconstruction in real-world scenarios. | [üîó Paper](http://arxiv.org/abs/2507.14095v1) |
## üîπ Responsible AI

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Multi-Centre Validation of a Deep Learning Model for Scoliosis
  Assessment](http://arxiv.org/abs/2507.14093v1) | ≈†imon Kubov, Simon Kl√≠ƒçn√≠k, Jakub Dand√°r, Zdenƒõk Straka, Karol√≠na Kvakov√°, Daniel Kvak | 2025-07-18 | Responsible AI, Model Evaluation, Training & Evaluation | Scoliosis affects roughly 2 to 4 percent of adolescents, and treatment decisions depend on precise Cobb angle measurement. Manual assessment is time consuming and subject to inter observer variation. We conducted a retrospective, multi centre evaluation of a fully automated deep learning software (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o.) on 103 standing anteroposterior whole spine radiographs collected from ten hospitals. Two musculoskeletal radiologists independently measured each study and served as reference readers. Agreement between the AI and each radiologist was assessed with Bland Altman analysis, mean absolute error (MAE), root mean squared error (RMSE), Pearson correlation coefficient, and Cohen kappa for four grade severity classification. Against Radiologist 1 the AI achieved an MAE of 3.89 degrees (RMSE 4.77 degrees) with a bias of 0.70 degrees and limits of agreement from minus 8.59 to plus 9.99 degrees. Against Radiologist 2 the AI achieved an MAE of 3.90 degrees (RMSE 5.68 degrees) with a bias of 2.14 degrees and limits from minus 8.23 to plus 12.50 degrees. Pearson correlations were r equals 0.906 and r equals 0.880 (inter reader r equals 0.928), while Cohen kappa for severity grading reached 0.51 and 0.64 (inter reader kappa 0.59). These results demonstrate that the proposed software reproduces expert level Cobb angle measurements and categorical grading across multiple centres, suggesting its utility for streamlining scoliosis reporting and triage in clinical workflows. | [üîó Paper](http://arxiv.org/abs/2507.14093v1) |
## üîπ Autonomous Agents

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [DPMT: Dual Process Multi-scale Theory of Mind Framework for Real-time
  Human-AI Collaboration](http://arxiv.org/abs/2507.14088v1) | Xiyun Li, Yining Ding, Yuhua Jiang, Yunlong Zhao, Runpeng Xie, Shuang Xu, Yuanhua Ni, Yiqin Yang, Bo Xu | 2025-07-18 | Autonomous Agents, LLM | Real-time human-artificial intelligence (AI) collaboration is crucial yet challenging, especially when AI agents must adapt to diverse and unseen human behaviors in dynamic scenarios. Existing large language model (LLM) agents often fail to accurately model the complex human mental characteristics such as domain intentions, especially in the absence of direct communication. To address this limitation, we propose a novel dual process multi-scale theory of mind (DPMT) framework, drawing inspiration from cognitive science dual process theory. Our DPMT framework incorporates a multi-scale theory of mind (ToM) module to facilitate robust human partner modeling through mental characteristic reasoning. Experimental results demonstrate that DPMT significantly enhances human-AI collaboration, and ablation studies further validate the contributions of our multi-scale ToM in the slow system. | [üîó Paper](http://arxiv.org/abs/2507.14088v1) |
## üîπ Security & Adversarial ML

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [An Adversarial-Driven Experimental Study on Deep Learning for RF
  Fingerprinting](http://arxiv.org/abs/2507.14109v1) | Xinyu Cao, Bimal Adhikari, Shangqing Zhao, Jingxian Wu, Yanjun Pan | 2025-07-18 | Security & Adversarial ML, Model Evaluation, Responsible AI | Radio frequency (RF) fingerprinting, which extracts unique hardware imperfections of radio devices, has emerged as a promising physical-layer device identification mechanism in zero trust architectures and beyond 5G networks. In particular, deep learning (DL) methods have demonstrated state-of-the-art performance in this domain. However, existing approaches have primarily focused on enhancing system robustness against temporal and spatial variations in wireless environments, while the security vulnerabilities of these DL-based approaches have often been overlooked. In this work, we systematically investigate the security risks of DL-based RF fingerprinting systems through an adversarial-driven experimental analysis. We observe a consistent misclassification behavior for DL models under domain shifts, where a device is frequently misclassified as another specific one. Our analysis based on extensive real-world experiments demonstrates that this behavior can be exploited as an effective backdoor to enable external attackers to intrude into the system. Furthermore, we show that training DL models on raw received signals causes the models to entangle RF fingerprints with environmental and signal-pattern features, creating additional attack vectors that cannot be mitigated solely through post-processing security methods such as confidence thresholds. | [üîó Paper](http://arxiv.org/abs/2507.14109v1) |
## üîπ General AI

| üìÑ Title | üñä Authors | üìÖ Date | üè∑ Tags | üìú Summary | üîó Link |
|---------|---------|---------|---------|---------|---------|
| [Missing baryons recovered: a measurement of the gas fraction in galaxies
  and groups with the kinematic Sunyaev-Zel'dovich effect and CMB lensing](http://arxiv.org/abs/2507.14136v1) | Boryana Hadzhiyska, Simone Ferraro, Gerrit S. Farren, Noah Sailer, Rongpu Zhou | 2025-07-18 | General AI | We present new constraints on the halo masses and matter density profiles of DESI galaxy groups by cross-correlating samples of Luminous Red Galaxies (LRGs) and Bright Galaxy Survey (BGS) galaxies with the publicly available CMB lensing convergence map from ACT DR6. This provides an independent, lensing-based calibration of halo masses, complementary to methods relying on clustering or dynamics. We derive constraints on the mean halo mass for three DESI-selected samples, finding $\log(M_{\rm halo}/(M_\odot/h)) \approx 13.18$, 13.03 and 13.02 for the Main LRG, Extended LRG, and BGS samples, respectively. Using a halo model approach, we also compare the projected galaxy-matter density profiles with previously reported gas profiles inferred from measurements of the kinematic Sunyaev-Zel'dovich (kSZ) effect. This work addresses one of the key uncertainties in interpreting kSZ signals -- the unknown host halo mass distribution -- by providing an independent and consistent mass calibration. The agreement between the gas and total mass profiles at large aperture suggests that sufficiently far from the group center (2--3 virial radii), we recover all the baryons, offering a resolution to the 'missing baryon' problem. We further study the cumulative gas fractions for all galaxies as well as for the most massive galaxy groups in the sample ($\log(M_{\rm halo}/(M_\odot/h)) \approx 13.5$), finding values that are physically sensible and in agreement with previous findings using kSZ and X-ray data: compared to the TNG300 simulation, the observed gas fractions are systematically lower at fixed radius by $\gtrsim$4$\sigma$, providing compelling, independent evidence for stronger baryonic feedback in the real Universe. These findings highlight the power of combining CMB lensing with galaxy surveys to probe the interplay between baryons and dark matter in group-sized halos. | [üîó Paper](http://arxiv.org/abs/2507.14136v1) |
| [Do mixed states exhibit deep thermalisation?](http://arxiv.org/abs/2507.14135v1) | Alan Sherry, Sthitadhi Roy | 2025-07-18 | General AI | The notion of $deep$ $thermalisation$, where ensembles of pure states on a local subsystem, conditioned on measurement outcomes on its complement, approach universal maximum-entropy ensembles constrained only by conservation laws, represents a stronger form of ergodicity than conventional thermalisation. We show that this framework fails dramatically for mixed initial states, evolved unitarily, even with infinitesimal initial mixedness. To address this, we introduce a new paradigm of deep thermalisation for mixed states, fundamentally distinct from that for pure-state ensembles. In our formulation, the deep thermal ensemble arises by tracing out auxiliary degrees of freedom from a maximum-entropy ensemble defined on an augmented system, with the ensemble structure depending explicitly on the entropy of the initial state. We demonstrate that such ensembles emerge dynamically in generic, locally interacting chaotic systems. For the self-dual kicked Ising chain, which we show to be exactly solvable for a class of mixed initial states, we find exact emergence of the so-defined mixed-state deep thermal ensemble at finite times. Our results therefore lead to fundamental insights into how maximum entropy principles and deep thermalisation manifest themselves in unitary dynamics of states with finite entropy. | [üîó Paper](http://arxiv.org/abs/2507.14135v1) |
| [The Extended Atlas of Low-resolution Spectra from the Infrared
  Astronomical Satellite](http://arxiv.org/abs/2507.14134v1) | G. C. Sloan, Kathleen E. Kraemer, K. Volk | 2025-07-18 | General AI | We present an updated atlas of spectra from the Low-Resolution Spectrometer (LRS) on the Infrared Astronomical Satellite (IRAS), which took spectra from 7.67 to 22.73 um with a spectral resolving power (lambda / Delta lambda) of 20-60. The updated atlas includes 11,238 spectra, including 5425 spectra published in the original LRS Atlas, 5796 spectra published in three later papers, and 17 spectra previously available online but not published. The updated atlas has significantly more sources close to the Galactic plane than the original atlas. We have applied an improved spectral correction to remove an artifact at 8 um in the original database. While the IRAS mission flew over 40 yr ago, the extended LRS atlas remains the single most complete database of mid-infrared spectra of nearby and bright objects in the Galaxy. | [üîó Paper](http://arxiv.org/abs/2507.14134v1) |
| [A Bayesian Dirichlet Auto-Regressive Conditional Heteroskedasticity
  Model for Compositional Time Series](http://arxiv.org/abs/2507.14132v1) | Harrison Katz, Robert E. Weiss | 2025-07-18 | General AI | We analyze daily Airbnb service-fee shares across eleven settlement currencies, a compositional series that shows bursts of volatility after shocks such as the COVID-19 pandemic. Standard Dirichlet time series models assume constant precision and therefore miss these episodes. We introduce B-DARMA-DARCH, a Bayesian Dirichlet autoregressive moving average model with a Dirichlet ARCH component, which lets the precision parameter follow an ARMA recursion. The specification preserves the Dirichlet likelihood so forecasts remain valid compositions while capturing clustered volatility. Simulations and out-of-sample tests show that B-DARMA-DARCH lowers forecast error and improves interval calibration relative to Dirichlet ARMA and log-ratio VARMA benchmarks, providing a concise framework for settings where both the level and the volatility of proportions matter. | [üîó Paper](http://arxiv.org/abs/2507.14132v1) |
| [On the relation between perspective-neutral, algebraic, and effective
  quantum reference frames](http://arxiv.org/abs/2507.14131v1) | Philipp A. Hoehn, Julian De Vuyst, Artur Tsobanjan | 2025-07-18 | General AI | The framework of internal quantum reference frames (QRFs) constitutes a universal toolset for dealing with symmetries in quantum theory and has led to new revelations in quantum gravity, gauge theories and foundational physics. Multiple approaches have emerged, sometimes differing in scope and the way symmetries are implemented, raising the question as to their relation. Here, we investigate the relation between three approaches to QRFs for gauge symmetries, namely the effective semiclassical, algebraic, and perspective-neutral (PN) approaches. Rather than constructing Hilbert spaces, as the PN approach, the effective approach is based on a quantum phase space parametrized by expectation values and fluctuations, while the emphasis of the algebraic approach is on the state space of complex linear functionals on a kinematical algebra. Nevertheless, external frame information is treated as gauge in all three formalisms, manifested in constraints on states and algebra. We show that these three approaches are, in fact, equivalent for ideal QRFs, distinguished by sharp orientations, which is the previous setting of the first two approaches. Our demonstration pertains to single constraints, including relativistic ones, and encompasses QRF changes. In particular, the QRF transformations of the PN framework agree semiclassically with those of the older effective approach, by which it was inspired. As a physical application, we explore the QRF covariance of uncertainties and fluctuations, which turn out to be frame-dependent. This is particularly well-suited for the effective and algebraic approaches, for which these quantities form a natural basis. Finally, we pave the way towards extending these two approaches to non-ideal QRFs by studying the projection and gauge-fixing operations of the Page-Wootters formalism, built into the PN framework, on algebraic states. | [üîó Paper](http://arxiv.org/abs/2507.14131v1) |
| [Design framework for programmable three-dimensional woven metamaterials](http://arxiv.org/abs/2507.14130v1) | Molly Carton, James Utama Surjadi, Bastien F. G. Aymon, Carlos M. Portela | 2025-07-18 | General AI | Mechanical metamaterials have continued to offer unprecedented tunability in mechanical properties, but most designs to date have prioritized attaining high stiffness and strength while sacrificing deformability. The emergence of woven lattices-three-dimensional networks of entangled fibers-has introduced a pathway to the largely overlooked compliant and stretchable regime of metamaterials. However, the design and implementation of these complex architectures has remained a primarily manual process, restricting identification and validation of their full achievable design and property space. Here, we present a geometric design framework that encodes woven topology using a graph structure, enabling the creation of woven lattices with tunable architectures, functional gradients, and arbitrary heterogeneity. Through use of microscale in situ tension experiments and computational mechanics models, we reveal highly tunable anisotropic stiffness (varying by over an order of magnitude) and extreme stretchability (up to a stretch of four) within the design space produced by the framework. Moreover, we demonstrate the ability of woven metamaterials to exhibit programmable failure patterns by leveraging tunability in the design process. This framework provides a design and modeling toolbox to access this previously unattainable high-compliance regime of mechanical metamaterials, enabling programmable large-deformation, nonlinear responses. | [üîó Paper](http://arxiv.org/abs/2507.14130v1) |
| [Exploring near critical lattice gauge simulators with Rydberg atoms
  facilities](http://arxiv.org/abs/2507.14128v1) | Avi Kaufman, James Corona, Zane Ozzello, Blake Senseman, Muhammad Asaduzzaman, Yannick Meurice | 2025-07-18 | General AI | We motivate the use of a ladder of Rydberg atoms as an analog simulator for a lattice gauge theory version of scalar electrodynamics also called the compact Abelian Higgs model. We demonstrate that by using a few thousand shots from a single copy of the ladder simulator it is possible to estimate the bipartite quantum von Neumann entanglement entropy $S^{vN}_A$. The estimation relies on an optimized filtration of the mutual information associated with the bitstrings obtained from public facilities of configurable Rydberg arrays named Aquila. We discuss the limitations associated with finite sampling, sorting fidelity, adiabatic preparation, ramp-down of the Rabi frequency before measurement, and readout errors. We use cumulative probability distribution to compare Aquila results with high accuracy density matrix renormalization group (DMRG) or exact results. The state preparation appears to be the main source of error. We discuss the large volume behavior of the cumulative probability distribution and show examples where for a finite number of shots, there appears to be some large enough size for which any given state is seen at most once with high probability. We show that the results presented can be extended to multipartite entanglement. We briefly discuss the cost of the calculations for large square arrays in the context of obtaining quantum advantage in the near future. | [üîó Paper](http://arxiv.org/abs/2507.14128v1) |
| [Quantum and classical algorithms for SOCP based on the multiplicative
  weights update method](http://arxiv.org/abs/2507.14127v1) | M. Isabel Franco Garrido, Alexander M. Dalzell, Sam McArdle | 2025-07-18 | General AI | We give classical and quantum algorithms for approximately solving second-order cone programs (SOCPs) based on the multiplicative weights (MW) update method. Our approach follows the MW framework previously applied to semidefinite programs (SDPs), of which SOCP is a special case. We show that the additional structure of SOCPs can be exploited to give better runtime with SOCP-specific algorithms. For an SOCP with $m$ linear constraints over $n$ variables partitioned into $r \leq n$ second-order cones, our quantum algorithm requires $\widetilde{O}(\sqrt{r}\gamma^5 + \sqrt{m}\gamma^4)$ (coherent) queries to the underlying data defining the instance, where $\gamma$ is a scale-invariant parameter proportional to the inverse precision. This nearly matches the complexity of solving linear programs (LPs), which are a less expressive subset of SOCP. It also outperforms (especially if $n \gg r$) the naive approach that applies existing SDP algorithms onto SOCPs, which has complexity $\widetilde{O}(\gamma^{4}(n + \gamma \sqrt{n} + \sqrt{m}))$. Our classical algorithm for SOCP has complexity $\widetilde{O}(n\gamma^4 + m \gamma^6)$ in the sample-and-query model. | [üîó Paper](http://arxiv.org/abs/2507.14127v1) |
| [Toward Temporal Causal Representation Learning with Tensor Decomposition](http://arxiv.org/abs/2507.14126v1) | Jianhong Chen, Meng Zhao, Mostafa Reisi Gahrooei, Xubo Yue | 2025-07-18 | General AI | Temporal causal representation learning is a powerful tool for uncovering complex patterns in observational studies, which are often represented as low-dimensional time series. However, in many real-world applications, data are high-dimensional with varying input lengths and naturally take the form of irregular tensors. To analyze such data, irregular tensor decomposition is critical for extracting meaningful clusters that capture essential information. In this paper, we focus on modeling causal representation learning based on the transformed information. First, we present a novel causal formulation for a set of latent clusters. We then propose CaRTeD, a joint learning framework that integrates temporal causal representation learning with irregular tensor decomposition. Notably, our framework provides a blueprint for downstream tasks using the learned tensor factors, such as modeling latent structures and extracting causal information, and offers a more flexible regularization design to enhance tensor decomposition. Theoretically, we show that our algorithm converges to a stationary point. More importantly, our results fill the gap in theoretical guarantees for the convergence of state-of-the-art irregular tensor decomposition. Experimental results on synthetic and real-world electronic health record (EHR) datasets (MIMIC-III), with extensive benchmarks from both phenotyping and network recovery perspectives, demonstrate that our proposed method outperforms state-of-the-art techniques and enhances the explainability of causal representations. | [üîó Paper](http://arxiv.org/abs/2507.14126v1) |
| [NGC 663 as a laboratory for massive star evolution](http://arxiv.org/abs/2507.14125v1) | Amparo Marco, Ignacio Negueruela, Norberto Castro, Sergio Sim√≥n-D√≠az | 2025-07-18 | General AI | Massive young clusters with rich populations of high-mass stars are ideal laboratories to explore their evolutionary paths. Despite being the most prominent cluster in the Perseus-arm Cas OB8 association, NGC 663 remains comparatively little studied. We present a comprehensive investigation of its properties, integrating astrometric, photometric and spectroscopic data for the cluster and its surroundings, including accurate spectral classification for over 150 members. Gaia astrometry indicates over 300 B-type members, possibly rendering NGC 663 the most massive cluster in the Perseus arm, with initial mass likely exceeding 10000 M_\sun . This large population makes NGC 663 an excellent laboratory for studying massive star evolution. Spectral analysis of the earliest members reveals approximately solar metallicity and a turn-off mass of approximate 8.5 M_\sun, consistent with the photometric age of 23 Ma. We identify five spectroscopic blue stragglers, including the Be/X-ray binary RX J0146.9$+$6121. We outline its evolutionary history and compare its properties with other Be stars. Although the cluster contains many Be stars, their relative fraction is not particularly high. Intriguingly, four of the six blue supergiant members appear to have significantly higher masses than the brightest giants near the Hertzsprung gap. These observations suggest that most mid-B supergiants may form via mergers, unless stars of 10-12 M_\sun born as primaries in binaries rarely undergo supernova explosions. Similarly, if Be stars form through the binary channel, then either most are produced through case A evolution or supernovae are uncommon among primaries in this mass range. | [üîó Paper](http://arxiv.org/abs/2507.14125v1) |
| [AdS$\times$S Mellin Bootstrap, Hidden 10d Symmetry and Five-point
  Kaluza-Klein Functions in $\mathcal{N}=4$ SYM](http://arxiv.org/abs/2507.14124v1) | Bruno Fernandes, Vasco Goncalves, Zhongjie Huang, Yichao Tang, Joao Vilas Boas, Ellis Ye Yuan | 2025-07-18 | General AI | We propose an AdS$\times$S factorization formula at the level of the generating function for correlators with arbitrary Kaluza-Klein configurations, and implement it in the supergravity limit of $\mathcal{N}=4$ super Yang-Mills. By incorporating this mechanism into Mellin space bootstrap, together with an observed $Z_2$ symmetry under AdS$\ \leftrightarrow\ $S, we manage to simultaneously work out unified formulas both for all five-point half-BPS correlators and for all four-point correlators with one superdescendant. This AdS\times$S bootstrap method is directly applicable to generic multi-point computation at tree level. | [üîó Paper](http://arxiv.org/abs/2507.14124v1) |
| [Last-Iterate Complexity of SGD for Convex and Smooth Stochastic Problems](http://arxiv.org/abs/2507.14122v1) | Guillaume Garrigos, Daniel Cortild, Lucas Ketels, Juan Peypouquet | 2025-07-18 | General AI | Most results on Stochastic Gradient Descent (SGD) in the convex and smooth setting are presented under the form of bounds on the ergodic function value gap. It is an open question whether bounds can be derived directly on the last iterate of SGD in this context. Recent advances suggest that it should be possible. For instance, it can be achieved by making the additional, yet unverifiable, assumption that the variance of the stochastic gradients is uniformly bounded. In this paper, we show that there is no need of such an assumption, and that SGD enjoys a $\tilde O \left( T^{-1/2} \right)$ last-iterate complexity rate for convex smooth stochastic problems. | [üîó Paper](http://arxiv.org/abs/2507.14122v1) |
| [A spherical hydrodynamical model of cosmic voids in ŒõCDM and
  beyond](http://arxiv.org/abs/2507.14120v1) | Tommaso Moretti, Giovanni Verza, Noemi Frusciante, Franceso Pace | 2025-07-18 | General AI | Cosmic voids have emerged as powerful probes for cosmology, providing complementary information on the large-scale structure of the universe. We present the first application of a hydrodynamical framework to model the evolution of cosmic voids. This approach offers a physically intuitive characterization of void dynamics and can naturally be applied to non-standard cosmologies. We derive the cosmology-dependent mapping that relates the linear (Lagrangian) and fully non-linear (Eulerian) evolution of the matter density contrast, a central component for accurate theoretical modeling of void statistics. Furthermore, we present a new method for determining the shell-crossing epoch across arbitrary cosmological backgrounds, thereby extending previous treatments restricted to the Einstein-de Sitter universe.   Motivated by recent DESI results hinting at dynamical dark energy, we investigate void evolution in $ w_0w_a$CDM cosmologies by varying $ w_0$ and $w_a$. We also consider the impact of varying the matter density parameter, $ \Omega_{\mathrm{m},0}$. We find that the evolution of isolated, spherically symmetric cosmic voids is most sensitive to $ \Omega_{\mathrm{m},0} $ and $ w_0 $, which can alter the non-linear density contrast by up to 20-30%. Variations in $w_a$ have a smaller impact, but may still lead to measurable effects. We also show that the cosmology-dependent mapping between linear and non-linear density contrasts may provide a sensitive probe of dynamical dark energy in precision void analyses. | [üîó Paper](http://arxiv.org/abs/2507.14120v1) |
| [Multiple $\wp$-Functions and Their Applications](http://arxiv.org/abs/2507.14118v1) | Hayato Kanno, Katsumi Kina | 2025-07-18 | General AI | In this paper, we introduce and study multiple $\wp$-functions, which generalize the classical Weierstrass $\wp$-function to iterated sums over lattice points, and we establish explicit formulas expressing them in terms of single $\wp$-functions with coefficients given by multiple Eisenstein series. As an application, we derive some relations among multiple Eisenstein series and multiple zeta values by exploiting the double periodicity of the multiple $\wp$-functions. | [üîó Paper](http://arxiv.org/abs/2507.14118v1) |
| [Quantum Boltzmann Machines using Parallel Annealing for Medical Image
  Classification](http://arxiv.org/abs/2507.14116v1) | Dani√´lle Schuman, Mark V. Seebode, Tobias Rohe, Maximilian Balthasar Mansky, Michael Schroedl-Baumann, Jonas Stein, Claudia Linnhoff-Popien, Florian Krellner | 2025-07-18 | General AI | Exploiting the fact that samples drawn from a quantum annealer inherently follow a Boltzmann-like distribution, annealing-based Quantum Boltzmann Machines (QBMs) have gained increasing popularity in the quantum research community. While they harbor great promises for quantum speed-up, their usage currently stays a costly endeavor, as large amounts of QPU time are required to train them. This limits their applicability in the NISQ era. Following the idea of No\`e et al. (2024), who tried to alleviate this cost by incorporating parallel quantum annealing into their unsupervised training of QBMs, this paper presents an improved version of parallel quantum annealing that we employ to train QBMs in a supervised setting. Saving qubits to encode the inputs, the latter setting allows us to test our approach on medical images from the MedMNIST data set (Yang et al., 2023), thereby moving closer to real-world applicability of the technology. Our experiments show that QBMs using our approach already achieve reasonable results, comparable to those of similarly-sized Convolutional Neural Networks (CNNs), with markedly smaller numbers of epochs than these classical models. Our parallel annealing technique leads to a speed-up of almost 70 % compared to regular annealing-based BM executions. | [üîó Paper](http://arxiv.org/abs/2507.14116v1) |
| [Weighted Matching in a Poly-Streaming Model](http://arxiv.org/abs/2507.14114v1) | Ahammed Ullah, S. M. Ferdous, Alex Pothen | 2025-07-18 | General AI | We introduce the poly-streaming model, a generalization of streaming models of computation in which $k$ processors process $k$ data streams containing a total of $N$ items. The algorithm is allowed $O\left(f(k)\cdot M_1\right)$ space, where $M_1$ is either $o\left(N\right)$ or the space bound for a sequential streaming algorithm. Processors may communicate as needed. Algorithms are assessed by the number of passes, per-item processing time, total runtime, space usage, communication cost, and solution quality.   We design a single-pass algorithm in this model for approximating the maximum weight matching (MWM) problem. Given $k$ edge streams and a parameter $\varepsilon > 0$, the algorithm computes a $\left(2+\epsilon\right)$-approximate MWM. We analyze its performance in a shared-memory parallel setting: for any constant $\varepsilon > 0$, it runs in time $\widetilde{O}\left(L_{\max}+n\right)$, where $n$ is the number of vertices and $L_{\max}$ is the maximum stream length. It supports $O\left(1\right)$ per-edge processing time using $\widetilde{O}\left(k\cdot n\right)$ space. We further generalize the design to hierarchical architectures, in which $k$ processors are partitioned into $r$ groups, each with its own shared local memory. The total intergroup communication is $\widetilde{O}\left(r \cdot n\right)$ bits, while all other performance guarantees are preserved.   We evaluate the algorithm on a shared-memory system using graphs with trillions of edges. It achieves substantial speedups as $k$ increases and produces matchings with weights significantly exceeding the theoretical guarantee. On our largest test graph, it reduces runtime by nearly two orders of magnitude and memory usage by five orders of magnitude compared to an offline algorithm. | [üîó Paper](http://arxiv.org/abs/2507.14114v1) |
| [Density of finitely supported invariant measures for automorphisms of
  compact abelian groups](http://arxiv.org/abs/2507.14113v1) | Rotem Yaari | 2025-07-18 | General AI | We study the structure of invariant measures for continuous automorphisms of compact metrizable abelian groups satisfying the descending chain condition. We show that the finitely supported invariant measures are weak-* dense in the space of all invariant probability measures, and that if the system is ergodic with respect to Haar measure, then the finitely supported ergodic invariant measures are also dense. A key ingredient in the proof is a variant of the specification property, which we establish for the ergodic systems in this class. Our results also yield the following two consequences: first, that every finitely generated group that is a semidirect product of $\mathbb{Z}$ with a countable abelian group $G$ is Hilbert-Schmidt stable; and second, a Livshitz-type theorem characterizing the uniform closure of coboundaries arising from continuous functions in terms of vanishing on periodic orbits. We also construct an example showing that, in general dynamical systems, the property of having dense finitely supported invariant measures does not pass to product systems. | [üîó Paper](http://arxiv.org/abs/2507.14113v1) |
| [Existence of a non-standard isoperimetric triple partition](http://arxiv.org/abs/2507.14112v1) | Matteo Novaga, Emanuele Paolini, Vincenzo Maria Tortorelli | 2025-07-18 | General AI | We show existence of a isoperimetric $3$-partition of $\mathbb R^8$, with one set of finite volume and two of infinite volume, which is asymptotic to a singular minimal cone. | [üîó Paper](http://arxiv.org/abs/2507.14112v1) |
| [Spatiotemporal Order and Parametric Instabilities from First-Principles](http://arxiv.org/abs/2507.14110v1) | Daniel Kaplan, Pavel A. Volkov, Jennifer Coulter, Shiwei Zhang, Premala Chandra | 2025-07-18 | General AI | Shaping crystal structure with light is an enduring goal of physics and materials engineering. Here we present calculations in candidate materials selected by symmetry that allow light-induced spatiotemporal parametric instabilities. We demonstrate a theoretical framework that includes a complete symmetry analysis of phonon modes that contribute to parametric instabilities across all non-centrosymmetric point groups, a detailed survey of the materials landscape and finally the computation of nonlinear couplings from first principles. We then showcase detailed results for chiral crystals, ferroelectrics, and layered van der Waals materials. Our results pave the way towards realizing designer time-crystalline order in quantum materials, detectable with time-resolved diffractive probes. | [üîó Paper](http://arxiv.org/abs/2507.14110v1) |
| [Fast charge noise sensing using a spectator valley state in a
  singlet-triplet qubit](http://arxiv.org/abs/2507.14108v1) | David W. Kanaar, Yasuo Oda, Mark F. Gyure, J. P. Kestner | 2025-07-18 | General AI | Semiconductor spin qubits are a promising platform for quantum computing but remain vulnerable to charge noise. Accurate, in situ measurement of charge noise could enable closed-loop control and improve qubit performance. Here, we propose a method for real-time detection of charge noise using a silicon singlet-triplet qubit with one electron initialized in an excited valley state. This valley excitation acts as a spectator degree of freedom, coupled to a high-quality resonator via the exchange interaction, which is sensitive to charge-noise-induced voltage fluctuations. Dispersive readout of the resonator enables a continuous, classical measurement of exchange fluctuations during qubit operation. Signal-to-noise analysis shows that, under realistic device parameters, sub-microsecond measurement times are possible using a quantum-limited amplifier. Even without such an amplifier, sub-millisecond performance is achievable with appropriately engineered resonator parameters. This approach allows the probe to monitor slow drift in exchange in real time, opening the door to feedback and feedforward strategies for maintaining high-fidelity quantum operations. Importantly, the protocol preserves spin coherence and can be run concurrently with qubit logic gates. | [üîó Paper](http://arxiv.org/abs/2507.14108v1) |
| [Quantitative Wavefront sensing with static Foucault and pyramid tests](http://arxiv.org/abs/2507.14104v1) | Francois H√©nault, Yan Feng, Alain Spang, Laura Schreiber | 2025-07-18 | General AI | Wavefront sensors (WFS) are now core components in the fields of metrology of optical systems, biomedical optics and adaptive optics systems for astronomy. Nowadays, the most popular WFS is the Shack-Hartmann, which is fully static but suffers from a limited spatial resolution in the pupil plane of the tested optical system. Higher spatial resolutions are achievable with other types of sensors, e.g. the pyramid WFS that requires temporal modulation of the recorded signals and implies high mechanical and electronic complexity. This paper examines the possibility of performing quantitative wavefront sensing inspired from the well-known Foucault test and only comprising static, non-modulated optical components. Here, two candidate designs of static WFS are proposed, based on a set of reflective prisms. Those prisms may be coated with gradient density filters. A simplified mathematical model allows for the definition of the wavefront slopes reconstruction formula and for the calculation of the wavefront itself. Numerical simulations demonstrate that the wavefront measurement accuracy is compliant with classical diffraction limit criteria when using coated prisms. Thus accurate WFE measurements are feasible in that case. | [üîó Paper](http://arxiv.org/abs/2507.14104v1) |
| [Maximal translation surfaces in Lorentz-Minkowski space](http://arxiv.org/abs/2507.14103v1) | Rafael L√≥pez | 2025-07-18 | General AI | A translation surface in Lorentz-Minkowski space $\rr^3$ is a surface defined as the sum of two spatial curves. In this paper we present a classification of maximal surfaces of translation type. We prove that if a generating curve is planar, then the other generating curve is also planar. We give a full description of these surfaces. In case that both curves are of Frenet type, we generalize the Scherk surfaces. In case that a curve is a pseudo-null curve, we obtain new examples of maximal surfaces which have not counterparts in Euclidean space. | [üîó Paper](http://arxiv.org/abs/2507.14103v1) |
| [UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based
  Classification in Computed Tomography](http://arxiv.org/abs/2507.14102v1) | Shravan Venkatraman, Pavan Kumar S, Rakesh Raj Madavan, Chandrakala S | 2025-07-18 | General AI | Accurate classification of computed tomography (CT) images is essential for diagnosis and treatment planning, but existing methods often struggle with the subtle and spatially diverse nature of pathological features. Current approaches typically process images uniformly, limiting their ability to detect localized abnormalities that require focused analysis. We introduce UGPL, an uncertainty-guided progressive learning framework that performs a global-to-local analysis by first identifying regions of diagnostic ambiguity and then conducting detailed examination of these critical areas. Our approach employs evidential deep learning to quantify predictive uncertainty, guiding the extraction of informative patches through a non-maximum suppression mechanism that maintains spatial diversity. This progressive refinement strategy, combined with an adaptive fusion mechanism, enables UGPL to integrate both contextual information and fine-grained details. Experiments across three CT datasets demonstrate that UGPL consistently outperforms state-of-the-art methods, achieving improvements of 3.29%, 2.46%, and 8.08% in accuracy for kidney abnormality, lung cancer, and COVID-19 detection, respectively. Our analysis shows that the uncertainty-guided component provides substantial benefits, with performance dramatically increasing when the full progressive learning pipeline is implemented. Our code is available at: https://github.com/shravan-18/UGPL | [üîó Paper](http://arxiv.org/abs/2507.14102v1) |
| [On the quantum algebra $su_q(1,1)$ from a Special Function standpoint](http://arxiv.org/abs/2507.14100v1) | R. Alvarez-Nodarse, A. Arenas-Gomez | 2025-07-18 | General AI | In this paper, we study the tensor product of two unitary irreducible representations, as well as the tensor product of a unitary irreducible representation with a finite-dimensional one, and determine the corresponding Clebsch-Gordan coefficients. Using von Neumann's projection operator method, we obtain an explicit representation of these coefficients, which allows us to express them as symmetric q-hypergeometric series. Finally, by leveraging the properties of the q-hypergeometric function, we derive several properties of the Clebsch-Gordan coefficients, including a number of new results, in a unified and straightforward manner. | [üîó Paper](http://arxiv.org/abs/2507.14100v1) |
| [Symmetrizing relativistic three-body partial wave amplitudes](http://arxiv.org/abs/2507.14098v1) | Andrew W. Jackura, Nicholas C. Chambers, Ra√∫l A. Brice√±o | 2025-07-18 | General AI | S matrix principles and symmetries impose constraints on three-particle scattering amplitudes, which can be formulated as a class of integral equations for their partial wave projections. However, these amplitudes are typically expressed in an asymmetric basis, where one of the initial and final state particles is singled out, and all quantum numbers are defined relative to this spectator. In this work, we show how to construct symmetric partial wave amplitudes, which have been symmetrized over all possible spectator combinations, using their asymmetric counterparts and sets of recoupling coefficients. We derive these recoupling coefficients for arbitrary angular momentum and isospin for arbitrary systems of spinless particles with SU(2) flavor symmetry. We propose a simple intensity observable suitable for visualizing the structure of three-body dynamics in Dalitz distributions. Finally, we provide some numerical examples of Dalitz distributions relevant for future lattice QCD calculations of $3\pi$ systems and provide numerical evidence that the symmetrization procedure is consistent with expected symmetries of Dalitz plots. | [üîó Paper](http://arxiv.org/abs/2507.14098v1) |
| [ChemZz I: Comparing Oxygen and Iron Abundance Patterns in the Milky Way,
  the Local Group and Cosmic Noon](http://arxiv.org/abs/2507.14094v1) | Stephanie Monty, Allison L. Strom, Thomas M. Stanton, Martyna Chru≈õli≈Ñska, Fergus Cullen, Chiaki Kobayashi, Tjitske Starkenburg, Souradeep Bhattacharya, Jason L. Sanders, Mark Gieles | 2025-07-18 | General AI | Our understanding of the chemical evolution of galaxies has advanced through measurements from both distant galaxies across redshift, and our own Milky Way (MW). To form a comprehensive picture, it is essential to unify these constraints, placing them on a common scale and parlance and to understand their systematic differences. In this study, we homogenize oxygen and iron measurements from star-forming galaxies at Cosmic Noon ($z{\sim}2-3$) with resolved stellar abundances from the Local Group. The MW is divided into four components, assuming the outer halo is dominated by debris from the Gaia-Sausage-Enceladus (GSE) progenitor. After converting all abundances to a common Solar scale, we identify clear $\alpha$- and iron-enhancement trends with mass in the $z{\sim}2-3$ galaxies and find good agreement between these galaxies and the MW high-$\alpha$ disc in [O/Fe] vs. [Fe/H]. We also find excellent agreement between the [O/Fe] trends seen in the MW high- and low-$\alpha$ discs with O-abundances seen in old and young planetary nebulae in M~31 respectively, supporting the existence of $\alpha$-bimodality in the inner regions of M~31. Finally, we use globular cluster ages to project the MW and GSE back in time to $z{\sim}3$ and find that their estimated mass, oxygen and iron abundances are strikingly consistent with the mass-metallicity relation of star-forming galaxies at $z{\sim}3$. In the future, increased transparency around the choice of Solar scale and abundance methodology will make combining chemical abundances easier -- contributing to a complete picture of the chemical evolution of all galaxies. | [üîó Paper](http://arxiv.org/abs/2507.14094v1) |
| [Ex Situ Fabrication of Superconducting Nanostructures for
  Low-Temperature STM](http://arxiv.org/abs/2507.14092v1) | Adrian Greichgauer, Roozbeh Yazdanpanah, Alexey Taskin, Oliver Breunig, Yoichi Ando, Jens Brede | 2025-07-18 | General AI | Nanofabrication enables flexible experimental design but is often incompatible with scanning tunneling microscopy and spectroscopy (STM/STS) due to the latter's stringent surface quality requirements. Here, we present a fabrication strategy that combines ex situ nanolithography with in situ ultrahigh-vacuum (UHV) cleaving to produce atomically clean, nanopatterned superconductor/topological insulator (TI) heterostructures suitable for high-resolution STM/STS. In our initial Design I, nanoribbons were defined by etching trenches into a TI film, followed by niobium capping and sample flipping before cleaving. This enabled STM/STS to be applied in large areas, although edge quality was limited by etch debris. To overcome this, we developed Design II, which avoids etching through the film by locally thinning it, leaving nanoscale ribbons raised above a continuous TI layer, followed again by Nb capping and sample flipping before cleaving. This method yields clean, reproducible nanostructures with well-defined superconducting gaps, demonstrating a reliable fabrication pathway for high-resolution STM/STS studies of nanoscale topological devices. | [üîó Paper](http://arxiv.org/abs/2507.14092v1) |
| [A variational approach to the emergence of domain structures in
  magnetostrictive solids](http://arxiv.org/abs/2507.14091v1) | Marco Bresciani, Manuel Friedrich | 2025-07-18 | General AI | We consider a variational model for magnetoelastic solids in the large-strain setting with the magnetization field defined on the unknown deformed configuration. Through a simultaneous linearization of the deformation and sharp-interface limit of the magnetization with respect to the easy axes, performed in terms of $\Gamma$-convergence, we identify a variational model describing the formation of domain structures and accounting for magnetostriction in the small-strain setting. Our analysis incorporates the effect of magnetic fields and, for uniaxial materials, ensures the regularity of minimizers of the effective energy. | [üîó Paper](http://arxiv.org/abs/2507.14091v1) |
| [Multimode Nanobeam Photonic Crystal Cavities for Purcell Enhanced
  Quantum Dot Emission](http://arxiv.org/abs/2507.14090v1) | Junyeob Song, Ashish Chanana, Emerson Melo, William Eshbaugh, Craig Copeland, Luca Sapienza, Edward Flagg, Jin-Dong Song, Kartik Srinivasan, Marcelo Davanco | 2025-07-18 | General AI | Epitaxial III-V semiconductor quantum dots in nanopthonic structures are promising candidates for implementing on-demand indistinguishable single-photon emission in integrated quantum photonic circuits. Quantum dot proximity to the etched sidewalls of hosting nanophotonic structures, however, has been shown to induce linewidth broadening of excitonic transitions, which limits emitted single-photon indistinguishability. Here, we design and demonstrate GaAs photonic crystal nanobeam cavities that maximize quantum dot distances to etched sidewalls beyond an empirically determined minimum that curtails spectral broadening. Although such geometric constraint necessarily leads to multimode propagation in nanobeams, which significantly complicates high quality factor cavity design, we achieve resonances with quality factors $Q\approx10^3$, which offer the potential for achieving Purcell radiative rate enhancements $F_p\approx100$. | [üîó Paper](http://arxiv.org/abs/2507.14090v1) |
| [An Efficient Massively Parallel Constant-Factor Approximation Algorithm
  for the $k$-Means Problem](http://arxiv.org/abs/2507.14089v1) | Vincent Cohen-Addad, Fabian Kuhn, Zahra Parsaeian | 2025-07-18 | General AI | In this paper, we present an efficient massively parallel approximation algorithm for the $k$-means problem. Specifically, we provide an MPC algorithm that computes a constant-factor approximation to an arbitrary $k$-means instance in $O(\log\log n \cdot \log\log\log n)$ rounds. The algorithm uses $O(n^\sigma)$ bits of memory per machine, where $\sigma > 0$ is a constant that can be made arbitrarily small. The global memory usage is $O(n^{1+\varepsilon})$ bits for an arbitrarily small constant $\varepsilon > 0$, and is thus only slightly superlinear. Recently, Czumaj, Gao, Jiang, Krauthgamer, and Vesel\'{y} showed that a constant-factor bicriteria approximation can be computed in $O(1)$ rounds in the MPC model. However, our algorithm is the first constant-factor approximation for the general $k$-means problem that runs in $o(\log n)$ rounds in the MPC model.   Our approach builds upon the foundational framework of Jain and Vazirani. The core component of our algorithm is a constant-factor approximation for the related facility location problem. While such an approximation was already achieved in constant time in the work of Czumaj et al.\ mentioned above, our version additionally satisfies the so-called Lagrangian Multiplier Preserving (LMP) property. This property enables the transformation of a facility location approximation into a comparably good $k$-means approximation. | [üîó Paper](http://arxiv.org/abs/2507.14089v1) |
