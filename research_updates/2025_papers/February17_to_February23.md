# ðŸ“Œ AI Research Papers (February17 to February23)

## ðŸ”¹ LLM

| ðŸ“„ Title | ðŸ–Š Authors | ðŸ“… Date | ðŸ· Tags | ðŸ“œ Summary | ðŸ”— Link |
|---------|---------|---------|---------|---------|---------|
| [FR-Spec: Accelerating Large-Vocabulary Language Models via Frequency-Ranked Speculative Sampling](http://arxiv.org/abs/2502.14856v1) | Weilin Zhao, Tengyu Pan, Xu Han, Yudi Zhang, Ao Sun, Yuxiang Huang, Kaihuo Zhang, Weilun Zhao, Yuxuan Li, Jianyong Wang, Zhiyuan Liu, Maosong Sun | 2025-02-20 | LLM | Speculative sampling has emerged as an important technique for accelerating the auto-regressive generation process of large language models (LLMs) by utilizing a draft-then-verify mechanism to produce multiple tokens per forward pass. While state-of-the-art speculative sampling methods use only a single layer and a language modeling (LM) head as the draft model to achieve impressive layer compression, their efficiency gains are substantially reduced for large-vocabulary LLMs, such as Llama-3-8B with a vocabulary of 128k tokens. To address this, we present FR-Spec, a frequency-ranked speculative sampling framework that optimizes draft candidate selection through vocabulary space compression. By constraining the draft search to a frequency-prioritized token subset, our method reduces LM Head computation overhead by 75% while ensuring the equivalence of the final output distribution. Experiments across multiple datasets demonstrate an average of 1.12$\times$ speedup over the state-of-the-art speculative sampling method EAGLE-2. | [ðŸ”— Paper](http://arxiv.org/abs/2502.14856v1) |
## ðŸ”¹ Optimization

| ðŸ“„ Title | ðŸ–Š Authors | ðŸ“… Date | ðŸ· Tags | ðŸ“œ Summary | ðŸ”— Link |
|---------|---------|---------|---------|---------|---------|
| [Aligning LLMs to Ask Good Questions A Case Study in Clinical Reasoning](http://arxiv.org/abs/2502.14860v1) | Shuyue Stella Li, Jimin Mun, Faeze Brahman, Jonathan S. Ilgen, Yulia Tsvetkov, Maarten Sap | 2025-02-20 | Optimization | Large language models (LLMs) often fail to ask effective questions under uncertainty, making them unreliable in domains where proactive information-gathering is essential for decisionmaking. We present ALFA, a framework that improves LLM question-asking by (i) decomposing the notion of a "good" question into a set of theory-grounded attributes (e.g., clarity, relevance), (ii) controllably synthesizing attribute-specific question variations, and (iii) aligning models via preference-based optimization to explicitly learn to ask better questions along these fine-grained attributes. Focusing on clinical reasoning as a case study, we introduce the MediQ-AskDocs dataset, composed of 17k real-world clinical interactions augmented with 80k attribute-specific preference pairs of follow-up questions, as well as a novel expert-annotated interactive healthcare QA task to evaluate question-asking abilities. Models aligned with ALFA reduce diagnostic errors by 56.6% on MediQ-AskDocs compared to SOTA instruction-tuned LLMs, with a question-level win-rate of 64.4% and strong generalizability. Our findings suggest that explicitly guiding question-asking with structured, fine-grained attributes offers a scalable path to improve LLMs, especially in expert application domains. | [ðŸ”— Paper](http://arxiv.org/abs/2502.14860v1) |
## ðŸ”¹ Training & Evaluation

| ðŸ“„ Title | ðŸ–Š Authors | ðŸ“… Date | ðŸ· Tags | ðŸ“œ Summary | ðŸ”— Link |
|---------|---------|---------|---------|---------|---------|
| [Time Travel: A Comprehensive Benchmark to Evaluate LMMs on Historical and Cultural Artifacts](http://arxiv.org/abs/2502.14865v1) | Sara Ghaboura, Ketan More, Ritesh Thawkar, Wafa Alghallabi, Omkar Thawakar, Fahad Shahbaz Khan, Hisham Cholakkal, Salman Khan, Rao Muhammad Anwer | 2025-02-20 | Training & Evaluation, Multimodal AI | Understanding historical and cultural artifacts demands human expertise and advanced computational techniques, yet the process remains complex and time-intensive. While large multimodal models offer promising support, their evaluation and improvement require a standardized benchmark. To address this, we introduce TimeTravel, a benchmark of 10,250 expert-verified samples spanning 266 distinct cultures across 10 major historical regions. Designed for AI-driven analysis of manuscripts, artworks, inscriptions, and archaeological discoveries, TimeTravel provides a structured dataset and robust evaluation framework to assess AI models' capabilities in classification, interpretation, and historical comprehension. By integrating AI with historical research, TimeTravel fosters AI-powered tools for historians, archaeologists, researchers, and cultural tourists to extract valuable insights while ensuring technology contributes meaningfully to historical discovery and cultural heritage preservation. We evaluate contemporary AI models on TimeTravel, highlighting their strengths and identifying areas for improvement. Our goal is to establish AI as a reliable partner in preserving cultural heritage, ensuring that technological advancements contribute meaningfully to historical discovery. Our code is available at: \url{https://github.com/mbzuai-oryx/TimeTravel}. | [ðŸ”— Paper](http://arxiv.org/abs/2502.14865v1) |
| [Prompt-to-Leaderboard](http://arxiv.org/abs/2502.14855v1) | Evan Frick, Connor Chen, Joseph Tennyson, Tianle Li, Wei-Lin Chiang, Anastasios N. Angelopoulos, Ion Stoica | 2025-02-20 | Training & Evaluation, Scaling Laws, LLM | Large language model (LLM) evaluations typically rely on aggregated metrics like accuracy or human preference, averaging across users and prompts. This averaging obscures user- and prompt-specific variations in model performance. To address this, we propose Prompt-to-Leaderboard (P2L), a method that produces leaderboards specific to a prompt. The core idea is to train an LLM taking natural language prompts as input to output a vector of Bradley-Terry coefficients which are then used to predict the human preference vote. The resulting prompt-dependent leaderboards allow for unsupervised task-specific evaluation, optimal routing of queries to models, personalization, and automated evaluation of model strengths and weaknesses. Data from Chatbot Arena suggest that P2L better captures the nuanced landscape of language model performance than the averaged leaderboard. Furthermore, our findings suggest that P2L's ability to produce prompt-specific evaluations follows a power law scaling similar to that observed in LLMs themselves. In January 2025, the router we trained based on this methodology achieved the \#1 spot in the Chatbot Arena leaderboard. Our code is available at this GitHub link: https://github.com/lmarena/p2l. | [ðŸ”— Paper](http://arxiv.org/abs/2502.14855v1) |
## ðŸ”¹ Prompt Engineering

| ðŸ“„ Title | ðŸ–Š Authors | ðŸ“… Date | ðŸ· Tags | ðŸ“œ Summary | ðŸ”— Link |
|---------|---------|---------|---------|---------|---------|
| [CLIPPER: Compression enables long-context synthetic data generation](http://arxiv.org/abs/2502.14854v1) | Chau Minh Pham, Yapei Chang, Mohit Iyyer | 2025-02-20 | Prompt Engineering | LLM developers are increasingly reliant on synthetic data, but generating high-quality data for complex long-context reasoning tasks remains challenging. We introduce CLIPPER, a compression-based approach for generating synthetic data tailored to narrative claim verification - a task that requires reasoning over a book to verify a given claim. Instead of generating claims directly from the raw text of the book, which results in artifact-riddled claims, CLIPPER first compresses the book into chapter outlines and book summaries and then uses these intermediate representations to generate complex claims and corresponding chain-of-thoughts. Compared to naive approaches, CLIPPER produces claims that are more valid, grounded, and complex. Using CLIPPER, we construct a dataset of 19K synthetic book claims paired with their source texts and chain-of-thought reasoning, and use it to fine-tune three open-weight models. Our best model achieves breakthrough results on narrative claim verification (from 28% to 76% accuracy on our test set) and sets a new state-of-the-art for sub-10B models on the NoCha leaderboard. Further analysis shows that our models generate more detailed and grounded chain-of-thought reasoning while also improving performance on other narrative understanding tasks (e.g., NarrativeQA). | [ðŸ”— Paper](http://arxiv.org/abs/2502.14854v1) |
## ðŸ”¹ AI Safety

| ðŸ“„ Title | ðŸ–Š Authors | ðŸ“… Date | ðŸ· Tags | ðŸ“œ Summary | ðŸ”— Link |
|---------|---------|---------|---------|---------|---------|
| [Interpretable Text Embeddings and Text Similarity Explanation: A Primer](http://arxiv.org/abs/2502.14862v1) | Juri Opitz, Lucas MÃ¶ller, Andrianos Michail, Simon Clematide | 2025-02-20 | AI Safety | Text embeddings and text embedding models are a backbone of many AI and NLP systems, particularly those involving search. However, interpretability challenges persist, especially in explaining obtained similarity scores, which is crucial for applications requiring transparency. In this paper, we give a structured overview of interpretability methods specializing in explaining those similarity scores, an emerging research area. We study the methods' individual ideas and techniques, evaluating their potential for improving interpretability of text embeddings and explaining predicted similarities. | [ðŸ”— Paper](http://arxiv.org/abs/2502.14862v1) |
## ðŸ”¹ Responsible AI

| ðŸ“„ Title | ðŸ–Š Authors | ðŸ“… Date | ðŸ· Tags | ðŸ“œ Summary | ðŸ”— Link |
|---------|---------|---------|---------|---------|---------|
| [Benchmarking Multimodal RAG through a Chart-based Document Question-Answering Generation Framework](http://arxiv.org/abs/2502.14864v1) | Yuming Yang, Jiang Zhong, Li Jin, Jingwang Huang, Jingpeng Gao, Qing Liu, Yang Bai, Jingyuan Zhang, Rui Jiang, Kaiwen Wei | 2025-02-20 | Responsible AI, Model Evaluation, Training & Evaluation, RAG, Multimodal AI | Multimodal Retrieval-Augmented Generation (MRAG) enhances reasoning capabilities by integrating external knowledge. However, existing benchmarks primarily focus on simple image-text interactions, overlooking complex visual formats like charts that are prevalent in real-world applications. In this work, we introduce a novel task, Chart-based MRAG, to address this limitation. To semi-automatically generate high-quality evaluation samples, we propose CHARt-based document question-answering GEneration (CHARGE), a framework that produces evaluation data through structured keypoint extraction, crossmodal verification, and keypoint-based generation. By combining CHARGE with expert validation, we construct Chart-MRAG Bench, a comprehensive benchmark for chart-based MRAG evaluation, featuring 4,738 question-answering pairs across 8 domains from real-world documents. Our evaluation reveals three critical limitations in current approaches: (1) unified multimodal embedding retrieval methods struggles in chart-based scenarios, (2) even with ground-truth retrieval, state-of-the-art MLLMs achieve only 58.19% Correctness and 73.87% Coverage scores, and (3) MLLMs demonstrate consistent text-over-visual modality bias during Chart-based MRAG reasoning. The CHARGE and Chart-MRAG Bench are released at https://github.com/Nomothings/CHARGE.git. | [ðŸ”— Paper](http://arxiv.org/abs/2502.14864v1) |
## ðŸ”¹ General AI

| ðŸ“„ Title | ðŸ–Š Authors | ðŸ“… Date | ðŸ· Tags | ðŸ“œ Summary | ðŸ”— Link |
|---------|---------|---------|---------|---------|---------|
| [Emergence of Fermi's Golden Rule in the Probing of a Quantum Many-Body System](http://arxiv.org/abs/2502.14867v1) | Jianyi Chen, Songtao Huang, Yunpeng Ji, Grant L. Schumacher, Alan Tsidilkovski, Alexander Schuckert, Gabriel G. T. AssumpÃ§Ã£o, Nir Navon | 2025-02-20 | General AI | Fermi's Golden Rule (FGR) is one of the most impactful formulas in quantum mechanics, providing a link between easy-to-measure observables - such as transition rates - and fundamental microscopic properties - such as density of states or spectral functions. Its validity relies on three key assumptions: the existence of a continuum, an appropriate time window, and a weak coupling. Understanding the regime of validity of FGR is critical for the proper interpretation of most spectroscopic experiments. While the assumptions underlying FGR are straightforward to analyze in simple models, their applicability is significantly more complex in quantum many-body systems. Here, we observe the emergence and breakdown of FGR, using a strongly interacting homogeneous spin-$1/2$ Fermi gas coupled to a radio-frequency (rf) field. Measuring the transition probability into an outcoupled internal state, we map the system's dynamical response diagram versus the rf-pulse duration $t$ and Rabi frequency $\Omega_0$. For weak drives, we identify three regimes: an early-time regime where the transition probability takes off as $t^2$, an intermediate-time FGR regime, and a long-time non-perturbative regime. Beyond a threshold Rabi frequency, Rabi oscillations appear. Our results provide a blueprint for the applicability of linear response theory to the spectroscopy of quantum many-body systems. | [ðŸ”— Paper](http://arxiv.org/abs/2502.14867v1) |
| [LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention](http://arxiv.org/abs/2502.14866v1) | Shang Yang, Junxian Guo, Haotian Tang, Qinghao Hu, Guangxuan Xiao, Jiaming Tang, Yujun Lin, Zhijian Liu, Yao Lu, Song Han | 2025-02-20 | General AI | Large language models (LLMs) have shown remarkable potential in processing long sequences, yet efficiently serving these long-context models remains challenging due to the quadratic computational complexity of attention in the prefilling stage and the large memory footprint of the KV cache in the decoding stage. To address these issues, we introduce LServe, an efficient system that accelerates long-sequence LLM serving via hybrid sparse attention. This method unifies different hardware-friendly, structured sparsity patterns for both prefilling and decoding attention into a single framework, where computations on less important tokens are skipped block-wise. LServe demonstrates the compatibility of static and dynamic sparsity in long-context LLM attention. This design enables multiplicative speedups by combining these optimizations. Specifically, we convert half of the attention heads to nearly free streaming heads in both the prefilling and decoding stages. Additionally, we find that only a constant number of KV pages is required to preserve long-context capabilities, irrespective of context length. We then design a hierarchical KV page selection policy that dynamically prunes KV pages based on query-centric similarity. On average, LServe accelerates LLM prefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is released at https://github.com/mit-han-lab/omniserve. | [ðŸ”— Paper](http://arxiv.org/abs/2502.14866v1) |
| [The Fourier coefficients of the holomorphic multiplicative chaos in the limit of large frequency](http://arxiv.org/abs/2502.14863v1) | Joseph Najnudel, Elliot Paquette, Nick Simm, Truong Vu | 2025-02-20 | General AI | The holomorphic multiplicative chaos (HMC) is a holomorphic analogue of the Gaussian multiplicative chaos. It arises naturally as the limit in large matrix size of the characteristic polynomial of Haar unitary, and more generally circular-$\beta$-ensemble, random matrices.   We consider the Fourier coefficients of the holomorphic multiplicative chaos in the $L^1$-phase, and we show that appropriately normalized, this converges in distribution to a complex normal random variable, scaled by the total mass of the Gaussian multiplicative chaos measure on the unit circle. We further generalize this to a process convergence, showing the joint convergence of consecutive Fourier coefficients. As a corollary, we derive convergence in law of the secular coefficients of sublinear index of the circular-$\beta$-ensemble for all $\beta > 2$. | [ðŸ”— Paper](http://arxiv.org/abs/2502.14863v1) |
| [Stacking-dependent topological electronic structures in honeycomb-kagome heterolayers](http://arxiv.org/abs/2502.14861v1) | Chan Bin Bark, Hanbyul Kim, Seik Pak, Hong-Guk Min, Sungkyun Ahn, Youngkuk Kim, Moon Jip Park | 2025-02-20 | General AI | Heterostructures of stacked two-dimensional lattices have shown great promise for engineering novel material properties. As an archetypal example of such a system, the hexagon-shared honeycomb-kagome lattice has been experimentally synthesized in various material platforms. In this work, we explore three rotationally symmetric variants of the honeycomb-kagome lattice: the hexagonal, triagonal, and biaxial phases. While the triagonal and biaxial phases exhibit trivial insulating and Dirac semimetal band structures, respectively, the hexagonal phase hosts a higher-order topological phase driven by band inversion near the $\Gamma$-point. This highlights a key distinction from the conventional band inversions at the $K$-point observed in hexagonal homobilayer systems. Furthermore, we demonstrate how the distinct topological properties of these phases result in network band structures within moir\'e heterostructures formed by twisted or lattice-mismatched HK systems. These network band structures can be experimentally observed through extrinsic twisting or intrinsic lattice mismatching between the honeycomb and kagome systems. | [ðŸ”— Paper](http://arxiv.org/abs/2502.14861v1) |
| [Taming Recoil Effect in Cavity-Assisted Quantum Interconnects](http://arxiv.org/abs/2502.14859v1) | Seigo Kikura, Ryotaro Inoue, Hayata Yamasaki, Akihisa Goban, Shinichi Sunami | 2025-02-20 | General AI | Photon recoil is one of the fundamental limitations for high-fidelity control of trapped-atom qubits such as neutral atoms and trapped ions. In this work, we derive an analytical model for efficiently evaluating the motion-induced infidelity in remote entanglement generation protocols. Our model is applicable for various photonic qubit encodings such as polarization, time-bin, and frequency, and with arbitrary initial motional states, thus providing a crucial theoretical tool for realizing high-fidelity quantum networking. For the case of tweezer-trapped neutral atoms, our results indicate that operating in the bad-cavity regime with cavity decay rate exceeding atom-photon coupling rate, and near-ground-state cooling with motional quanta below 1, are desired to suppress the motion-induced infidelity sufficiently below the 1\% level required for efficient quantum networking. Finite temperature effects can be mitigated efficiently by detection time filtering at the moderate cost of success probability and network speed. These results extend the understanding of infidelity sources in remote entanglement generation protocols, establishing a concrete path towards fault-tolerant quantum networking with scalable trapped-atom qubit systems. | [ðŸ”— Paper](http://arxiv.org/abs/2502.14859v1) |
| [The $p$-adic Galois Cohomology of Valuation Fields](http://arxiv.org/abs/2502.14858v1) | Tongmu He | 2025-02-20 | General AI | We compute the Galois cohomology of any $p$-adic valuation field extension of a pre-perfectoid field. Moreover, we obtain a generalization and also a new proof of the classical results of Tate and Hyodo on discrete valuation fields, without using higher ramification group, local class field theory or Epp's elimination of ramifications. A key ingredient is Gabber-Ramero's computation of cotangent complexes for valuation rings. | [ðŸ”— Paper](http://arxiv.org/abs/2502.14858v1) |
| [A Correlation Inequality on Three Functions](http://arxiv.org/abs/2502.14857v2) | Kada Williams | 2025-02-20 | General AI | Let $X$ and $Y$ be upward closed set systems in the lattice of $\{0,1\}^n$. The celebrated Harris-Kleitman inequality implies that if $ X =\alpha 2^n$, $ Y =\beta 2^n$, the density of the set of points in exactly one of $X$ and $Y$ is maximal when $X$ and $Y$ are independent, meaning $ X\cap Y =\alpha\beta 2^n$. Is the same true of three upward closed systems, $X$, $Y$, and $Z$? Suppose $ X = Y = Z $. Kahn asked whether the set of points in exactly one of $X$, $Y$, $Z$ has density at most $\frac49$. We answer this question in the negative. | [ðŸ”— Paper](http://arxiv.org/abs/2502.14857v2) |
| [On the $H$-property for Step-graphons: Residual Case](http://arxiv.org/abs/2502.14853v1) | Wanting Gao, Xudong Chen | 2025-02-20 | General AI | We sample graphs $G_n$ on $n$ nodes from a step-graphon and evaluate the probability that $G_n$ has a Hamiltonian decomposition in the asymptotic regime as $n\to\infty$. It has recently been shown that for almost all step-graphons, this probability converges to either zero or one. In this paper, we focus on the class of step-graphons such that the zero-one property does not hold. We show in this case that the limit of the probability still exists and provide an explicit expression of it. | [ðŸ”— Paper](http://arxiv.org/abs/2502.14853v1) |
