# ğŸ“Œ AI Research Papers (July21 to July27)

## ğŸ”¹ RLHF

| ğŸ“„ Title | ğŸ–Š Authors | ğŸ“… Date | ğŸ· Tags | ğŸ“œ Summary | ğŸ”— Link |
|---------|---------|---------|---------|---------|---------|
| [SIDA: Synthetic Image Driven Zero-shot Domain Adaptation](http://arxiv.org/abs/2507.18632v1) | Ye-Chan Kim, SeungJu Cha, Si-Woo Kim, Taewhan Kim, Dong-Jin Kim | 2025-07-24 | RLHF, Prompt Engineering, Multimodal AI | Zero-shot domain adaptation is a method for adapting a model to a target domain without utilizing target domain image data. To enable adaptation without target images, existing studies utilize CLIP's embedding space and text description to simulate target-like style features. Despite the previous achievements in zero-shot domain adaptation, we observe that these text-driven methods struggle to capture complex real-world variations and significantly increase adaptation time due to their alignment process. Instead of relying on text descriptions, we explore solutions leveraging image data, which provides diverse and more fine-grained style cues. In this work, we propose SIDA, a novel and efficient zero-shot domain adaptation method leveraging synthetic images. To generate synthetic images, we first create detailed, source-like images and apply image translation to reflect the style of the target domain. We then utilize the style features of these synthetic images as a proxy for the target domain. Based on these features, we introduce Domain Mix and Patch Style Transfer modules, which enable effective modeling of real-world variations. In particular, Domain Mix blends multiple styles to expand the intra-domain representations, and Patch Style Transfer assigns different styles to individual patches. We demonstrate the effectiveness of our method by showing state-of-the-art performance in diverse zero-shot adaptation scenarios, particularly in challenging domains. Moreover, our approach achieves high efficiency by significantly reducing the overall adaptation time. | [ğŸ”— Paper](http://arxiv.org/abs/2507.18632v1) |
## ğŸ”¹ Multimodal AI

| ğŸ“„ Title | ğŸ–Š Authors | ğŸ“… Date | ğŸ· Tags | ğŸ“œ Summary | ğŸ”— Link |
|---------|---------|---------|---------|---------|---------|
| [Captain Cinema: Towards Short Movie Generation](http://arxiv.org/abs/2507.18634v1) | Junfei Xiao, Ceyuan Yang, Lvmin Zhang, Shengqu Cai, Yang Zhao, Yuwei Guo, Gordon Wetzstein, Maneesh Agrawala, Alan Yuille, Lu Jiang | 2025-07-24 | Multimodal AI, Memory & Context Length, Diffusion Models | We present Captain Cinema, a generation framework for short movie generation. Given a detailed textual description of a movie storyline, our approach firstly generates a sequence of keyframes that outline the entire narrative, which ensures long-range coherence in both the storyline and visual appearance (e.g., scenes and characters). We refer to this step as top-down keyframe planning. These keyframes then serve as conditioning signals for a video synthesis model, which supports long context learning, to produce the spatio-temporal dynamics between them. This step is referred to as bottom-up video synthesis. To support stable and efficient generation of multi-scene long narrative cinematic works, we introduce an interleaved training strategy for Multimodal Diffusion Transformers (MM-DiT), specifically adapted for long-context video data. Our model is trained on a specially curated cinematic dataset consisting of interleaved data pairs. Our experiments demonstrate that Captain Cinema performs favorably in the automated creation of visually coherent and narrative consistent short movies in high quality and efficiency. Project page: https://thecinema.ai | [ğŸ”— Paper](http://arxiv.org/abs/2507.18634v1) |
## ğŸ”¹ Optimization

| ğŸ“„ Title | ğŸ–Š Authors | ğŸ“… Date | ğŸ· Tags | ğŸ“œ Summary | ğŸ”— Link |
|---------|---------|---------|---------|---------|---------|
| [Layer-Aware Representation Filtering: Purifying Finetuning Data to
  Preserve LLM Safety Alignment](http://arxiv.org/abs/2507.18631v2) | Hao Li, Lijun Li, Zhenghao Lu, Xianyi Wei, Rui Li, Jing Shao, Lei Sha | 2025-07-24 | Optimization, RLHF | With rapid advancement and increasing accessibility of LLMs, fine-tuning aligned models has become a critical step for adapting them to real-world applications, which makes the safety of this fine-tuning process more important than ever. However, recent studies have highlighted a critical challenge: even when fine-tuning with seemingly benign downstream datasets, the safety of aligned LLMs can be compromised, making them more susceptible to malicious instructions.   In this paper, we show that fine-tuning datasets often contain samples with safety-degrading features that are not easily identifiable on the surface. These samples can significantly degrade the safety alignment of LLMs during fine-tuning. To address this issue, we propose LARF, a Layer-Aware Representation Filtering method. This method identifies safety-sensitive layers within the LLM and leverages their representations to detect which data samples in the post-training dataset contain safety-degrading features.   Experimental results demonstrate that LARF can effectively identify benign data with safety-degrading features. After removing such data, the safety alignment degradation caused by fine-tuning is mitigated. Please see our code at https://github.com/LLLeoLi/LARF. | [ğŸ”— Paper](http://arxiv.org/abs/2507.18631v2) |
| [Design and optimization of a novel leaf-shape antenna for RF energy
  transfer](http://arxiv.org/abs/2507.18630v1) | Junbin Zhong, Mingtong Chen, Zhengbao Yang | 2025-07-24 | Optimization, RLHF | In this research, the design and optimization of a novel leaf-shaped antenna inspired by natural leaf structures for radio frequency energy transfer is presented. The objectives of this study are to develop a bio-inspired antenna, optimize its performance through impedance matching for the 915 MHz frequency band, and evaluate its efficiency in capturing RF energy. The design process involves selecting an appropriate leaf shape, modeling the antenna using AutoCAD and HFSS software, and fabricating a printed circuit board (PCB) prototype. Simulations and physical tests are conducted to optimize the antennas performance, achieving an S11 parameter of nearly -20 dB at 915 MHz, indicating effective energy capture. Experimental results demonstrate the antennas ability to power a device at distances up to 200 cm, with charging times reflecting its efficiency. The study concludes that the bio-inspired design of the proposed antenna improves RF energy transfer. Future work should focus on testing the antennas penetration through concrete and developing a feedback system for autonomous alignment. | [ğŸ”— Paper](http://arxiv.org/abs/2507.18630v1) |
## ğŸ”¹ Prompt Engineering

| ğŸ“„ Title | ğŸ–Š Authors | ğŸ“… Date | ğŸ· Tags | ğŸ“œ Summary | ğŸ”— Link |
|---------|---------|---------|---------|---------|---------|
| [Identifying Prompted Artist Names from Generated Images](http://arxiv.org/abs/2507.18633v1) | Grace Su, Sheng-Yu Wang, Aaron Hertzmann, Eli Shechtman, Jun-Yan Zhu, Richard Zhang | 2025-07-24 | Prompt Engineering, Diffusion Models | A common and controversial use of text-to-image models is to generate pictures by explicitly naming artists, such as "in the style of Greg Rutkowski". We introduce a benchmark for prompted-artist recognition: predicting which artist names were invoked in the prompt from the image alone. The dataset contains 1.95M images covering 110 artists and spans four generalization settings: held-out artists, increasing prompt complexity, multiple-artist prompts, and different text-to-image models. We evaluate feature similarity baselines, contrastive style descriptors, data attribution methods, supervised classifiers, and few-shot prototypical networks. Generalization patterns vary: supervised and few-shot models excel on seen artists and complex prompts, whereas style descriptors transfer better when the artist's style is pronounced; multi-artist prompts remain the most challenging. Our benchmark reveals substantial headroom and provides a public testbed to advance the responsible moderation of text-to-image models. We release the dataset and benchmark to foster further research: https://graceduansu.github.io/IdentifyingPromptedArtists/ | [ğŸ”— Paper](http://arxiv.org/abs/2507.18633v1) |
