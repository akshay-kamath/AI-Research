# 📌 AI Research Papers (August11 to August17)

## 🔹 Diffusion Models

| 📄 Title | 🖊 Authors | 📅 Date | 🏷 Tags | 📜 Summary | 🔗 Link |
|---------|---------|---------|---------|---------|---------|
| [Exchange-driven self-diffusion of nanoscale crystalline parahydrogen
  clusters on graphite](http://arxiv.org/abs/2508.10883v1) | K. M. Kolevski, M. Boninsegni | 2025-08-14 | Diffusion Models | Computer simulations yield evidence of superfluid behavior of nanoscale size clusters of parahydrogen adsorbed on a graphite substrate at low temperature ($T\lesssim 0.25 \text{ K}$). Clusters with a number of molecules between 7 and 12 display concurrent superfluidity and crystalline order, reflecting the corrugation of the substrate. Remarkably, it is found that specific clusters with a number of molecules ranging between 7 and 12 self-diffuse on the surface like free particles, despite the strong pinning effect of the substrate. This effect is underlain by coordinated quantum-mechanical exchanges of groups of identical molecules, i.e., it has no classical counterpart. | [🔗 Paper](http://arxiv.org/abs/2508.10883v1) |
## 🔹 Multimodal AI

| 📄 Title | 🖊 Authors | 📅 Date | 🏷 Tags | 📜 Summary | 🔗 Link |
|---------|---------|---------|---------|---------|---------|
| [A Dataset for Distilling Knowledge Priors from Literature for
  Therapeutic Design](http://arxiv.org/abs/2508.10899v1) | Haydn Thomas Jones, Natalie Maus, Josh Magnus Ludan, Maggie Ziyu Huan, Jiaming Liang, Marcelo Der Torossian Torres, Jiatao Liang, Zachary Ives, Yoseph Barash, Cesar de la Fuente-Nunez, Jacob R. Gardner, Mark Yatskar | 2025-08-14 | Multimodal AI | AI-driven discovery can greatly reduce design time and enhance new therapeutics' effectiveness. Models using simulators explore broad design spaces but risk violating implicit constraints due to a lack of experimental priors. For example, in a new analysis we performed on a diverse set of models on the GuacaMol benchmark using supervised classifiers, over 60\% of molecules proposed had high probability of being mutagenic. In this work, we introduce \ourdataset, a dataset of priors for design problems extracted from literature describing compounds used in lab settings. It is constructed with LLM pipelines for discovering therapeutic entities in relevant paragraphs and summarizing information in concise fair-use facts. \ourdataset~ consists of 32.3 million pairs of natural language facts, and appropriate entity representations (i.e. SMILES or refseq IDs). To demonstrate the potential of the data, we train LLM, CLIP, and LLava architectures to reason jointly about text and design targets and evaluate on tasks from the Therapeutic Data Commons (TDC). \ourdataset~is highly effective for creating models with strong priors: in supervised prediction problems that use our data as pretraining, our best models with 15M learnable parameters outperform larger 2B TxGemma on both regression and classification TDC tasks, and perform comparably to 9B models on average. Models built with \ourdataset~can be used as constraints while optimizing for novel molecules in GuacaMol, resulting in proposals that are safer and nearly as effective. We release our dataset at \href{https://huggingface.co/datasets/medexanon/Medex}{huggingface.co/datasets/medexanon/Medex}, and will provide expanded versions as available literature grows. | [🔗 Paper](http://arxiv.org/abs/2508.10899v1) |
| [ESSENTIAL: Episodic and Semantic Memory Integration for Video
  Class-Incremental Learning](http://arxiv.org/abs/2508.10896v1) | Jongseo Lee, Kyungho Bae, Kyle Min, Gyeong-Moon Park, Jinwoo Choi | 2025-08-14 | Multimodal AI | In this work, we tackle the problem of video classincremental learning (VCIL). Many existing VCIL methods mitigate catastrophic forgetting by rehearsal training with a few temporally dense samples stored in episodic memory, which is memory-inefficient. Alternatively, some methods store temporally sparse samples, sacrificing essential temporal information and thereby resulting in inferior performance. To address this trade-off between memory-efficiency and performance, we propose EpiSodic and SEmaNTIc memory integrAtion for video class-incremental Learning (ESSENTIAL). ESSENTIAL consists of episodic memory for storing temporally sparse features and semantic memory for storing general knowledge represented by learnable prompts. We introduce a novel memory retrieval (MR) module that integrates episodic memory and semantic prompts through cross-attention, enabling the retrieval of temporally dense features from temporally sparse features. We rigorously validate ESSENTIAL on diverse datasets: UCF-101, HMDB51, and Something-Something-V2 from the TCD benchmark and UCF-101, ActivityNet, and Kinetics-400 from the vCLIMB benchmark. Remarkably, with significantly reduced memory, ESSENTIAL achieves favorable performance on the benchmarks. | [🔗 Paper](http://arxiv.org/abs/2508.10896v1) |
## 🔹 Optimization

| 📄 Title | 🖊 Authors | 📅 Date | 🏷 Tags | 📜 Summary | 🔗 Link |
|---------|---------|---------|---------|---------|---------|
| [Puppeteer: Rig and Animate Your 3D Models](http://arxiv.org/abs/2508.10898v1) | Chaoyue Song, Xiu Li, Fan Yang, Zhongcong Xu, Jiacheng Wei, Fayao Liu, Jiashi Feng, Guosheng Lin, Jianfeng Zhang | 2025-08-14 | Optimization, LLM | Modern interactive applications increasingly demand dynamic 3D content, yet the transformation of static 3D models into animated assets constitutes a significant bottleneck in content creation pipelines. While recent advances in generative AI have revolutionized static 3D model creation, rigging and animation continue to depend heavily on expert intervention. We present Puppeteer, a comprehensive framework that addresses both automatic rigging and animation for diverse 3D objects. Our system first predicts plausible skeletal structures via an auto-regressive transformer that introduces a joint-based tokenization strategy for compact representation and a hierarchical ordering methodology with stochastic perturbation that enhances bidirectional learning capabilities. It then infers skinning weights via an attention-based architecture incorporating topology-aware joint attention that explicitly encodes inter-joint relationships based on skeletal graph distances. Finally, we complement these rigging advances with a differentiable optimization-based animation pipeline that generates stable, high-fidelity animations while being computationally more efficient than existing approaches. Extensive evaluations across multiple benchmarks demonstrate that our method significantly outperforms state-of-the-art techniques in both skeletal prediction accuracy and skinning quality. The system robustly processes diverse 3D content, ranging from professionally designed game assets to AI-generated shapes, producing temporally coherent animations that eliminate the jittering issues common in existing methods. | [🔗 Paper](http://arxiv.org/abs/2508.10898v1) |
| [STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer](http://arxiv.org/abs/2508.10893v1) | Yushi Lan, Yihang Luo, Fangzhou Hong, Shangchen Zhou, Honghua Chen, Zhaoyang Lyu, Shuai Yang, Bo Dai, Chen Change Loy, Xingang Pan | 2025-08-14 | Optimization, LLM | We present STream3R, a novel approach to 3D reconstruction that reformulates pointmap prediction as a decoder-only Transformer problem. Existing state-of-the-art methods for multi-view reconstruction either depend on expensive global optimization or rely on simplistic memory mechanisms that scale poorly with sequence length. In contrast, STream3R introduces an streaming framework that processes image sequences efficiently using causal attention, inspired by advances in modern language modeling. By learning geometric priors from large-scale 3D datasets, STream3R generalizes well to diverse and challenging scenarios, including dynamic scenes where traditional methods often fail. Extensive experiments show that our method consistently outperforms prior work across both static and dynamic scene benchmarks. Moreover, STream3R is inherently compatible with LLM-style training infrastructure, enabling efficient large-scale pretraining and fine-tuning for various downstream 3D tasks. Our results underscore the potential of causal Transformer models for online 3D perception, paving the way for real-time 3D understanding in streaming environments. More details can be found in our project page: https://nirvanalan.github.io/projects/stream3r. | [🔗 Paper](http://arxiv.org/abs/2508.10893v1) |
| [Empirical Investigation into Configuring Echo State Networks for
  Representative Benchmark Problem Domains](http://arxiv.org/abs/2508.10887v1) | Brooke R. Weborg, Gursel Serpen | 2025-08-14 | Optimization | This paper examines Echo State Network, a reservoir computer, performance using four different benchmark problems, then proposes heuristics or rules of thumb for configuring the architecture, as well as the selection of parameters and their values, which are applicable to problems within the same domain, to help serve to fill the experience gap needed by those entering this field of study. The influence of various parameter selections and their value adjustments, as well as architectural changes made to an Echo State Network, a powerful recurrent neural network configured as a reservoir computer, can be challenging to fully comprehend without experience in the field, and even some hyperparameter optimization algorithms may have difficulty adjusting parameter values without proper manual selections made first. Therefore, it is imperative to understand the effects of parameters and their value selection on Echo State Network architecture performance for a successful build. Thus, to address the requirement for an extensive background in Echo State Network architecture, as well as examine how Echo State Network performance is affected with respect to variations in architecture, design, and parameter selection and values, a series of benchmark tasks representing different problem domains, including time series prediction, pattern generation, chaotic system prediction, and time series classification, were modeled and experimented on to show the impact on the performance of Echo State Network. | [🔗 Paper](http://arxiv.org/abs/2508.10887v1) |
| [ToonComposer: Streamlining Cartoon Production with Generative
  Post-Keyframing](http://arxiv.org/abs/2508.10881v1) | Lingen Li, Guangzhi Wang, Zhaoyang Zhang, Yaowei Li, Xiaoyu Li, Qi Dou, Jinwei Gu, Tianfan Xue, Ying Shan | 2025-08-14 | Optimization, Training & Evaluation, Multimodal AI | Traditional cartoon and anime production involves keyframing, inbetweening, and colorization stages, which require intensive manual effort. Despite recent advances in AI, existing methods often handle these stages separately, leading to error accumulation and artifacts. For instance, inbetweening approaches struggle with large motions, while colorization methods require dense per-frame sketches. To address this, we introduce ToonComposer, a generative model that unifies inbetweening and colorization into a single post-keyframing stage. ToonComposer employs a sparse sketch injection mechanism to provide precise control using keyframe sketches. Additionally, it uses a cartoon adaptation method with the spatial low-rank adapter to tailor a modern video foundation model to the cartoon domain while keeping its temporal prior intact. Requiring as few as a single sketch and a colored reference frame, ToonComposer excels with sparse inputs, while also supporting multiple sketches at any temporal location for more precise motion control. This dual capability reduces manual workload and improves flexibility, empowering artists in real-world scenarios. To evaluate our model, we further created PKBench, a benchmark featuring human-drawn sketches that simulate real-world use cases. Our evaluation demonstrates that ToonComposer outperforms existing methods in visual quality, motion consistency, and production efficiency, offering a superior and more flexible solution for AI-assisted cartoon production. | [🔗 Paper](http://arxiv.org/abs/2508.10881v1) |
## 🔹 Scaling Laws

| 📄 Title | 🖊 Authors | 📅 Date | 🏷 Tags | 📜 Summary | 🔗 Link |
|---------|---------|---------|---------|---------|---------|
| [Conic Formulations of Transport Metrics for Unbalanced Measure Networks
  and Hypernetworks](http://arxiv.org/abs/2508.10888v1) | Mary Chriselda Antony Oliver, Emmanuel Hartman, Tom Needham | 2025-08-14 | Scaling Laws, Responsible AI, Model Evaluation | The Gromov-Wasserstein (GW) variant of optimal transport, designed to compare probability densities defined over distinct metric spaces, has emerged as an important tool for the analysis of data with complex structure, such as ensembles of point clouds or networks. To overcome certain limitations, such as the restriction to comparisons of measures of equal mass and sensitivity to outliers, several unbalanced or partial transport relaxations of the GW distance have been introduced in the recent literature. This paper is concerned with the Conic Gromov-Wasserstein (CGW) distance introduced by S\'{e}journ\'{e}, Vialard, and Peyr\'{e}. We provide a novel formulation in terms of semi-couplings, and extend the framework beyond the metric measure space setting, to compare more general network and hypernetwork structures. With this new formulation, we establish several fundamental properties of the CGW metric, including its scaling behavior under dilation, variational convergence in the limit of volume growth constraints, and comparison bounds with established optimal transport metrics. We further derive quantitative bounds that characterize the robustness of the CGW metric to perturbations in the underlying measures. The hypernetwork formulation of CGW admits a simple and provably convergent block coordinate ascent algorithm for its estimation, and we demonstrate the computational tractability and scalability of our approach through experiments on synthetic and real-world high-dimensional and structured datasets. | [🔗 Paper](http://arxiv.org/abs/2508.10888v1) |
## 🔹 Training & Evaluation

| 📄 Title | 🖊 Authors | 📅 Date | 🏷 Tags | 📜 Summary | 🔗 Link |
|---------|---------|---------|---------|---------|---------|
| [Human-in-Context: Unified Cross-Domain 3D Human Motion Modeling via
  In-Context Learning](http://arxiv.org/abs/2508.10897v1) | Mengyuan Liu, Xinshun Wang, Zhongbin Fang, Deheng Ye, Xia Li, Tao Tang, Songtao Wu, Xiangtai Li, Ming-Hsuan Yang | 2025-08-14 | Training & Evaluation | This paper aims to model 3D human motion across domains, where a single model is expected to handle multiple modalities, tasks, and datasets. Existing cross-domain models often rely on domain-specific components and multi-stage training, which limits their practicality and scalability. To overcome these challenges, we propose a new setting to train a unified cross-domain model through a single process, eliminating the need for domain-specific components and multi-stage training. We first introduce Pose-in-Context (PiC), which leverages in-context learning to create a pose-centric cross-domain model. While PiC generalizes across multiple pose-based tasks and datasets, it encounters difficulties with modality diversity, prompting strategy, and contextual dependency handling. We thus propose Human-in-Context (HiC), an extension of PiC that broadens generalization across modalities, tasks, and datasets. HiC combines pose and mesh representations within a unified framework, expands task coverage, and incorporates larger-scale datasets. Additionally, HiC introduces a max-min similarity prompt sampling strategy to enhance generalization across diverse domains and a network architecture with dual-branch context injection for improved handling of contextual dependencies. Extensive experimental results show that HiC performs better than PiC in terms of generalization, data scale, and performance across a wide range of domains. These results demonstrate the potential of HiC for building a unified cross-domain 3D human motion model with improved flexibility and scalability. The source codes and models are available at https://github.com/BradleyWang0416/Human-in-Context. | [🔗 Paper](http://arxiv.org/abs/2508.10897v1) |
## 🔹 Responsible AI

| 📄 Title | 🖊 Authors | 📅 Date | 🏷 Tags | 📜 Summary | 🔗 Link |
|---------|---------|---------|---------|---------|---------|
| [The Dark Energy Bedrock All-Sky Supernova Program: Motivation, Design,
  Implementation, and Preliminary Data Release](http://arxiv.org/abs/2508.10878v1) | Nora F. Sherman, Maria Acevedo, Dillon Brout, Bailey Martin, Daniel Scolnic, Dingyuan Cao, Christopher Lidman, Noor Ali, Patrick Armstrong, K. Auchett, Rebecca C. Chen, Alex Drlica-Wagner, Peter S. Ferguson, Kenneth Herner, Gautham Narayan, Erik R. Peterson, Liana Rauf, Armin Rest, Adam G. Riess, Masao Sako, Brian Schmidt, Xianzhe TZ Tang, Brad E. Tucker | 2025-08-14 | Responsible AI, Model Evaluation | Precise measurements of Type Ia supernovae (SNe Ia) at low redshifts ($z$) serve as one of the most viable keys to unlocking our understanding of cosmic expansion, isotropy, and growth of structure. The Dark Energy Bedrock All-Sky Supernovae (DEBASS) program will deliver the largest uniformly calibrated low-$z$ SN Ia data set in the southern hemisphere to date. DEBASS utilizes the Dark Energy Camera to image supernovae in conjunction with the Wide-Field Spectrograph (WiFeS) to gather comprehensive host galaxy information. By using the same photometric instrument as both the Dark Energy Survey (DES) and the DECam Local Volume Exploration Survey, DEBASS not only benefits from a robust photometric pipeline and well-calibrated images across the southern sky, but can replace the historic and external low-$z$ samples that were used in the final DES supernova analysis. DEBASS has accumulated more than 400 spectroscopically confirmed SNe Ia in the redshift range of $0.01<z<0.08$ from 2021 to mid-2025, and, in this paper along with a companion paper Acevedo et al. submitted, we present an early data release of 77 SNe within the DES footprint to demonstrate the merit and constraining power of the data set. Here, we introduce the DEBASS program, discuss its scientific goals and the advantages it offers for supernova cosmology, and present our initial results. With this early data release, we find a robust median absolute standard deviation of Hubble diagram residuals of $\sim$0.10 mag and an initial measurement of the host-galaxy mass step of $0.06\pm0.04$ mag, both before performing bias corrections. This low scatter shows the promise of a low-$z$ SN Ia program with a well-calibrated telescope and high signal-to-noise ratio across multiple bands. | [🔗 Paper](http://arxiv.org/abs/2508.10878v1) |
## 🔹 Fine-Tuning

| 📄 Title | 🖊 Authors | 📅 Date | 🏷 Tags | 📜 Summary | 🔗 Link |
|---------|---------|---------|---------|---------|---------|
| [MAESTRO: Masked AutoEncoders for Multimodal, Multitemporal, and
  Multispectral Earth Observation Data](http://arxiv.org/abs/2508.10894v1) | Antoine Labatie, Michael Vaccaro, Nina Lardiere, Anatol Garioud, Nicolas Gonthier | 2025-08-14 | Fine-Tuning, Multimodal AI | Self-supervised learning holds great promise for remote sensing, but standard self-supervised methods must be adapted to the unique characteristics of Earth observation data. We take a step in this direction by conducting a comprehensive benchmark of fusion strategies and reconstruction target normalization schemes for multimodal, multitemporal, and multispectral Earth observation data. Based on our findings, we propose MAESTRO, a novel adaptation of the Masked Autoencoder, featuring optimized fusion strategies and a tailored target normalization scheme that introduces a spectral prior as a self-supervisory signal. Evaluated on four Earth observation datasets, MAESTRO sets a new state-of-the-art on tasks that strongly rely on multitemporal dynamics, while remaining highly competitive on tasks dominated by a single mono-temporal modality. Code to reproduce all our experiments is available at https://github.com/ignf/maestro. | [🔗 Paper](http://arxiv.org/abs/2508.10894v1) |
## 🔹 General AI

| 📄 Title | 🖊 Authors | 📅 Date | 🏷 Tags | 📜 Summary | 🔗 Link |
|---------|---------|---------|---------|---------|---------|
| [Exceptional flat bands in bipartite non-Hermitian quantum crystals](http://arxiv.org/abs/2508.10901v1) | Juan Pablo Esparza, Vladimir Juricic | 2025-08-14 | General AI | Flat bands, in which kinetic energy is quenched and quantum states become macroscopically degenerate, host a rich variety of correlated and topological phases, from unconventional superconductors to fractional Chern insulators. In Hermitian lattices, their formation mechanisms are now well understood, but whether such states persist, and acquire new features in non-Hermitian (NH) quantum crystals, relevant to open and driven systems, has remained an open question. Here we show that the Hermitian principle for flat-band formation in bipartite lattices, based on a sublattice degeneracy mismatch, extends directly to the NH regime: whenever one sublattice hosts a momentum-independent eigenvalue with degeneracy exceeding that of its partner on the other sublattice, flat bands arise regardless of gain, loss, or complex couplings. Strikingly, at exceptional points, dispersive bands coalesce to form \emph{exceptional flat bands} that persist beyond these singularities, exhibiting biorthogonal eigenmodes spanning both sublattices, with energies and lifetimes tunable via sublattice asymmetry and non-reciprocal couplings. This general framework unifies Hermitian and NH flat-band constructions, and reveals dispersionless states with no closed-system analogue. The proposed construction is applicable to synthetic platforms, from classical metamaterials, where flat bands can be directly emulated, to quantum-engineered systems such as photonic crystals and ultracold atom arrays, which should host correlated and topological phases emerging from such exceptional flat bands. | [🔗 Paper](http://arxiv.org/abs/2508.10901v1) |
| [Web-Halo Model (WHM): Accurate non-linear matter power spectrum
  predictions without free parameters](http://arxiv.org/abs/2508.10902v1) | Samuel Brieden, Florian Beutler, Marcos Pellejero-Ibañez | 2025-08-14 | General AI | We present a parameter-free variant of the halo model that significantly improves the precision of matter clustering predictions, particularly in the challenging 1-halo to 2-halo transition regime, where standard halo models often fail. Unlike HMcode-2020, which relies on 12 phenomenological parameters, our approach achieves comparable or superior accuracy without any free fitting parameters. This new web-halo model (WHM) extends the traditional halo model by incorporating structures that have collapsed along two dimensions (filaments) and one dimension (sheets), in addition to haloes, and combines these with 1-loop Lagrangian Perturbation Theory (1$\ell$-LPT) in a consistent framework. We show that WHM matches N-body simulation power spectra within the precision of state-of-the-art emulators at the 2-halo to 1-halo transition regime at all redshifts. Specifically, the WHM achieves better than 2\% accuracy up to scales of $k = 0.4\, h\,\mathrm{Mpc}^{-1}$, $0.7\, h\,\mathrm{Mpc}^{-1}$, and $1.3\, h\,\mathrm{Mpc}^{-1}$ at redshifts $z = 0.0$, $0.8$, and $1.5$, respectively, for both the baccoemu and EuclidEmu2 emulators, across their full $w_0w_a\mathrm{CDM} + \sum m_\nu$ cosmological parameter space. This marks a substantial improvement over 1$\ell$-LPT and HMcode-2020, the latter of which performs similarly at low redshift but deteriorates at higher redshifts despite its 12 tuned parameters. We publicly release WHM as WHMcode, integrated into existing HMcode implementations for CAMB and CLASS. | [🔗 Paper](http://arxiv.org/abs/2508.10902v1) |
| [Quantum Visual Fields with Neural Amplitude Encoding](http://arxiv.org/abs/2508.10900v1) | Shuteng Wang, Christian Theobalt, Vladislav Golyanik | 2025-08-14 | General AI | Quantum Implicit Neural Representations (QINRs) include components for learning and execution on gate-based quantum computers. While QINRs recently emerged as a promising new paradigm, many challenges concerning their architecture and ansatz design, the utility of quantum-mechanical properties, training efficiency and the interplay with classical modules remain. This paper advances the field by introducing a new type of QINR for 2D image and 3D geometric field learning, which we collectively refer to as Quantum Visual Field (QVF). QVF encodes classical data into quantum statevectors using neural amplitude encoding grounded in a learnable energy manifold, ensuring meaningful Hilbert space embeddings. Our ansatz follows a fully entangled design of learnable parametrised quantum circuits, with quantum (unitary) operations performed in the real Hilbert space, resulting in numerically stable training with fast convergence. QVF does not rely on classical post-processing -- in contrast to the previous QINR learning approach -- and directly employs projective measurement to extract learned signals encoded in the ansatz. Experiments on a quantum hardware simulator demonstrate that QVF outperforms the existing quantum approach and widely used classical foundational baselines in terms of visual representation accuracy across various metrics and model characteristics, such as learning of high-frequency details. We also show applications of QVF in 2D and 3D field completion and 3D shape interpolation, highlighting its practical potential. | [🔗 Paper](http://arxiv.org/abs/2508.10900v1) |
| [Stars Born in the Wind: M82's Outflow and Halo Star Formation](http://arxiv.org/abs/2508.10895v1) | Vaishnav V. Rao, Adam Smercina, Eric F. Bell, Benjamin Williams, Julianne J. Dalcanton, Andrew Dolphin, Adam Leroy, Antonela Monachesi, Jeremy Bailin, Roelof S. de Jong, Fabian Walter | 2025-08-14 | General AI | Starburst galaxies, like M82, launch kiloparsec-scale galactic outflows that interact with the circumgalactic medium (CGM) in complex ways. Apart from enriching the CGM with metals and energy, these outflows may trigger star formation in the halo -- either by driving shocks into the CGM or transporting cold, star-forming gas. To investigate such processes, we analyze the star formation history (SFH) of the Southern Arcs -- arc-like stellar features located ~5 kpc from M82's star-forming disk along the minor axis -- using Hubble Space Telescope Wide Field Camera 3 photometry. From resolved stellar populations, we derive SFHs over the last ~500 Myr, finding that ~85% of the stellar mass formed between ~150 and ~70 Myr ago, followed by a brief pause, with the remaining ~15% forming since ~30 Myr ago. The two stellar populations are co-spatial on scales of at least ~200 pc. The timing of the ~100 Myr burst aligns with star formation in the M82 disk and the age distribution of its star clusters, suggesting a causal link between the disk starburst and halo star formation. We explore two mechanisms that could explain these observations. In the first, shocks driven by the interaction between hot outflowing gas and cooler CGM material compress dense clouds, triggering collapse and star formation. In the second, stars form directly within massive, cool clouds associated with the outflow. As these clouds move ballistically through the halo, subsequent interactions with tidal debris may trigger additional star formation, producing the observed episodic structure. | [🔗 Paper](http://arxiv.org/abs/2508.10895v1) |
| [Upper bound on heat kernels of finite particle systems of Keller-Segel
  type](http://arxiv.org/abs/2508.10892v1) | S. E. Boutiah, D. Kinzebulatov | 2025-08-14 | General AI | We obtain an upper bound on the heat kernel of the Keller-Segel finite particle system that exhibits blow up effects. The proof exploits a connection between Keller-Segel finite particles and certain non-local operators. The latter allows to address some aspects of the critical behaviour of the Keller-Segel system resulting from its two-dimensionality. | [🔗 Paper](http://arxiv.org/abs/2508.10892v1) |
| [Fuel Consumption in Platoons: A Literature Review](http://arxiv.org/abs/2508.10891v1) | Oumaima Barhoumi, Ghazal Farhani, Taufiq Rahman, Mohamed H. Zaki, Sofiène Tahar, Fadi Araji | 2025-08-14 | General AI | Platooning has emerged as a promising strategy for improving fuel efficiency in automated vehicle systems, with significant implications for reducing emissions and operational costs. While existing literature on vehicle platooning primarily focuses on individual aspects such as aerodynamic drag reduction or specific control strategies, this work takes a more comprehensive approach by bringing together a wide range of factors and components that contribute to fuel savings in platoons. In this literature review, we examine the impact of platooning on fuel consumption, highlighting the key components of platoon systems, the factors and actors influencing fuel savings, methods for estimating fuel use, and the effect of platoon instability on efficiency. Furthermore, we study the role of reduced aerodynamic drag, vehicle coordination, and the challenges posed by instability in real-world conditions. By compiling insights from recent studies, this work provides a comprehensive overview of the latest advancements in platooning technologies and highlights both the challenges and opportunities for future research to maximize fuel savings in real-world scenarios. | [🔗 Paper](http://arxiv.org/abs/2508.10891v1) |
| [Random Permutation Circuits are Quantum Chaotic](http://arxiv.org/abs/2508.10890v1) | Bruno Bertini, Katja Klobas, Pavel Kos, Daniel Malz | 2025-08-14 | General AI | Random permutation circuits were recently introduced as minimal models for local many-body dynamics that can be interpreted both as classical and quantum. Standard indicators of chaos such as damage spreading, show that these systems exhibit sensitivity to initial conditions in the classical setting. Here, we address their quantum chaoticity by studying the time evolution of local operator entanglement (LOE). We show that the behaviour of LOE in random permutation circuits depends on the dimension of the local configuration space q. When q = 2, i.e. the circuits act on qubits, random permutations are Clifford and the LOE of any local operator is bounded by a constant, indicating that they are not truly chaotic. On the other hand, when the dimension of the local configuration space exceeds two, the LOE grows linearly in time. We prove this in the limit of large dimensions and present numerical evidence that a three-dimensional local configuration space is sufficient for a linear growth of LOE. Our findings highlight that quantum chaos can be produced by essentially classical dynamics. Moreover, we show that LOE can be defined also in the classical realm and put it forward as a universal indicator chaos, both quantum and classical. | [🔗 Paper](http://arxiv.org/abs/2508.10890v1) |
| [Discovery of Niobium Hydride Precipitates in Superconducting Qubits](http://arxiv.org/abs/2508.10889v1) | Zuhawn Sung, Daniel Bafia, Arely Cano, Akshay Murthy, Jaeyel Lee, Matthew J Reagor, Juan Rubio-Zuazo, Anna Grassellino, Alexander Romanenko | 2025-08-14 | General AI | We report the evidence of the formation of niobium hydride phase within niobium films on silicon substrates in superconducting qubits fabricated at Rigetti Computing. For this study, we combined complementary techniques, including room-temperature and cryogenic atomic force microscopy (AFM), synchrotron Xray diffraction, and time of flight secondary ion mass spectroscopy (ToF-SIMS), to directly reveal the existence of niobium hydride precipitates in the Rigetti chip area. Upon cryogenic cooling, we observed variation in the size and morphology of the hydrides, ranging from small (5 nm) irregular shapes to large (~10-100 nm) domain within the Nb grains, fully converted to niobium hydrides. Since niobium hydrides are non-superconducting and can easily change in size and location upon different cooldowns to cryogenic temperature, our finding highlights a new and previously unknown source of decoherence in superconducting qubits. This contributes to both quasiparticle and two level system (TLS) losses, offering a potential explanation for changes in qubit performance upon cooldowns. Finally, by leveraging the RF performance of a 3D bulk Nb resonator, we can quantify RF dissipation on a superconducting qubit, caused by hydrogen concentration variation, and are able to propose a practical engineering pathway to mitigate the formation of the Nb hydrides for superconducting qubit applications. | [🔗 Paper](http://arxiv.org/abs/2508.10889v1) |
| [Large implies henselian](http://arxiv.org/abs/2508.10886v1) | Will Johnson, Chieu-Minh Tran, Erik Walsberg, Jinhe Ye | 2025-08-14 | General AI | Fix a field $K$. We show that $K$ is large if and only if some elementary extension of $K$ is the fraction field of a henselian local domain which is not a field. The proof uses a new result about the \'etale-open topology over $K$: if $K$ is not separably closed and $V \to W$ is an \'etale morphism of $K$-varieties then $V(K) \to W(K)$ is a local homeomorphism in the \'etale-open topology. This, in turn, follows from results comparing the \'etale-open topology on $V(K)$ and the finite-closed topology on $V(K)$, newly introduced in this paper. We show that the \'etale-open topology refines the finite-closed topology when $K$ is perfect, and that the finite-closed topology refines the \'etale-open topology when $K$ is bounded. It follows that these two topologies agree in many natural examples. On the other hand, we construct several examples where these two differ, which allows us to answer a question of Lampe. | [🔗 Paper](http://arxiv.org/abs/2508.10886v1) |
| [New Materials, New Functionalities: Molecular Beam Epitaxy of Ultra-High
  Conductivity Oxides](http://arxiv.org/abs/2508.10885v1) | Gaurab Rimal, Tanzila Tasnim, Brian Opatosky, Ryan B. Comes, Debarghya Mallick, Simon Kim, Rob G. Moore, Seongshik Oh, Matthew Brahlek | 2025-08-14 | General AI | Understanding fundamental properties of materials is necessary for all modern electronic technologies. Toward this end, the fabrication of new ultrapure thin film materials is critical to discover and understand novel properties that can allow further development of technology. Oxide materials are a vast material class abound with diverse properties, and, therefore, harnessing such phases is critical for realizing emerging technologies. Pushing forward, however, requires understanding basic properties of insulating, semiconducting and metallic oxides, as well as the more complex phases that arise out of strong electronic correlations unique to this class of materials. In this review, we will focus on one of the unique aspects of oxides: the ultra-high conductivity metallic state, which can be a critical component for future all-oxide microelectronics such as low-loss interconnects and gate-metals, spintronics, as well as future quantum technologies that employ emergent magnetic or superconducting ground states. Like most oxides, a critical challenge to understanding and ultimately integrating high-conductivity metals into new technologies is the ability to synthesize high-quality materials. Therefore, we will frame the discussions in the context of epitaxial film growth via molecular beam epitaxy (MBE), which has provided insights into the electronic behavior of specific materials while providing samples with unprecedented quality. We will highlight and underscore how MBE has enabled developments and deeper understanding of their properties and how it plays a critical role in the future of this unique class of materials. | [🔗 Paper](http://arxiv.org/abs/2508.10885v1) |
| [Nash maps over large fields](http://arxiv.org/abs/2508.10884v1) | Erik Walsberg | 2025-08-14 | General AI | In this short note we give some corollaries of the polynomial inverse function theorem for large fields. We prove inverse and implicit function theorems for Nash maps over large fields, characterize large fields as fields satisfying inverse or implicit function theorems, give inverse and implicit function theorems for gt-henselian field topologies, and show that definable functions in various logically tame fields of characteristic zero are generically Nash. We prove a version of Krasner's lemma for large fields and describe how Nash functions give a natural proof of the well-known fact that a large field $K$ is existentially closed in $K(\!(t)\!)$. | [🔗 Paper](http://arxiv.org/abs/2508.10884v1) |
| [Bicharacter twists of quantum groups](http://arxiv.org/abs/2508.10882v1) | Ian Martin, Alexander Tsymbaliuk | 2025-08-14 | General AI | We apply the general construction of a twist of bigraded Hopf algebras by skew bicharacters to obtain two-parameter quantum groups in the Drinfeld-Jimbo, new Drinfeld (for affine types), and FRT (for both finite and affine) presentations from their standard one-parameter versions. This yields new elementary proofs of the fundamental results on two-parameter quantum groups that appeared in the literature over the last two decades, and also leads to natural generalizations in the super and multiparameter cases. | [🔗 Paper](http://arxiv.org/abs/2508.10882v1) |
| [Searching for Privacy Risks in LLM Agents via Simulation](http://arxiv.org/abs/2508.10880v1) | Yanzhe Zhang, Diyi Yang | 2025-08-14 | General AI | The widespread deployment of LLM-based agents is likely to introduce a critical privacy threat: malicious agents that proactively engage others in multi-turn interactions to extract sensitive information. These dynamic dialogues enable adaptive attack strategies that can cause severe privacy violations, yet their evolving nature makes it difficult to anticipate and discover sophisticated vulnerabilities manually. To tackle this problem, we present a search-based framework that alternates between improving attacker and defender instructions by simulating privacy-critical agent interactions. Each simulation involves three roles: data subject, data sender, and data recipient. While the data subject's behavior is fixed, the attacker (data recipient) attempts to extract sensitive information from the defender (data sender) through persistent and interactive exchanges. To explore this interaction space efficiently, our search algorithm employs LLMs as optimizers, using parallel search with multiple threads and cross-thread propagation to analyze simulation trajectories and iteratively propose new instructions. Through this process, we find that attack strategies escalate from simple direct requests to sophisticated multi-turn tactics such as impersonation and consent forgery, while defenses advance from rule-based constraints to identity-verification state machines. The discovered attacks and defenses transfer across diverse scenarios and backbone models, demonstrating strong practical utility for building privacy-aware agents. | [🔗 Paper](http://arxiv.org/abs/2508.10880v1) |
| [An Iterative Algorithm for Differentially Private $k$-PCA with Adaptive
  Noise](http://arxiv.org/abs/2508.10879v1) | Johanna Düngler, Amartya Sanyal | 2025-08-14 | General AI | Given $n$ i.i.d. random matrices $A_i \in \mathbb{R}^{d \times d}$ that share a common expectation $\Sigma$, the objective of Differentially Private Stochastic PCA is to identify a subspace of dimension $k$ that captures the largest variance directions of $\Sigma$, while preserving differential privacy (DP) of each individual $A_i$. Existing methods either (i) require the sample size $n$ to scale super-linearly with dimension $d$, even under Gaussian assumptions on the $A_i$, or (ii) introduce excessive noise for DP even when the intrinsic randomness within $A_i$ is small. Liu et al. (2022a) addressed these issues for sub-Gaussian data but only for estimating the top eigenvector ($k=1$) using their algorithm DP-PCA. We propose the first algorithm capable of estimating the top $k$ eigenvectors for arbitrary $k \leq d$, whilst overcoming both limitations above. For $k=1$ our algorithm matches the utility guarantees of DP-PCA, achieving near-optimal statistical error even when $n = \tilde{\!O}(d)$. We further provide a lower bound for general $k > 1$, matching our upper bound up to a factor of $k$, and experimentally demonstrate the advantages of our algorithm over comparable baselines. | [🔗 Paper](http://arxiv.org/abs/2508.10879v1) |
