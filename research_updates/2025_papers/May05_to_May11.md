# 📌 AI Research Papers (May05 to May11)

## 🔹 LLM

| 📄 Title | 🖊 Authors | 📅 Date | 🏷 Tags | 📜 Summary | 🔗 Link |
|---------|---------|---------|---------|---------|---------|
| [Generating Physically Stable and Buildable LEGO Designs from Text](http://arxiv.org/abs/2505.05469v1) | Ava Pun, Kangle Deng, Ruixuan Liu, Deva Ramanan, Changliu Liu, Jun-Yan Zhu | 2025-05-08 | LLM | We introduce LegoGPT, the first approach for generating physically stable LEGO brick models from text prompts. To achieve this, we construct a large-scale, physically stable dataset of LEGO designs, along with their associated captions, and train an autoregressive large language model to predict the next brick to add via next-token prediction. To improve the stability of the resulting designs, we employ an efficient validity check and physics-aware rollback during autoregressive inference, which prunes infeasible token predictions using physics laws and assembly constraints. Our experiments show that LegoGPT produces stable, diverse, and aesthetically pleasing LEGO designs that align closely with the input text prompts. We also develop a text-based LEGO texturing method to generate colored and textured designs. We show that our designs can be assembled manually by humans and automatically by robotic arms. We also release our new dataset, StableText2Lego, containing over 47,000 LEGO structures of over 28,000 unique 3D objects accompanied by detailed captions, along with our code and models at the project website: https://avalovelace1.github.io/LegoGPT/. | [🔗 Paper](http://arxiv.org/abs/2505.05469v1) |
## 🔹 RLHF

| 📄 Title | 🖊 Authors | 📅 Date | 🏷 Tags | 📜 Summary | 🔗 Link |
|---------|---------|---------|---------|---------|---------|
| [Flow-GRPO: Training Flow Matching Models via Online RL](http://arxiv.org/abs/2505.05470v1) | Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, Wanli Ouyang | 2025-05-08 | RLHF, Diffusion Models | We propose Flow-GRPO, the first method integrating online reinforcement learning (RL) into flow matching models. Our approach uses two key strategies: (1) an ODE-to-SDE conversion that transforms a deterministic Ordinary Differential Equation (ODE) into an equivalent Stochastic Differential Equation (SDE) that matches the original model's marginal distribution at all timesteps, enabling statistical sampling for RL exploration; and (2) a Denoising Reduction strategy that reduces training denoising steps while retaining the original inference timestep number, significantly improving sampling efficiency without performance degradation. Empirically, Flow-GRPO is effective across multiple text-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly perfect object counts, spatial relations, and fine-grained attributes, boosting GenEval accuracy from $63\%$ to $95\%$. In visual text rendering, its accuracy improves from $59\%$ to $92\%$, significantly enhancing text generation. Flow-GRPO also achieves substantial gains in human preference alignment. Notably, little to no reward hacking occurred, meaning rewards did not increase at the cost of image quality or diversity, and both remained stable in our experiments. | [🔗 Paper](http://arxiv.org/abs/2505.05470v1) |
## 🔹 Multimodal AI

| 📄 Title | 🖊 Authors | 📅 Date | 🏷 Tags | 📜 Summary | 🔗 Link |
|---------|---------|---------|---------|---------|---------|
| [SVAD: From Single Image to 3D Avatar via Synthetic Data Generation with
  Video Diffusion and Data Augmentation](http://arxiv.org/abs/2505.05475v1) | Yonwoo Choi | 2025-05-08 | Multimodal AI, Diffusion Models | Creating high-quality animatable 3D human avatars from a single image remains a significant challenge in computer vision due to the inherent difficulty of reconstructing complete 3D information from a single viewpoint. Current approaches face a clear limitation: 3D Gaussian Splatting (3DGS) methods produce high-quality results but require multiple views or video sequences, while video diffusion models can generate animations from single images but struggle with consistency and identity preservation. We present SVAD, a novel approach that addresses these limitations by leveraging complementary strengths of existing techniques. Our method generates synthetic training data through video diffusion, enhances it with identity preservation and image restoration modules, and utilizes this refined data to train 3DGS avatars. Comprehensive evaluations demonstrate that SVAD outperforms state-of-the-art (SOTA) single-image methods in maintaining identity consistency and fine details across novel poses and viewpoints, while enabling real-time rendering capabilities. Through our data augmentation pipeline, we overcome the dependency on dense monocular or multi-view training data typically required by traditional 3DGS approaches. Extensive quantitative, qualitative comparisons show our method achieves superior performance across multiple metrics against baseline models. By effectively combining the generative power of diffusion models with both the high-quality results and rendering efficiency of 3DGS, our work establishes a new approach for high-fidelity avatar generation from a single image input. | [🔗 Paper](http://arxiv.org/abs/2505.05475v1) |
| [StreamBridge: Turning Your Offline Video Large Language Model into a
  Proactive Streaming Assistant](http://arxiv.org/abs/2505.05467v1) | Haibo Wang, Bo Feng, Zhengfeng Lai, Mingze Xu, Shiyu Li, Weifeng Ge, Afshin Dehghan, Meng Cao, Ping Huang | 2025-05-08 | Multimodal AI, LLM | We present StreamBridge, a simple yet effective framework that seamlessly transforms offline Video-LLMs into streaming-capable models. It addresses two fundamental challenges in adapting existing models into online scenarios: (1) limited capability for multi-turn real-time understanding, and (2) lack of proactive response mechanisms. Specifically, StreamBridge incorporates (1) a memory buffer combined with a round-decayed compression strategy, supporting long-context multi-turn interactions, and (2) a decoupled, lightweight activation model that can be effortlessly integrated into existing Video-LLMs, enabling continuous proactive responses. To further support StreamBridge, we construct Stream-IT, a large-scale dataset tailored for streaming video understanding, featuring interleaved video-text sequences and diverse instruction formats. Extensive experiments show that StreamBridge significantly improves the streaming understanding capabilities of offline Video-LLMs across various tasks, outperforming even proprietary models such as GPT-4o and Gemini 1.5 Pro. Simultaneously, it achieves competitive or superior performance on standard video understanding benchmarks. | [🔗 Paper](http://arxiv.org/abs/2505.05467v1) |
## 🔹 Optimization

| 📄 Title | 🖊 Authors | 📅 Date | 🏷 Tags | 📜 Summary | 🔗 Link |
|---------|---------|---------|---------|---------|---------|
| [DiffusionSfM: Predicting Structure and Motion via Ray Origin and
  Endpoint Diffusion](http://arxiv.org/abs/2505.05473v1) | Qitao Zhao, Amy Lin, Jeff Tan, Jason Y. Zhang, Deva Ramanan, Shubham Tulsiani | 2025-05-08 | Optimization, LLM, Diffusion Models | Current Structure-from-Motion (SfM) methods typically follow a two-stage pipeline, combining learned or geometric pairwise reasoning with a subsequent global optimization step. In contrast, we propose a data-driven multi-view reasoning approach that directly infers 3D scene geometry and camera poses from multi-view images. Our framework, DiffusionSfM, parameterizes scene geometry and cameras as pixel-wise ray origins and endpoints in a global frame and employs a transformer-based denoising diffusion model to predict them from multi-view inputs. To address practical challenges in training diffusion models with missing data and unbounded scene coordinates, we introduce specialized mechanisms that ensure robust learning. We empirically validate DiffusionSfM on both synthetic and real datasets, demonstrating that it outperforms classical and learning-based approaches while naturally modeling uncertainty. | [🔗 Paper](http://arxiv.org/abs/2505.05473v1) |
## 🔹 Training & Evaluation

| 📄 Title | 🖊 Authors | 📅 Date | 🏷 Tags | 📜 Summary | 🔗 Link |
|---------|---------|---------|---------|---------|---------|
| [3D Scene Generation: A Survey](http://arxiv.org/abs/2505.05474v1) | Beichen Wen, Haozhe Xie, Zhaoxi Chen, Fangzhou Hong, Ziwei Liu | 2025-05-08 | Training & Evaluation, Multimodal AI, Diffusion Models | 3D scene generation seeks to synthesize spatially structured, semantically meaningful, and photorealistic environments for applications such as immersive media, robotics, autonomous driving, and embodied AI. Early methods based on procedural rules offered scalability but limited diversity. Recent advances in deep generative models (e.g., GANs, diffusion models) and 3D representations (e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene distributions, improving fidelity, diversity, and view consistency. Recent advances like diffusion models bridge 3D scene synthesis and photorealism by reframing generation as image or video synthesis problems. This survey provides a systematic overview of state-of-the-art approaches, organizing them into four paradigms: procedural generation, neural 3D-based generation, image-based generation, and video-based generation. We analyze their technical foundations, trade-offs, and representative results, and review commonly used datasets, evaluation protocols, and downstream applications. We conclude by discussing key challenges in generation capacity, 3D representation, data and annotations, and evaluation, and outline promising directions including higher fidelity, physics-aware and interactive generation, and unified perception-generation models. This review organizes recent advances in 3D scene generation and highlights promising directions at the intersection of generative AI, 3D vision, and embodied intelligence. To track ongoing developments, we maintain an up-to-date project page: https://github.com/hzxie/Awesome-3D-Scene-Generation. | [🔗 Paper](http://arxiv.org/abs/2505.05474v1) |
## 🔹 Prompt Engineering

| 📄 Title | 🖊 Authors | 📅 Date | 🏷 Tags | 📜 Summary | 🔗 Link |
|---------|---------|---------|---------|---------|---------|
| [Mogao: An Omni Foundation Model for Interleaved Multi-Modal Generation](http://arxiv.org/abs/2505.05472v1) | Chao Liao, Liyang Liu, Xun Wang, Zhengxiong Luo, Xinyu Zhang, Wenliang Zhao, Jie Wu, Liang Li, Zhi Tian, Weilin Huang | 2025-05-08 | Prompt Engineering, Scaling Laws, Diffusion Models | Recent progress in unified models for image understanding and generation has been impressive, yet most approaches remain limited to single-modal generation conditioned on multiple modalities. In this paper, we present Mogao, a unified framework that advances this paradigm by enabling interleaved multi-modal generation through a causal approach. Mogao integrates a set of key technical improvements in architecture design, including a deep-fusion design, dual vision encoders, interleaved rotary position embeddings, and multi-modal classifier-free guidance, which allow it to harness the strengths of both autoregressive models for text generation and diffusion models for high-quality image synthesis. These practical improvements also make Mogao particularly effective to process interleaved sequences of text and images arbitrarily. To further unlock the potential of unified models, we introduce an efficient training strategy on a large-scale, in-house dataset specifically curated for joint text and image generation. Extensive experiments show that Mogao not only achieves state-of-the-art performance in multi-modal understanding and text-to-image generation, but also excels in producing high-quality, coherent interleaved outputs. Its emergent capabilities in zero-shot image editing and compositional generation highlight Mogao as a practical omni-modal foundation model, paving the way for future development and scaling the unified multi-modal systems. | [🔗 Paper](http://arxiv.org/abs/2505.05472v1) |
## 🔹 Responsible AI

| 📄 Title | 🖊 Authors | 📅 Date | 🏷 Tags | 📜 Summary | 🔗 Link |
|---------|---------|---------|---------|---------|---------|
| [Facets of Disparate Impact: Evaluating Legally Consistent Bias in
  Machine Learning](http://arxiv.org/abs/2505.05471v1) | Jarren Briscoe, Assefaw Gebremedhin | 2025-05-08 | Responsible AI, Model Evaluation | Leveraging current legal standards, we define bias through the lens of marginal benefits and objective testing with the novel metric "Objective Fairness Index". This index combines the contextual nuances of objective testing with metric stability, providing a legally consistent and reliable measure. Utilizing the Objective Fairness Index, we provide fresh insights into sensitive machine learning applications, such as COMPAS (recidivism prediction), highlighting the metric's practical and theoretical significance. The Objective Fairness Index allows one to differentiate between discriminatory tests and systemic disparities. | [🔗 Paper](http://arxiv.org/abs/2505.05471v1) |
## 🔹 General AI

| 📄 Title | 🖊 Authors | 📅 Date | 🏷 Tags | 📜 Summary | 🔗 Link |
|---------|---------|---------|---------|---------|---------|
| [A Solovay-Kitaev theorem for quantum signal processing](http://arxiv.org/abs/2505.05468v1) | Zane M. Rossi | 2025-05-08 | General AI | Quantum signal processing (QSP) studies quantum circuits interleaving known unitaries (the phases) and unknown unitaries encoding a hidden scalar (the signal). For a wide class of functions one can quickly compute the phases applying a desired function to the signal; surprisingly, this ability can be shown to unify many quantum algorithms. A separate, basic subfield in quantum computing is gate approximation: among its results, the Solovay-Kitaev theorem (SKT) establishes an equivalence between the universality of a gate set and its ability to efficiently approximate other gates.   In this work we prove an 'SKT for QSP,' showing that the density of parameterized circuit ans\"atze in classes of functions implies the existence of short circuits approximating desired functions. This is quite distinct from a pointwise application of the usual SKT, and yields a suite of independently interesting 'lifted' variants of standard SKT proof techniques. Our method furnishes alternative, flexible proofs for results in QSP, extends simply to ans\"atze for which standard QSP proof methods fail, and establishes a formal intersection between QSP and gate approximation. | [🔗 Paper](http://arxiv.org/abs/2505.05468v1) |
| [Comparison of integral equations used to study $T_{cc}^+$](http://arxiv.org/abs/2505.05466v1) | Sebastian M. Dawid, Fernando Romero-López, Stephen R. Sharpe | 2025-05-08 | General AI | We perform a detailed comparison between three formalisms used in recent studies of $DD^*$ scattering, which aim to understand the properties of the doubly-charmed tetraquark, $T_{cc}^+(3875)$. These methods are the three-particle relativistic field theory (RFT) formalism, the two-body Lippmann-Schwinger (LS) equation with chiral effective field theory potentials, and the two-particle relativistic framework proposed by Baiao Raposo and Hansen (BRH approach). In a simplified single-channel setting, we derive the conditions under which the infinite-volume integral equations from the RFT and BRH approaches reduce to the LS form. We present numerical examples showing that differences between these methods can be largely removed by adjusting short-range couplings. We also address a number of technical issues in the RFT approach. | [🔗 Paper](http://arxiv.org/abs/2505.05466v1) |
