# ğŸ“Œ February 2025 AI Research Papers

## ğŸ”¹ LLM

| ğŸ“„ Title | ğŸ–Š Authors | ğŸ“… Date | ğŸ· Tags | ğŸ“œ Summary | ğŸ“ˆ Citations | ğŸ”— Link |
|---------|---------|---------|---------|---------|---------|---------|
| [Fillerbuster: Multi-View Scene Completion for Casual Captures](http://arxiv.org/abs/2502.05175v1) | Ethan Weber, Norman MÃ¼ller, Yash Kant, Vasu Agrawal, Michael ZollhÃ¶fer, Angjoo Kanazawa, Christian Richardt | 2025-02-07 | LLM, Diffusion Models | We present Fillerbuster, a method that completes unknown regions of a 3D scene by utilizing a novel large-scale multi-view latent diffusion transformer. Casual captures are often sparse and miss surrounding content behind objects or above the scene. Existing methods are not suitable for handling this challenge as they focus on making the known pixels look good with sparse-view priors, or on creating the missing sides of objects from just one or two photos. In reality, we often have hundreds of input frames and want to complete areas that are missing and unobserved from the input frames. Additionally, the images often do not have known camera parameters. Our solution is to train a generative model that can consume a large context of input frames while generating unknown target views and recovering image poses when desired. We show results where we complete partial captures on two existing datasets. We also present an uncalibrated scene completion task where our unified model predicts both poses and creates new content. Our model is the first to predict many images and poses together for scene completion. | 0 | [ğŸ”— Paper](http://arxiv.org/abs/2502.05175v1) |
## ğŸ”¹ Diffusion Models

| ğŸ“„ Title | ğŸ–Š Authors | ğŸ“… Date | ğŸ· Tags | ğŸ“œ Summary | ğŸ“ˆ Citations | ğŸ”— Link |
|---------|---------|---------|---------|---------|---------|---------|
| [FlashVideo:Flowing Fidelity to Detail for Efficient High-Resolution
  Video Generation](http://arxiv.org/abs/2502.05179v1) | Shilong Zhang, Wenbo Li, Shoufa Chen, Chongjian Ge, Peize Sun, Yida Zhang, Yi Jiang, Zehuan Yuan, Binyue Peng, Ping Luo | 2025-02-07 | Diffusion Models | DiT diffusion models have achieved great success in text-to-video generation, leveraging their scalability in model capacity and data scale. High content and motion fidelity aligned with text prompts, however, often require large model parameters and a substantial number of function evaluations (NFEs). Realistic and visually appealing details are typically reflected in high resolution outputs, further amplifying computational demands especially for single stage DiT models. To address these challenges, we propose a novel two stage framework, FlashVideo, which strategically allocates model capacity and NFEs across stages to balance generation fidelity and quality. In the first stage, prompt fidelity is prioritized through a low resolution generation process utilizing large parameters and sufficient NFEs to enhance computational efficiency. The second stage establishes flow matching between low and high resolutions, effectively generating fine details with minimal NFEs. Quantitative and visual results demonstrate that FlashVideo achieves state-of-the-art high resolution video generation with superior computational efficiency. Additionally, the two-stage design enables users to preview the initial output before committing to full resolution generation, thereby significantly reducing computational costs and wait times as well as enhancing commercial viability . | 0 | [ğŸ”— Paper](http://arxiv.org/abs/2502.05179v1) |
| [AuraFusion360: Augmented Unseen Region Alignment for Reference-based
  360Â° Unbounded Scene Inpainting](http://arxiv.org/abs/2502.05176v1) | Chung-Ho Wu, Yang-Jung Chen, Ying-Huan Chen, Jie-Ying Lee, Bo-Hsu Ke, Chun-Wei Tuan Mu, Yi-Chuan Huang, Chin-Yang Lin, Min-Hung Chen, Yen-Yu Lin, Yu-Lun Liu | 2025-02-07 | Diffusion Models | Three-dimensional scene inpainting is crucial for applications from virtual reality to architectural visualization, yet existing methods struggle with view consistency and geometric accuracy in 360{\deg} unbounded scenes. We present AuraFusion360, a novel reference-based method that enables high-quality object removal and hole filling in 3D scenes represented by Gaussian Splatting. Our approach introduces (1) depth-aware unseen mask generation for accurate occlusion identification, (2) Adaptive Guided Depth Diffusion, a zero-shot method for accurate initial point placement without requiring additional training, and (3) SDEdit-based detail enhancement for multi-view coherence. We also introduce 360-USID, the first comprehensive dataset for 360{\deg} unbounded scene inpainting with ground truth. Extensive experiments demonstrate that AuraFusion360 significantly outperforms existing methods, achieving superior perceptual quality while maintaining geometric accuracy across dramatic viewpoint changes. See our project page for video results and the dataset at https://kkennethwu.github.io/aurafusion360/. | 0 | [ğŸ”— Paper](http://arxiv.org/abs/2502.05176v1) |
## ğŸ”¹ Optimization

| ğŸ“„ Title | ğŸ–Š Authors | ğŸ“… Date | ğŸ· Tags | ğŸ“œ Summary | ğŸ“ˆ Citations | ğŸ”— Link |
|---------|---------|---------|---------|---------|---------|---------|
| [QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive
  Multimodal Understanding and Generation](http://arxiv.org/abs/2502.05178v1) | Yue Zhao, Fuzhao Xue, Scott Reed, Linxi Fan, Yuke Zhu, Jan Kautz, Zhiding Yu, Philipp KrÃ¤henbÃ¼hl, De-An Huang | 2025-02-07 | Optimization, Multimodal AI | We introduce Quantized Language-Image Pretraining (QLIP), a visual tokenization method that combines state-of-the-art reconstruction quality with state-of-the-art zero-shot image understanding. QLIP trains a binary-spherical-quantization-based autoencoder with reconstruction and language-image alignment objectives. We are the first to show that the two objectives do not need to be at odds. We balance the two loss terms dynamically during training and show that a two-stage training pipeline effectively mixes the large-batch requirements of image-language pre-training with the memory bottleneck imposed by the reconstruction objective. We validate the effectiveness of QLIP for multimodal understanding and text-conditioned image generation with a single model. Specifically, QLIP serves as a drop-in replacement for the visual encoder for LLaVA and the image tokenizer for LlamaGen with comparable or even better performance. Finally, we demonstrate that QLIP enables a unified mixed-modality auto-regressive model for understanding and generation. | 0 | [ğŸ”— Paper](http://arxiv.org/abs/2502.05178v1) |
| [Long-VITA: Scaling Large Multi-modal Models to 1 Million Tokens with
  Leading Short-Context Accuray](http://arxiv.org/abs/2502.05177v1) | Yunhang Shen, Chaoyou Fu, Shaoqi Dong, Xiong Wang, Peixian Chen, Mengdan Zhang, Haoyu Cao, Ke Li, Xiawu Zheng, Yan Zhang, Yiyi Zhou, Rongrong Ji, Xing Sun | 2025-02-07 | Optimization, Multimodal AI | Establishing the long-context capability of large vision-language models is crucial for video understanding, high-resolution image understanding, multi-modal agents and reasoning. We introduce Long-VITA, a simple yet effective large multi-modal model for long-context visual-language understanding tasks. It is adept at concurrently processing and analyzing modalities of image, video, and text over 4K frames or 1M tokens while delivering advanced performances on short-context multi-modal tasks. We propose an effective multi-modal training schema that starts with large language models and proceeds through vision-language alignment, general knowledge learning, and two sequential stages of long-sequence fine-tuning. We further implement context-parallelism distributed inference and logits-masked language modeling head to scale Long-VITA to infinitely long inputs of images and texts during model inference. Regarding training data, Long-VITA is built on a mix of $17$M samples from public datasets only and demonstrates the state-of-the-art performance on various multi-modal benchmarks, compared against recent cutting-edge models with internal data. Long-VITA is fully reproducible and supports both NPU and GPU platforms for training and testing. We hope Long-VITA can serve as a competitive baseline and offer valuable insights for the open-source community in advancing long-context multi-modal understanding. | 0 | [ğŸ”— Paper](http://arxiv.org/abs/2502.05177v1) |
## ğŸ”¹ General AI

| ğŸ“„ Title | ğŸ–Š Authors | ğŸ“… Date | ğŸ· Tags | ğŸ“œ Summary | ğŸ“ˆ Citations | ğŸ”— Link |
|---------|---------|---------|---------|---------|---------|---------|
| [MELON: Indirect Prompt Injection Defense via Masked Re-execution and
  Tool Comparison](http://arxiv.org/abs/2502.05174v1) | Kaijie Zhu, Xianjun Yang, Jindong Wang, Wenbo Guo, William Yang Wang | 2025-02-07 | General AI | Recent research has explored that LLM agents are vulnerable to indirect prompt injection (IPI) attacks, where malicious tasks embedded in tool-retrieved information can redirect the agent to take unauthorized actions. Existing defenses against IPI have significant limitations: either require essential model training resources, lack effectiveness against sophisticated attacks, or harm the normal utilities. We present MELON (Masked re-Execution and TooL comparisON), a novel IPI defense. Our approach builds on the observation that under a successful attack, the agent's next action becomes less dependent on user tasks and more on malicious tasks. Following this, we design MELON to detect attacks by re-executing the agent's trajectory with a masked user prompt modified through a masking function. We identify an attack if the actions generated in the original and masked executions are similar. We also include three key designs to reduce the potential false positives and false negatives. Extensive evaluation on the IPI benchmark AgentDojo demonstrates that MELON outperforms SOTA defenses in both attack prevention and utility preservation. Moreover, we show that combining MELON with a SOTA prompt augmentation defense (denoted as MELON-Aug) further improves its performance. We also conduct a detailed ablation study to validate our key designs. | 0 | [ğŸ”— Paper](http://arxiv.org/abs/2502.05174v1) |
| [Joint MoE Scaling Laws: Mixture of Experts Can Be Memory Efficient](http://arxiv.org/abs/2502.05172v1) | Jan Ludziejewski, Maciej PiÃ³ro, Jakub Krajewski, Maciej Stefaniak, MichaÅ‚ Krutul, Jan MaÅ‚aÅ›nicki, Marek Cygan, Piotr Sankowski, Kamil Adamczewski, Piotr MiÅ‚oÅ›, Sebastian Jaszczur | 2025-02-07 | General AI | Mixture of Experts (MoE) architectures have significantly increased computational efficiency in both research and real-world applications of large-scale machine learning models. However, their scalability and efficiency under memory constraints remain relatively underexplored. In this work, we present joint scaling laws for dense and MoE models, incorporating key factors such as the number of active parameters, dataset size, and the number of experts. Our findings provide a principled framework for selecting the optimal MoE configuration under fixed memory and compute budgets. Surprisingly, we show that MoE models can be more memory-efficient than dense models, contradicting conventional wisdom. To derive and validate the theoretical predictions of our scaling laws, we conduct over 280 experiments with up to 2.7B active parameters and up to 5B total parameters. These results offer actionable insights for designing and deploying MoE models in practical large-scale training scenarios. | 0 | [ğŸ”— Paper](http://arxiv.org/abs/2502.05172v1) |
| [Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth
  Approach](http://arxiv.org/abs/2502.05171v1) | Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, Tom Goldstein | 2025-02-07 | General AI | We study a novel language model architecture that is capable of scaling test-time computation by implicitly reasoning in latent space. Our model works by iterating a recurrent block, thereby unrolling to arbitrary depth at test-time. This stands in contrast to mainstream reasoning models that scale up compute by producing more tokens. Unlike approaches based on chain-of-thought, our approach does not require any specialized training data, can work with small context windows, and can capture types of reasoning that are not easily represented in words. We scale a proof-of-concept model to 3.5 billion parameters and 800 billion tokens. We show that the resulting model can improve its performance on reasoning benchmarks, sometimes dramatically, up to a computation load equivalent to 50 billion parameters. | 0 | [ğŸ”— Paper](http://arxiv.org/abs/2502.05171v1) |
| [Observation of a dynamic magneto-chiral instability in photoexcited
  tellurium](http://arxiv.org/abs/2502.05170v1) | Yijing Huang, Nick Abboud, Yinchuan Lv, Penghao Zhu, Azel Murzabekova, Changjun Lee, Emma A. Pappas, Dominic Petruzzi, Jason Y. Yan, Dipanjan Chauduri, Peter Abbamonte, Daniel P. Shoemaker, Rafael M. Fernandes, Jorge Noronha, Fahad Mahmood | 2025-02-07 | General AI | In a system of charged chiral fermions driven out of equilibrium, an electric current parallel to the magnetic field can generate a dynamic instability by which electromagnetic waves become amplified. Whether a similar instability can occur in chiral solid-state systems remains an open question. Using time-domain terahertz (THz) emission spectroscopy, we detect signatures of what we dub a ``dynamic magneto-chiral instability" in elemental tellurium, a structurally chiral crystal. Upon transient photoexcitation in a moderate external magnetic field, tellurium emits THz radiation consisting of coherent modes that amplify over time. An explanation for this amplification is proposed using a theoretical model based on a dynamic instability of electromagnetic waves interacting with infrared-active oscillators of impurity acceptor states in tellurium to form an amplifying polariton. Our work not only uncovers the presence of a magneto-chiral instability but also highlights its promise for THz-wave amplification in chiral materials. | 0 | [ğŸ”— Paper](http://arxiv.org/abs/2502.05170v1) |
| [Flopping for FLOPs: Leveraging equivariance for computational efficiency](http://arxiv.org/abs/2502.05169v1) | Georg BÃ¶kman, David NordstrÃ¶m, Fredrik Kahl | 2025-02-07 | General AI | Incorporating geometric invariance into neural networks enhances parameter efficiency but typically increases computational costs. This paper introduces new equivariant neural networks that preserve symmetry while maintaining a comparable number of floating-point operations (FLOPs) per parameter to standard non-equivariant networks. We focus on horizontal mirroring (flopping) invariance, common in many computer vision tasks. The main idea is to parametrize the feature spaces in terms of mirror-symmetric and mirror-antisymmetric features, i.e., irreps of the flopping group. This decomposes the linear layers to be block-diagonal, requiring half the number of FLOPs. Our approach reduces both FLOPs and wall-clock time, providing a practical solution for efficient, scalable symmetry-aware architectures. | 0 | [ğŸ”— Paper](http://arxiv.org/abs/2502.05169v1) |
